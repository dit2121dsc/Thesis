<div class="entities" style="line-height: 2.5; direction: ltr"> In order to get more effective information and improve the recognition <mark class="entity" style="background: #9B59B6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">accuracy<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">Metrics</span></mark>, this paper proposes a <mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">speech emotion recognition<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark> model based on multi-feature fusion and deep convolutional <mark class="entity" style="background: #BF11C8; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">neural network<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">Technology</span></mark>. First, the speech emotion data is preprocessed to obtain the two-dimensional three-channel fusion feature parameters, which are used as the input layer of the AlexNet DCNN model. Second, the model is improved. Batch normalization is added after each convolutional layer, and use genetic algorithm and simulated annealing algorithm to optimize the model. Final, we use SoftMax classifier to classify emotion. In this paper, the cross-<mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">validation<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark> method is used to evaluate the model, and it is verified on the <mark class="entity" style="background: #5D6D7E; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">EMO<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">Terms</span></mark>-DB and IEMO-CAP datasets. The experimental results verify that the method is superior to the existing <mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">speech emotion recognition<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark> technology. • <mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">Machine learning<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark> → <mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">Machine learning<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark> approaches; <mark class="entity" style="background: #E74C3C; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">Neural networks<span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">CSO</span></mark>.</div>