{"paper_id": "3485447", "header": {"generated_with": "S2ORC 1.0.0", "date_generated": "2024-03-20T17:53:03.205548Z"}, "title": "Unsupervised Representation Learning of Player Behavioral Data with Confidence Guided Masking", "authors": [{"first": "Jiashu", "middle": [], "last": "Pu", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "pujiashu@corp.netease.com"}, {"first": "Jianshi", "middle": [], "last": "Lin", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "linjianshi@corp.netease.com"}, {"first": "Xiaoxi", "middle": [], "last": "Mao", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "maoxiaoxi@corp.netease.com"}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "hztaojianrong@corp.netease.com"}, {"first": "Xudong", "middle": [], "last": "Shen", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "hzshenxudong@corp.netease.com"}, {"first": "Yue", "middle": [], "last": "Shang", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "shangyue@corp.netease.com"}, {"first": "Runze", "middle": [], "last": "Wu", "suffix": "", "affiliation": {"laboratory": "Fuxi AI Lab", "institution": "NetEase Inc", "location": {"settlement": "Hangzhou", "country": "China"}}, "email": "wurunze1@corp.netease.com"}], "year": "", "venue": null, "identifiers": {}, "abstract": "Players of online games generate rich behavioral data during gaming. Based on these data, game developers can build a range of data science applications, such as bot detection and social recommendation, to improve the gaming experience. However, the development of such applications requires data cleansing, training sample labeling, feature engineering, and model development, which makes the use of such applications in small and medium-sized game studios still uncommon. While acquiring supervised learning data is costly, unlabeled behavioral logs are often continuously and automatically generated in games. Thus we resort to unsupervised representation learning of player behavioral data to optimize intelligent services in games. Behavioral data has many unique properties, including semantic complexity, excessive length, etc. A worth noting property within raw player behavioral data is that a lot of it is task-irrelevant. For these data characteristics, we introduce a BPE-enhanced compression method and propose a novel adaptive masking strategy called Masking by Token Confidence (MTC) for the Masked Language Modeling (MLM) pre-training task. MTC is designed to increase the masking probabilities of task-relevant tokens. Experiments on four downstream tasks and successful deployment in a world-renowned Massively Multiplayer Online Role-Playing Game (MMORPG) prove the effectiveness of the MTC strategy 1 .", "pdf_parse": {"abstract": [{"text": "Players of online games generate rich behavioral data during gaming. Based on these data, game developers can build a range of data science applications, such as bot detection and social recommendation, to improve the gaming experience. However, the development of such applications requires data cleansing, training sample labeling, feature engineering, and model development, which makes the use of such applications in small and medium-sized game studios still uncommon. While acquiring supervised learning data is costly, unlabeled behavioral logs are often continuously and automatically generated in games. Thus we resort to unsupervised representation learning of player behavioral data to optimize intelligent services in games. Behavioral data has many unique properties, including semantic complexity, excessive length, etc. A worth noting property within raw player behavioral data is that a lot of it is task-irrelevant. For these data characteristics, we introduce a BPE-enhanced compression method and propose a novel adaptive masking strategy called Masking by Token Confidence (MTC) for the Masked Language Modeling (MLM) pre-training task. MTC is designed to increase the masking probabilities of task-relevant tokens. Experiments on four downstream tasks and successful deployment in a world-renowned Massively Multiplayer Online Role-Playing Game (MMORPG) prove the effectiveness of the MTC strategy 1 .", "cite_spans": [], "section": "Abstract", "sec_num": null}], "body_text": [{"text": "At least 3.0 billion players have been entertained by various types of video games, constituting a $175.8 billion booming market 2 . For improving the gaming experiences and revenue, many of the latest machine learning techniques have been adopted to deal with tasks including bot detection [42] , player churn prediction [53] , personalized recommendation [10] , etc.", "cite_spans": [{"start": 291, "end": 295, "text": "[42]", "ref_id": "BIBREF41"}, {"start": 322, "end": 326, "text": "[53]", "ref_id": "BIBREF52"}, {"start": 357, "end": 361, "text": "[10]", "ref_id": "BIBREF9"}], "section": "INTRODUCTION", "sec_num": "1"}, {"text": "Despite its impressive success, developing supervised learningbased applications on behavioral data is both time-consuming and labor-intensive. A typical development process includes data cleansing, feature engineering, training sample labeling, model development, and deployment [42] . Data cleansing is for filtering out taskirrelevant records in the data, requiring sufficient knowledge of data logging format. Manual feature engineering requires extensive expert knowledge. Sample labeling is notoriously expensive as the labeling process requires extensive in-game knowledge. This process often needs to be repeated after a major update of the online game. On the other hand, the accumulation of raw (unlabeled) behavioral logs is never-ending, and the rate at which it accumulates is proportional to the number of active players. Over time, both old and new games have the opportunity to accumulate a sufficient amount of behavioral data. Thus we believe the key to efficiently iterating on intelligent services in games is mining useful knowledge from the raw behavioral data with less or no human intervention.", "cite_spans": [{"start": 280, "end": 284, "text": "[42]", "ref_id": "BIBREF41"}], "section": "INTRODUCTION", "sec_num": "1"}, {"text": "We resort to unsupervised representation learning of player behavioral data. Unsupervised representation learning has been highly successful in many deep-learning applications [11, 27] . Nevertheless, the paradigm is still rudimentary in the game domain. Some initial efforts have been proposed [29, 42] , but [29] still requires manual efforts in extracting concerning sub-sequences. It is also undesirable to directly copy the pre-training scheme from the Natural Language Processing (NLP) field as texts and behavioral data are inherently different.", "cite_spans": [{"start": 176, "end": 180, "text": "[11,", "ref_id": "BIBREF10"}, {"start": 181, "end": 184, "text": "27]", "ref_id": "BIBREF26"}, {"start": 295, "end": 299, "text": "[29,", "ref_id": "BIBREF28"}, {"start": 300, "end": 303, "text": "42]", "ref_id": "BIBREF41"}, {"start": 310, "end": 314, "text": "[29]", "ref_id": "BIBREF28"}], "section": "INTRODUCTION", "sec_num": "1"}, {"text": "In the next section, we elaborate on the differences between texts and behavioral data, based on which we propose a novel pretraining task with an effective sequence compression method. To verify the effectiveness of our framework, we conduct a series of experiments over behavioral datasets collected from two real-world games-Justice Online3 and Ghost II 4 , and set four probing tasks for evaluation, including bot detection, churn prediction, purchase timing prediction and similar player inducing. Furthermore, we (b) Each value on the X-axis represents the right bound of a token confidence interval. For example, 0.055 represents a confidence interval from 0.027-0.055; about 38% of the token's average confidence falls in this interval, with around 48% them being proactive tokens. have successfully deployed our method online in a world-renowned MMORPG for bot detection.", "cite_spans": [], "section": "INTRODUCTION", "sec_num": "1"}, {"text": "We summarize two key discrepancies between texts and behavioral sequences. First, texts in natural language are usually of appropriate length and segmented to facilitate understanding of the semantics while behavioral sequences in the game are much longer (more than 1000 tokens on average), and no explicit segmentation is provided. Second, the high information density of natural language leads to ambiguity and non-determinism in interpretation in different contexts, while raw behavioral sequences contain redundant information such as automatic system logs, or information less relevant to the player's subjective volition (e.g. passive action logs). Giving too much redundant and irrelevant information may prevent the model from learning different play styles and player personalities, leading to convergent or meaningless behavioral representations. Therefore, we hope the model to spend more effort on learning tokens that reflect players' autonomous thinking and initiative. In pre-training, we find directly applying the vanilla Maksed Language Modeling (MLM) leads to biased sampling. As can be seen in Figure 1 (a), the most frequent masked tokens are automatic systems logs, [UNK] (Out-Of-Vocabulary id with a very low frequency) token, and passive logs. We label the top 6 masked tokens with serial numbers on top of bars. Tokens ranked first and third are [UNK] log ids, which have no consistent meanings across different sequences. Tokens ranked second, fourth, fifth, and sixth are similar passive logging actions, which say 'Friendliness over 100'. Moreover, Figure 1 (b) shows that the proportion of proactive tokens increases as the confidence value of token decreases (more obvious when confidence value < 0.246), which is consistent with the general perception that the player's subjective will is more difficult to be inferred. The difficulty is probably caused by the lack of similar contexts for the same token. Overall, we can conclude that, under vanilla MLM, task-irrelevant logs are oversampled and task-relevant proactive logs are not fully exploited. Based on the above analysis, we think it is not reasonable to directly adopt the vanilla MLM. We propose our solutions to both problems. For the excessive length and the absence of semantic segmentation, we propose to use Byte Pairing Encoding [14] (BPE) to iteratively merge co-occurring adjacent behavioral tokens as a pre-processing step before pre-training. For the biased token sampling issue, we propose to use the model's prediction to guide the sampling process, which is named as Mask with Token Confidence (MTC). The motivation of MTC is to increase the learning of proactive tokens during pre-training. The model dynamically adjusts tokens' masking probability based on the model's previous prediction, increasing the masking probabilities of low confidence tokens. We also test a hybrid masking strategy which ensembles the conservative uniform masking strategy with MTC, named Partially Mask with Token Confidence (PMTC).", "cite_spans": [{"start": 2327, "end": 2331, "text": "[14]", "ref_id": "BIBREF13"}], "section": "Challenges of modeling player behaviors", "sec_num": "1.1"}, {"text": "This paper highlights an effective sequence compression method (BPE) and masking strategies guided by token confidence. We summarize the merits of our framework as follows5 :", "cite_spans": [], "section": "Challenges of modeling player behaviors", "sec_num": "1.1"}, {"text": "\u2022 Our approach fully exploits the task-relevant knowledge from the raw behavioral data and does not rely on expert knowledge of the game. \u2022 Low requirements for the pre-training data: our method does not require any data cleansing or manual filtering. \u2022 Pre-training once meets the needs of multiple scenarios simultaneously. Experimental results show the models with adaptive masking strategy perform well beyond baselines on four probing tasks and generalize well to different games. \u2022 Our approach can markedly reduce the cost of labeling. Even with a small amount of data, fine-tuning the pre-trained model on downstream tasks can yield satisfactory results. We sample several behavioral sequences from Justice Online and consult some experienced players to score the difficulty of predicting proactive tokens, and game developers to score the passive/system logs. Here we present two typical examples, which show that predicting proactive logs is more difficult but helps the model learn more about the player's characteristics while predicting passive/system logs can sometimes be too easy or contextually irrelevant. Figure 3 : Each value on the x-axis represents a left bound of proactive events' ratio interval, e.g., the bin and the data point of 0.0556 indicates there are approximately 4.8% of total sequences whose ratio of proactive events range from 0.0556 to 0.111, with an average sequence length of about 800.", "cite_spans": [], "section": "Challenges of modeling player behaviors", "sec_num": "1.1"}, {"text": "MMORPGs provide virtual environments where interactions among player-controlled avatars are carried out on a daily basis [44] . When players explore or interact with other people in the game, game developers often choose to record a large number of behavior logs. The sheer quantity is to ensure the integrity of the recorded information. Behavioral logs may later be traced for use in various game services [36] . The player behavior log often consists of a sequence of events in chronological order, where each event indicates a specific player action or a system record at that moment. Each event consists of three parts:", "cite_spans": [{"start": 121, "end": 125, "text": "[44]", "ref_id": "BIBREF43"}, {"start": 408, "end": 412, "text": "[36]", "ref_id": "BIBREF35"}], "section": "BEHAVIORAL DATA IN ONLINE GAMES", "sec_num": "2"}, {"text": "\u2022 Event ID: The behavioral event conducted by a player, for example, obtaining a certain item, leveling up, etc. \u2022 Object ID: The specific targets of events, such as the map player entered, the item player bought or used. \u2022 Timestamp: The moment corresponding to that behavioral event.", "cite_spans": [], "section": "BEHAVIORAL DATA IN ONLINE GAMES", "sec_num": "2"}, {"text": "Event ID represents a coarse-grained behavioral type. There are 875 types in Justice Online, while only 120 types in Ghost II. Object ID is used along with Event ID. There are 150,978 and 46,231 Object ID in Justice Online and Ghost II respectively.", "cite_spans": [], "section": "BEHAVIORAL DATA IN ONLINE GAMES", "sec_num": "2"}, {"text": "A behavioral event can be proactive, passive, or a part of a system log, with examples shown in Figure 2 . Proactive events represent the active choices and free will of the player. Examples of proactive events include entering maps, picking up items, upgrading, etc. On the other hand, the in-game logging system records players' states periodically, generating passive behavioral logs, which are also considered as player portrait states information [51] . The recording of system logs operate in a heartbeat manner, including player status query, current map query, etc. It is worth noting that we conceptually categorize and distinguish between proactive and passive logs, while the proactive/passive label itself is not available in the logging system. We manually label proactive events in Justice Online and present the corresponding statistics in Figure 3 .", "cite_spans": [{"start": 452, "end": 456, "text": "[51]", "ref_id": "BIBREF50"}], "section": "BEHAVIORAL DATA IN ONLINE GAMES", "sec_num": "2"}, {"text": "Notably, this form of behavioral logging and extraction is quite common. It is mentioned in other researches about MMORPGs [40, 41, 54] and also present in other game genres such as Action Role-Playing Game (ARPG) [24] .", "cite_spans": [{"start": 123, "end": 127, "text": "[40,", "ref_id": "BIBREF39"}, {"start": 128, "end": 131, "text": "41,", "ref_id": "BIBREF40"}, {"start": 132, "end": 135, "text": "54]", "ref_id": "BIBREF53"}, {"start": 214, "end": 218, "text": "[24]", "ref_id": "BIBREF23"}], "section": "BEHAVIORAL DATA IN ONLINE GAMES", "sec_num": "2"}, {"text": "We denote a player's behavioral sequence S of length l as (e 1 , o 1 , . . . , e l /2 , o l /2 ), where e i is the event id and o i is the object id. We aim to build a sequence encoder f \u03b8 : S \u2192 R d , either being fine-tuned or used as a feature extractor on a variety of downstream tasks in the game, to improve the quality of intelligent services and reduce the workload of labeling and feature engineering. To reach these goals, we propose to pre-train the sequence encoder on the raw behavioral data to mine useful knowledge before adapting to downstream tasks. The sequence encoder is expected to accurately represent different types of behaviors and generalize well to different scenarios.", "cite_spans": [], "section": "TASK FORMULATION", "sec_num": "3"}, {"text": "We propose to use Byte Pair Encoding (BPE) [14] to compress raw behavioral sequences. BPE is widely adopted in NLP to deal with the out-of-vocabulary problem [8, 35] . Here we employ BPE for sequence compression and semantic segmentation. Specifically, we adapt BPE to the behavioral sequences by treating event/object ids as the atomic symbol for merging, in an attempt of incorporating dataset-wide information to guide the local token merging operations. During the vocabulary building process, we merge iteratively the most frequent pair of event/object ids until we reach the specified vocabulary size. Afterwards, we use the BPE tokenizer to compress S = (e 1 , o 1 , . . . , e l /2 , o l /2 ) of length l into a shorter sequence T = (t 1 , t 2 , . . . , t q ) of length q, where t i is merged from a segment of length r (1 <= r < q) containing e i and/or o i .", "cite_spans": [{"start": 43, "end": 47, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 158, "end": 161, "text": "[8,", "ref_id": "BIBREF7"}, {"start": 162, "end": 165, "text": "35]", "ref_id": "BIBREF34"}], "section": "METHOD 4.1 Behavioral Sequence Compression", "sec_num": "4"}, {"text": "We choose BERT model [11] as the main experimental target because of its decent sequence modeling capability [46] and great adaptability-BERT has several successful applications in computer version [31] and biomedical [25] . A typical transformer encoder is composed of stacked Transformer blocks [45] . Each block consists of a multi-head self-attention layer followed by a fully connected positional feed-forward network.", "cite_spans": [{"start": 21, "end": 25, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 109, "end": 113, "text": "[46]", "ref_id": "BIBREF45"}, {"start": 198, "end": 202, "text": "[31]", "ref_id": "BIBREF30"}, {"start": 218, "end": 222, "text": "[25]", "ref_id": "BIBREF24"}, {"start": 297, "end": 301, "text": "[45]", "ref_id": "BIBREF44"}], "section": "Model", "sec_num": "4.2"}, {"text": "The most commonly used pre-training task for NLP is MLM, where a few tokens are masked as the input and recovered from the output; MLM aims to learn high-level co-occurring patterns. We choose MLM and transplant it to the game domain because the contextsensitive property is also prevalent in behavioral sequences [44] , for instance, the player may need to prepare advanced equipment, level up, form a balanced team, and accomplish several small tasks before accomplishing a milestone task (e.g. kill the final boss). Taskrelevant events are correlated and can be inferred from each other in the context of the task. Below we formally define the objective of MLM. Given a sequence S = (s 1 , . . . , s l ) of l tokens, we obtain a corrupted S by masking approximately 15% of its tokens. Then we have the reconstruction goal -to train BERT parameterized by \u03b8 to predict masked tokens s conditioned on S,", "cite_spans": [{"start": 314, "end": 318, "text": "[44]", "ref_id": "BIBREF43"}], "section": "Pre-training task", "sec_num": "4.3"}, {"text": "EQUATION", "cite_spans": [], "section": "Pre-training task", "sec_num": "4.3"}, {"text": ")", "cite_spans": [], "section": "Pre-training task", "sec_num": "4.3"}, {"text": "where C is the index set of masked tokens. Token s i is corrupted in three forms: 80% of the time it is replaced with [MASK] token, 10% of the time it remains unchanged and the rest 10% time it is replaced with a randomly picked token.", "cite_spans": [], "section": "Pre-training task", "sec_num": "4.3"}, {"text": "For a sequence S = (s 1 , . . . , s l ) of length l, we can denote its corresponding masking probability vector as", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "p = [p 1 \u2022 \u2022 \u2022 p l ],", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "where p i represents the masking probability for s i . In each training batch, we sample mask tokens by Bernoulli's distribution according to p. Below we list two baseline masking strategies and introduce our innovation. We focus on describing how each strategy generates p and illustrate the differences in Figure 4 . Before introducing masking strategies, we list the annotations shared by all. We denote \u03b2 as the masking ratio, and it is set to 0.15 throughout all experiments. The index set of S is defined as I .", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "Masking uniformly The origin paper of BERT [11] proposes to randomly sample tokens with a uniform distribution, with an equal probability (0.15) assigned to each token. We denote the uniform probability vector as", "cite_spans": [{"start": 43, "end": 47, "text": "[11]", "ref_id": "BIBREF10"}], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "p u = [0.15 \u2022 \u2022 \u2022 0.15].", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "Masking by ITF-IDF Fairseq6 provides an additional mask strategy based on term frequency. We follow their idea and propose to sample tokens proportional to the it f \u2022 id f value, with it f being the reciprocal of term frequency in a sequence and id f being the Inverted Document Frequency (IDF) [34] . For the game dataset, we consider a player's sequence as a document and compute IDF values accordingly. The motivation is to increase the masking probability of less frequent tokens, either when they are less frequent in a sequence (high ITF) or when they are less frequent under a global perspective (high IDF). We denote", "cite_spans": [{"start": 295, "end": 299, "text": "[34]", "ref_id": "BIBREF33"}], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "p it f id f = [p it f id f 1 \u2022 \u2022 \u2022 p it f id f l", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "] as the masking probability vector under the ITF-IDF strategy. We calculate the the masking probability p it f id f i of token-i in S according to Equation 3,", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "EQUATION", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "EQUATION", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "where it f i and id f i are the ITF and IDF value of i-th token. We denote T as the temperature coefficient of the Softmax function. We multiply the Softmax output by \u03b2 and l to ensure an approximate masking ratio of 100%\u03b2 for each sequence.", "cite_spans": [], "section": "Masking strategy.", "sec_num": "4.3.1"}, {"text": "The ITF-IDF masking strategy is subject to the assumption that training tokens with high ITF-IDF value help to model players' behavior better, which is not necessarily true. At the same time, it fails to consider the dynamic learning ability of the model, where some tokens may be easily inferred with reoccurring similar contexts, while others may rarely appear or be difficult to learn due to varying contexts. Apart from the limitation of above masking strategies, we empirically find low confidence tokens containing more proactive information (Figure 1(b) ). Therefore we propose to use the model's confidence on seen tokens as guidance for masking new sequences, by assigning more masking probabilities to tokens with lower confidence values. To store tokens' confidence values, we continuously track the model's confidence over the entire vocabulary by updating a tracking vector tr", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "= [tr 1 \u2022 \u2022 \u2022 tr v ] \u2208 R v", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": ", where v is the size of the Vocabulary V . We initialize tr with a zero vector \u00ec 0 \u2208 R v . During training, we update tr every n steps (when t % n == 0) according to Equation 4,", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "tr t = [f \u03b8 (s 1 | S) \u2022 \u2022 \u2022 f \u03b8 (s j | S), s j \u2208 B]; tr B = [tr i , tr i \u2208 B] tr B = (1 -\u03b1) \u2022 tr B + \u03b1 \u2022 tr t (4)", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "where t is the index of the training step and B is the token subset in batch t. We denote tr t as a batch tracking vector, tr B as a subset of tr containing old confidence values, the sequence encoder as f \u03b8 , and the model's average confidence value on token s j as f \u03b8 (s j | S) (Model may predict s j several times with different contexts S in batch t). The coefficient \u03b1 controls the decaying speed of old confidence values. We illustrate how tr is updated in Figure 4 compute p m i according to Equation 5,", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "EQUATION", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "where tr (s i ) is the confidence value of token s i in tr , (1 -tr (s i )) is an intermediate value ranging from 0 to 1 and T is the Softmax temperature. The masking probability of s i is inversely proportional to the confidence value tr (s i ). We use the Softmax function defined in Equation 2 to ensure a normalized distribution. In experiments, we find that the masking property is strongly affected by the Softmax temperature T . One simple approach is to set a fixed temperature throughout the training procedure. However, we find such an approach has one obvious drawback. A static temperature can only be effective in a limited period because tokens' confidence values are constantly growing as the training progress; such limitation is illustrated in Figure 5 . Setting temperature too high results in a close-to-uniform distribution, while setting the temperature too low (0.005) gives close-to-zero masking probabilities for tokens ranked above 130. Our solution is to adjust the temperature dynamically according to tokens' confidence values. Empirically, we find a proper distinguishable masking distribution can be achieved by setting the temperature T to the average of token confidences in the training batch (batch size equals 1 in Figure 5 ).", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "In our experiment, we find the globally averaged value is very close to the batch averaged value in all training stages-the average disparity is within 0.001. The globally averaged value is easy to track and can avoid repetitive computation. Thus we propose to use globally averaged value as a substitution and formalize its computation as T = ( v i=1 tr i )/v, i \u2208 V . However, in experiments, we find it unstable to directly apply this masking strategy. Oscillations of the training loss appear much more frequently in the early stage than the uniform sampling strategy, indicating the risking of diverging. There might be two possible factors that lead to the training instability: (1) the confidence level of the model's prediction may be inaccurate at the beginning. (2) the overly aggressive masking strategy is detrimental to the convergence of the loss. Partially Masking by Token Confidence (PMTC) To alleviate the instability entailed by MTC, we propose to train alternatively between MTC and a conservative strategy (e.g. uniform sampling), with one training strategy being randomly chosen for each training batch. In concrete, first, we initialize the hyperparameter \u03c1 \u2208 [0.0, 1.0] to control the percentage of training by MTC, then for each training step, we generate a random number r ranging from 0.0 to 1.0, if r < \u03c1, we train this batch by MTC, otherwise by uniform sampling. For experiments in Section 7, we also include one variant of MTC that linearly increases \u03c1 from 0.0 to 1.0 during training.", "cite_spans": [], "section": "Masking by Token Confidence (MTC)", "sec_num": null}, {"text": "We choose four probing tasks from two games to verify whether the pre-trained transformer encoder can represent the behavioral sequence well and generalize to different scenarios.", "cite_spans": [], "section": "DOWNSTREAM TASKS", "sec_num": "5"}, {"text": "The proliferation of bot scripts in the game industry has become a severe problem [42] . Bot detection is formalized as a binary classification task. The model predicts whether the player has cheated given the behavioral sequence of a particular date.", "cite_spans": [{"start": 82, "end": 86, "text": "[42]", "ref_id": "BIBREF41"}], "section": "Bot detection", "sec_num": null}, {"text": "Churn prediction Accurately predicting players' churn tendency is crucial for game developers to maintain game popularity and high revenue [52, 54] . The model predicts whether a player will leave the game (no login for 7 consecutive days) given his/her behavioral sequences of the previous fourteen consecutive days. Purchase timing prediction As the right timing is a key part of the equipment/item recommendation, we design a binary classification task to predict the purchase timing. Given a player's behavioral sequence of arbitrary length, the model predicts players' purchase intentions (buy or not buy within one minute). Inducing similar players Proactively finding like-minded players is beneficial for social recommendations. Given massive behavioral data, the task is formalized as an unsupervised clustering, where the model is expected to discover clusters of similar players.", "cite_spans": [{"start": 139, "end": 143, "text": "[52,", "ref_id": "BIBREF51"}, {"start": 144, "end": 147, "text": "54]", "ref_id": "BIBREF53"}], "section": "Bot detection", "sec_num": null}, {"text": "Since Justice Online and Ghost II have different requirements for their business, they only provide labeled data for specific downstream tasks. The purchase timing prediction task is from Ghost II while the other three tasks are from Justice Online. For each supervised task, the transformer encoder either converts the behavioral sequence S into a fixed-length vector-average pooling of the hidden state of the encoder's penultimate layer, used as the input for downstream models, or is directly fine-tuned on downstream datasets. The fine-tuning process is rather simple, we update the full parameters of the encoder and the model of the downstream task simultaneously, against the cross-entropy loss. To induce similar players, the transformer encoder projects behavioral sequences into a high-dimensional space for clustering.", "cite_spans": [], "section": "Bot detection", "sec_num": null}, {"text": "For both games, transformer encoders are pre-trained on the dataset consisting of 100,000 behavioral sequences. The pre-training data of Justice Online and Ghost II contain 93,465 and 99,200 unique players respectively; Justice Online spans from March 2020 to May 2020, while Ghost II spans from April 2021 to June 2021. The data for the downstream tasks contain a similar proportion of unique players and are sourced from the following month, in order to avoid data overlap. About 5% of the pre-training data in Justice Online corresponds to bot logs. For other training settings of downstream tasks and pre-training, please refer to Supplementary Materials.", "cite_spans": [], "section": "EXPERIMENT", "sec_num": "6"}, {"text": "For the bot detection, the churn prediction, the purchase prediction task, we include four state-of-the-art baselines7 :", "cite_spans": [], "section": "Baselines", "sec_num": "6.1"}, {"text": "MSDMT [53] : A three-module framework for both churn and payment prediction, capable of integrating data of multi-source. ChurnPredLSTM [54] : An end-to-end LSTM framework for churn prediction, leveraging both login behaviors and in-game behaviors. NGUARD [42] : A powerful bot detection framework enhanced with the Time-interval Event2vec and the SA-ABLSTM structure. NGUARD+ [50] : Attention-based Bidirectional LSTM and Hierarchical Self-attention Network.", "cite_spans": [{"start": 6, "end": 10, "text": "[53]", "ref_id": "BIBREF52"}, {"start": 136, "end": 140, "text": "[54]", "ref_id": "BIBREF53"}, {"start": 256, "end": 260, "text": "[42]", "ref_id": "BIBREF41"}, {"start": 377, "end": 381, "text": "[50]", "ref_id": "BIBREF49"}], "section": "Baselines", "sec_num": "6.1"}, {"text": "Regarding baselines of the sequence encoder, we include the Bags of Word (BOW) model [26] and two efficient transformers, Longformer [6] and Reformer [22] , which are trained on raw behavioral sequences with the white-space tokenizer.", "cite_spans": [{"start": 85, "end": 89, "text": "[26]", "ref_id": "BIBREF25"}, {"start": 133, "end": 136, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 150, "end": 154, "text": "[22]", "ref_id": "BIBREF21"}], "section": "Baselines", "sec_num": "6.1"}, {"text": "Besides, we include the Player Portrait feature [51] , a typical feature of the traditional churn and payment prediction models [54] . It is a multi-dimensional representation of the player, including the player's online time, capital, profession, level, etc.", "cite_spans": [{"start": 48, "end": 52, "text": "[51]", "ref_id": "BIBREF50"}, {"start": 128, "end": 132, "text": "[54]", "ref_id": "BIBREF53"}], "section": "Baselines", "sec_num": "6.1"}, {"text": "We report experimental results of feature extraction and fine-tuning in Table 1 and Table 2 respectively. We report the f1-score [16] on the bot detection task, the churn prediction task, and the purchase timing prediction task while the clustering task is evaluated with Adjusted Rand Index (ARI) score [38] . All scores are averaged over 15 runs, each with a different random seed controlling the data split. Below we discuss the implications of various factors.", "cite_spans": [{"start": 129, "end": 133, "text": "[16]", "ref_id": "BIBREF15"}, {"start": 304, "end": 308, "text": "[38]", "ref_id": "BIBREF37"}], "section": "RESULTS AND DISCUSSIONS", "sec_num": "7"}, {"text": "Tokenization The BPE tokenizer performs substantially better than the white-space counterpart in all experiments. The success of the BPE tokenizer can be summarized in two aspects. First, BPE markedly reduces the average sequence length to approximately one-fifth of the original, circumventing the problem of BERT's weak support for long sequences [43] . Furthermore, in segments merged by BPE, we find meaningful action groups and long series of system logs, which can be analogous to Whole Word Masking [27] in NLP.", "cite_spans": [{"start": 349, "end": 353, "text": "[43]", "ref_id": "BIBREF42"}, {"start": 506, "end": 510, "text": "[27]", "ref_id": "BIBREF26"}], "section": "RESULTS AND DISCUSSIONS", "sec_num": "7"}, {"text": "Sequence encoder The BERT model with a BPE tokenizer outperforms two efficient transformers on all tasks, reflecting the difficulty of pre-training on raw behavioral sequences. Our speculation on such failure is that the task of MLM is too easy to accomplish on raw behavioral sequences, which is supported by the perplexity statistics of the Justice Online dataset when training is finished; the perplexity for BERT (uniform sampling) with BPE tokenizer, LongFormer, and reformer with white space tokenizer are 8.16, 1.68, 2.71 respectively. Notably, the non-pre-trained BERT performs significantly worse than the pre-trained counterpart, which proves the paradigm of pre-training is also effective on the game behavioral sequences. One study [2] attributes the benefits of pre-training to minimized intrinsic dimension. Overall, models of pre-trained BERT with the BPE tokenizer greatly outperform other baselines in all tasks except for bot detection when models are not fine-tuned. Masking Strategy On the bot detection task, we find all masking strategies perform comparably. Below we focus our discussion within the scope of the churn prediction and the purchase timing prediction task. Results in Table 1 suggest PMTC (\u03c1 = 0.1) is the overall best method for feature extraction, however, the advantage is relatively slim compared with ITF-IDF and MTC. Fine-tuning results in Table 2 show PMTC with a linearly increased \u03c1 significantly outperforms other masking strategies. On the unsupervised clustering task, we find a huge performance gap between MTC (T = a.t .c) and MTC (T = 0.0001), but their performance on other tasks is indistinguishable, which indicates the clustering task is sensitive to the setting of hyperparameter T . Whether we should set a fixed \u03c1 or linearly increase it for PMTC has mixed results in Table 1 and Table 2 , where a fixed \u03c1 performs better when encoders' parameters are frozen while a linear-increased \u03c1 has a clear advantage when encoders are fine-tuned. Overall, no single masking strategy is optimal for all tasks, but the strategies of PMTC have significant advantages over others, especially when the encoders Table 1 : We report the average score when models are used for feature extraction, with the standard deviation omitted for brevity. In the model column, the subscript ws denotes the white-space tokenizer while the subscript bpe denotes the BPE tokenizer. Numbers in the second row corresponds to the size of the training data. The abbreviation 'a.t.c' means setting the temperature of Softmax to the average token confidence. '\u03c1 linear increase' refers to the condition where \u03c1 is linearly increased from 0.0 to 1.0 from the start to the end of training. Best scores are in bold and the second best are underlined. Paired t-test [20] is performed between the best model and others, with \u2020 indicating p-value < 0.05.", "cite_spans": [{"start": 744, "end": 747, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 2784, "end": 2788, "text": "[20]", "ref_id": "BIBREF19"}], "section": "RESULTS AND DISCUSSIONS", "sec_num": "7"}, {"text": "Masking assist in generating annotated samples. Every day we obtain the latest raw behavioral logs from the data warehouse, train the PMTC transformer encoder, and use the updated encoder to extract the representations of the behavioral sequences. Then we adopt the OP-TICS algorithm [3] to induce clusters of similar bot-like behaviors from these representations. A major advantage of pre-training is that it yields highly time-sensitive representations while traditional Player Portrait features take much longer to update and may not capture the latest gameplay changes in time. The system's former feature extraction modules -DOC2VEC [23] and LDA [7] require retraining on the full data each day to ensure representations are up-to-date, which clearly cannot meet the demand for fast iteration. In contrast, our PMTC transformer encoder is incrementally pre-trained on the latest sequence data to avoid distribution shift. In practice, our method entails more accurate representations of the behavioral sequences, which allows us to obtain more clusters per day, while also ensuring intra-cluster consistency and significantly speeding up manual annotation, forming a virtuous circle. After switching to the PMTC transformer encoder, the daily number of suspicious players continues to rise (Figure 6(b) ).", "cite_spans": [{"start": 284, "end": 287, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 638, "end": 642, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 651, "end": 654, "text": "[7]", "ref_id": "BIBREF6"}], "section": "Model", "sec_num": null}, {"text": "Game Player Modeling [37] introduces a general vocabulary that helps build a taxonomy of player modeling. [30] proposes DPBM (deep player behavior models) and compares with machine learning methods and show the efficiency in several tasks such as predicting skill-use. [12] uses unsupervised learning methods to find player patterns in games that could help game developers to design and tailor game mechanics. [18] investigates human decision-making in games by introducing generative agents. Besides, there are many existing applications using game player modeling techniques: [10] takes the interaction between game players and game items to model the preferences of players for personalized recommendations. [41, 42] extracts a variety of features from behavioral sequences or relationship graphs to automatically recognize the suspected bot players. [53] adopts carefully designed tabular features and devises a multi-source fusion model for predicting game player churn rate and payment. [13, 49] analyzes the action sequence of each player and proposes predictive models for suggesting what the player is likely to do next. All existing methods are based on caseby-case feature engineering while we propose to mine transferable knowledge from massive behavioral data. Sequence Compressing via BPE BPE has been proved essential and successful in many areas. BPE has its crucial contribution to pre-trained language models (LM), reducing vocabulary size and dealing with Out-Of-Vocabulary(OOV) problems [27, 32, 33] . BPE is also effective in other types of sequence data, such as speech signals [48] , the programming language [19] and protein motifs [28] . However, we are the first to conduct compression and semantic segmentation of behavioral sequences with BPE. Pre-training task The pre-training task is often designed to match sequence characteristics. For the text sequence, it is natural to apply Casual Language Modeling (CLM) [32] or MLM [11, 27] as missing text tokens are often correlated with uni(or bi)-directional contexts. Following MLM, various masking schemes have been proposed. ERNIE [39] incorporates entity-level masking and phraselevel masking. StructBERT [47] applies the Span Order Recovery task to integrate language structures. Contrastive learning (CTL) [4] has received a lot of attention recently, including Deep InfoMax (DIM) [17] , Replaced Token Detection (RTD) [9] ,etc.", "cite_spans": [{"start": 21, "end": 25, "text": "[37]", "ref_id": "BIBREF36"}, {"start": 106, "end": 110, "text": "[30]", "ref_id": "BIBREF29"}, {"start": 269, "end": 273, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 411, "end": 415, "text": "[18]", "ref_id": "BIBREF17"}, {"start": 579, "end": 583, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 712, "end": 716, "text": "[41,", "ref_id": "BIBREF40"}, {"start": 717, "end": 720, "text": "42]", "ref_id": "BIBREF41"}, {"start": 855, "end": 859, "text": "[53]", "ref_id": "BIBREF52"}, {"start": 994, "end": 998, "text": "[13,", "ref_id": "BIBREF12"}, {"start": 999, "end": 1002, "text": "49]", "ref_id": "BIBREF48"}, {"start": 1508, "end": 1512, "text": "[27,", "ref_id": "BIBREF26"}, {"start": 1513, "end": 1516, "text": "32,", "ref_id": "BIBREF31"}, {"start": 1517, "end": 1520, "text": "33]", "ref_id": "BIBREF32"}, {"start": 1601, "end": 1605, "text": "[48]", "ref_id": "BIBREF47"}, {"start": 1633, "end": 1637, "text": "[19]", "ref_id": "BIBREF18"}, {"start": 1657, "end": 1661, "text": "[28]", "ref_id": "BIBREF27"}, {"start": 1943, "end": 1947, "text": "[32]", "ref_id": "BIBREF31"}, {"start": 1955, "end": 1959, "text": "[11,", "ref_id": "BIBREF10"}, {"start": 1960, "end": 1963, "text": "27]", "ref_id": "BIBREF26"}, {"start": 2111, "end": 2115, "text": "[39]", "ref_id": "BIBREF38"}, {"start": 2186, "end": 2190, "text": "[47]", "ref_id": "BIBREF46"}, {"start": 2289, "end": 2292, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 2364, "end": 2368, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 2402, "end": 2405, "text": "[9]", "ref_id": "BIBREF8"}], "section": "RELATED WORK", "sec_num": "9"}, {"text": "Traditional supervised learning is difficult to be applied by small and medium-sized game studios due to its need for expert knowledge and large amounts of labeled data. Raw behavioral logs in games are quickly accumulating in quantity but are not well utilized. We believe raw behavioral logs contain a great deal of exploitable knowledge, which can be leveraged to overcome label shortages and help game developers quickly develop new intelligent services or adapt to new games. Thus we propose a game-insensitive framework to unsupervisedly learn task agnostic representations of player behavioral sequences from raw logs. Our framework highlights a BPE-enhanced sequence compression method and a novel masking strategy (PMTC) that is in accordance with the nature of the behavioral sequence, where proactive events often have low confidence values but are more valuable as predicting targets. The confidence-guided masking strategy of PMTC is proved effective in four downstream tasks, even when the labeled data is minimal.", "cite_spans": [], "section": "CONCLUSION", "sec_num": "10"}, {"text": "In recent years, online game developers are committed to introducing more and more intelligence services to enhance the game experience [10, 42, 53] . A highly intelligent virtual world is the future of online games, which is in line with the kernel of WEB4.0 [1] -a massive web of highly intelligent interactions. We believe our work can inspire other intelligent web-based applications, especially those containing human interactions. is 256. To test whether the model generalizes well to different players, we partition the dataset in such a way that the player IDs contained in the training and test sets are non-overlapping.", "cite_spans": [{"start": 136, "end": 140, "text": "[10,", "ref_id": "BIBREF9"}, {"start": 141, "end": 144, "text": "42,", "ref_id": "BIBREF41"}, {"start": 145, "end": 148, "text": "53]", "ref_id": "BIBREF52"}, {"start": 260, "end": 263, "text": "[1]", "ref_id": "BIBREF0"}], "section": "CONCLUSION", "sec_num": "10"}, {"text": "The training set contains 6048 samples while the test set contains 604 samples. The ratio of positive (purchase any equipment or item in the next 60 seconds) and negative samples is 1:8. 11.1.4 Similar Player Inducing. Assuming the same player has similar behaviors over a small period, i.e. a couple of days, we can construct an evaluation dataset C for similar player induction and formalize the task as a clustering problem. The dataset C contains N sequences S i and the corresponding label y i , where S i denotes a player's behavioral sequence, and y i \u2208 {Y = 1, . . . , C} denotes the cluster's label. Players with the same cluster label y i are considered to behave in similar ways. This is a pure unsupervised task, and we use sequence encoders f SeqEn \u03b8 to build raw sequence into highdimensional fixed-length vectors, on which clustering is conducted. We collect a total of 7395 samples, sourced from 35 different players. For tokenizers, the white-space tokenizer splits the sequence by the white space. The BPE tokenizer is built on the same data where the model is pre-trained. The vocabulary size of both is set to 50,000.", "cite_spans": [], "section": "CONCLUSION", "sec_num": "10"}, {"text": "Regarding the training details of transformer encoders, the hidden size of the transformer, the intermediate size of the transformer, and the number of layers are set to 768, 768, 6 respectively; other hyperparameters are the same as the BERT-base [11] . The LongFormer shares the same hyperparameter with BERT except for the attention window sizes, which are set to 32 across all 6 layers. We set the reformer's axial pos value to 32, hidden size to 192, intermediate size to 192, and attention heads to 3. All transformers are trained", "cite_spans": [{"start": 248, "end": 252, "text": "[11]", "ref_id": "BIBREF10"}], "section": "Experimental details", "sec_num": "11.2"}, {"text": "https://venturebeat.com/2021/07/04/newzoo-game-market-will-hit-200b-in-2024", "cite_spans": [], "section": "", "sec_num": null}, {"text": "https://n.163.com", "cite_spans": [], "section": "", "sec_num": null}, {"text": "https://xqn.163.com", "cite_spans": [], "section": "", "sec_num": null}, {"text": "Code is available at https://github.com/iamlxb3/PMTC", "cite_spans": [], "section": "", "sec_num": null}, {"text": "https://ai.facebook.com/tools/fairseq/", "cite_spans": [], "section": "", "sec_num": null}, {"text": "Notably, due to the discrepancies between datasets, we only preserve baselines' structures related to the behavioral sequences.", "cite_spans": [], "section": "", "sec_num": null}, {"text": "We cannot disclose the game name due to the confidentiality requirement.", "cite_spans": [], "section": "", "sec_num": null}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "EVOLUTION OF THE WORLD WIDE WEB: FROM WEB 1.0 TO WEB 4.0", "authors": [{"first": "Sareh", "middle": [], "last": "Aghaei", "suffix": ""}, {"first": "Mohammad", "middle": [], "last": "Ali Nematbakhsh", "suffix": ""}, {"first": "Hadi", "middle": [], "last": "Khosravi", "suffix": ""}, {"first": "Farsani", "middle": [], "last": "", "suffix": ""}], "dblp_id": null, "year": 2012, "venue": "ternational Journal of Web & Semantic Technology (IJWesT)", "volume": "3", "issue": "", "pages": "", "other_ids": {"DOI": ["10.5121/ijwest.2012.3101"]}, "num": null, "urls": [], "raw_text": "Sareh Aghaei, Mohammad Ali Nematbakhsh, and Hadi Khosravi Farsani. 2012. EVOLUTION OF THE WORLD WIDE WEB: FROM WEB 1.0 TO WEB 4.0. In- ternational Journal of Web & Semantic Technology (IJWesT) 3, 1 (2012). https: //doi.org/10.5121/ijwest.2012.3101", "links": null}, "BIBREF1": {"ref_id": "b1", "title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning", "authors": [{"first": "Armen", "middle": [], "last": "Aghajanyan", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "Sonal", "middle": [], "last": "Gupta", "suffix": ""}], "dblp_id": "conf/acl/AghajanyanGZ20", "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2012.13255"]}, "num": null, "urls": [], "raw_text": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimen- sionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255 (2020).", "links": null}, "BIBREF2": {"ref_id": "b2", "title": "OPTICS: Ordering points to identify the clustering structure", "authors": [{"first": "Mihael", "middle": [], "last": "Ankerst", "suffix": ""}, {"first": "Markus", "middle": ["M"], "last": "Breunig", "suffix": ""}, {"first": "Hans-Peter", "middle": [], "last": "Kriegel", "suffix": ""}, {"first": "J\u00f6rg", "middle": [], "last": "Sander", "suffix": ""}], "dblp_id": "conf/sigmod/AnkerstBKS99", "year": 1999, "venue": "ACM Sigmod record", "volume": "28", "issue": "2", "pages": "49--60", "other_ids": {}, "num": null, "urls": [], "raw_text": "Mihael Ankerst, Markus M Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander. 1999. OPTICS: Ordering points to identify the clustering structure. ACM Sigmod record 28, 2 (1999), 49-60.", "links": null}, "BIBREF3": {"ref_id": "b3", "title": "A theoretical analysis of contrastive unsupervised representation learning", "authors": [{"first": "Sanjeev", "middle": [], "last": "Arora", "suffix": ""}, {"first": "Hrishikesh", "middle": [], "last": "Khandeparkar", "suffix": ""}, {"first": "Mikhail", "middle": [], "last": "Khodak", "suffix": ""}, {"first": "Orestis", "middle": [], "last": "Plevrakis", "suffix": ""}, {"first": "Nikunj", "middle": [], "last": "Saunshi", "suffix": ""}], "dblp_id": "conf/icml/SaunshiPAKK19", "year": 2019, "venue": "36th International Conference on Machine Learning, ICML 2019", "volume": "", "issue": "", "pages": "9904--9923", "other_ids": {}, "num": null, "urls": [], "raw_text": "Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. 2019. A theoretical analysis of contrastive unsupervised representation learning. In 36th International Conference on Machine Learning, ICML 2019. International Machine Learning Society (IMLS), 9904-9923.", "links": null}, "BIBREF4": {"ref_id": "b4", "title": "k-means++: The advantages of careful seeding", "authors": [{"first": "David", "middle": [], "last": "Arthur", "suffix": ""}, {"first": "Sergei", "middle": [], "last": "Vassilvitskii", "suffix": ""}], "dblp_id": "conf/soda/ArthurV07", "year": 2006, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "David Arthur and Sergei Vassilvitskii. 2006. k-means++: The advantages of careful seeding. Technical Report.", "links": null}, "BIBREF5": {"ref_id": "b5", "title": "Longformer: The longdocument transformer", "authors": [{"first": "Iz", "middle": [], "last": "Beltagy", "suffix": ""}, {"first": "Matthew", "middle": ["E"], "last": "Peters", "suffix": ""}, {"first": "Arman", "middle": [], "last": "Cohan", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2004.05150"]}, "num": null, "urls": [], "raw_text": "Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long- document transformer. arXiv preprint arXiv:2004.05150 (2020).", "links": null}, "BIBREF6": {"ref_id": "b6", "title": "Latent dirichlet allocation", "authors": [{"first": "Andrew", "middle": ["Y"], "last": "David M Blei", "suffix": ""}, {"first": "Michael", "middle": ["I"], "last": "Ng", "suffix": ""}, {"first": "", "middle": [], "last": "Jordan", "suffix": ""}], "dblp_id": "conf/nips/BleiNJ01", "year": 2003, "venue": "the Journal of machine Learning research", "volume": "3", "issue": "", "pages": "993--1022", "other_ids": {}, "num": null, "urls": [], "raw_text": "David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research 3 (2003), 993-1022.", "links": null}, "BIBREF7": {"ref_id": "b7", "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining", "authors": [{"first": "Kaj", "middle": [], "last": "Bostrom", "suffix": ""}, {"first": "Greg", "middle": [], "last": "Durrett", "suffix": ""}], "dblp_id": "conf/emnlp/BostromD20", "year": 2020, "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings", "volume": "", "issue": "", "pages": "4617--4624", "other_ids": {}, "num": null, "urls": [], "raw_text": "Kaj Bostrom and Greg Durrett. 2020. Byte Pair Encoding is Suboptimal for Language Model Pretraining. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 4617-4624.", "links": null}, "BIBREF8": {"ref_id": "b8", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": [{"first": "Kevin", "middle": [], "last": "Clark", "suffix": ""}, {"first": "Minh-Thang", "middle": [], "last": "Luong", "suffix": ""}, {"first": "Quoc V", "middle": [], "last": "Le", "suffix": ""}, {"first": "Christopher", "middle": ["D"], "last": "Manning", "suffix": ""}], "dblp_id": "conf/iclr/ClarkLLM20", "year": 2019, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning Representations.", "links": null}, "BIBREF9": {"ref_id": "b9", "title": "Personalized Bundle Recommendation in Online Games", "authors": [{"first": "Qilin", "middle": [], "last": "Deng", "suffix": ""}, {"first": "Kai", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Minghao", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Zhene", "middle": [], "last": "Zou", "suffix": ""}, {"first": "Runze", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Liang", "middle": [], "last": "Chen", "suffix": ""}], "dblp_id": "conf/cikm/DengWZZWTFC20", "year": 2020, "venue": "CIKM", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Qilin Deng, Kai Wang, Minghao Zhao, Zhene Zou, Runze Wu, Jianrong Tao, Changjie Fan, and Liang Chen. 2020. Personalized Bundle Recommendation in Online Games. In CIKM. 2381-2388.", "links": null}, "BIBREF10": {"ref_id": "b10", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": [{"first": "Jacob", "middle": [], "last": "Devlin", "suffix": ""}, {"first": "Ming-Wei", "middle": [], "last": "Chang", "suffix": ""}, {"first": "Kenton", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Kristina", "middle": [], "last": "Toutanova", "suffix": ""}], "dblp_id": "conf/naacl/DevlinCLT19", "year": 2019, "venue": "NAACL", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. 4171-4186.", "links": null}, "BIBREF11": {"ref_id": "b11", "title": "Player modeling using self-organization in Tomb Raider: Underworld", "authors": [{"first": "Anders", "middle": [], "last": "Drachen", "suffix": ""}, {"first": "Alessandro", "middle": [], "last": "Canossa", "suffix": ""}, {"first": "Georgios", "middle": ["N"], "last": "Yannakakis", "suffix": ""}], "dblp_id": "conf/cig/DrachenCY09", "year": 2009, "venue": "IEEE symposium on computational intelligence and games", "volume": "", "issue": "", "pages": "1--8", "other_ids": {}, "num": null, "urls": [], "raw_text": "Anders Drachen, Alessandro Canossa, and Georgios N Yannakakis. 2009. Player modeling using self-organization in Tomb Raider: Underworld. In 2009 IEEE symposium on computational intelligence and games. IEEE, 1-8.", "links": null}, "BIBREF12": {"ref_id": "b12", "title": "Game Action Modeling for Fine Grained Analyses of Player Behavior in Multi-player Card Games (Rummy as Case Study)", "authors": [{"first": "Mridul", "middle": [], "last": "Sharanya Eswaran", "suffix": ""}, {"first": "Vikram", "middle": [], "last": "Sachdeva", "suffix": ""}, {"first": "Deepanshi", "middle": [], "last": "Vimal", "suffix": ""}, {"first": "Suhaas", "middle": [], "last": "Seth", "suffix": ""}, {"first": "Sanjay", "middle": [], "last": "Kalpam", "suffix": ""}, {"first": "Tridib", "middle": [], "last": "Agarwal", "suffix": ""}, {"first": "Samrat", "middle": [], "last": "Mukherjee", "suffix": ""}, {"first": "", "middle": [], "last": "Dattagupta", "suffix": ""}], "dblp_id": "conf/kdd/EswaranSVSKAMD20", "year": 2020, "venue": "SIGKDD", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Sharanya Eswaran, Mridul Sachdeva, Vikram Vimal, Deepanshi Seth, Suhaas Kalpam, Sanjay Agarwal, Tridib Mukherjee, and Samrat Dattagupta. 2020. Game Action Modeling for Fine Grained Analyses of Player Behavior in Multi-player Card Games (Rummy as Case Study). In SIGKDD. 2657-2665.", "links": null}, "BIBREF13": {"ref_id": "b13", "title": "A new algorithm for data compression", "authors": [{"first": "Philip", "middle": [], "last": "Gage", "suffix": ""}], "dblp_id": null, "year": 1994, "venue": "C Users Journal", "volume": "12", "issue": "2", "pages": "23--38", "other_ids": {}, "num": null, "urls": [], "raw_text": "Philip Gage. 1994. A new algorithm for data compression. C Users Journal 12, 2 (1994), 23-38.", "links": null}, "BIBREF14": {"ref_id": "b14", "title": "A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation", "authors": [{"first": "Akhilesh", "middle": [], "last": "Gotmare", "suffix": ""}, {"first": "Nitish", "middle": [], "last": "Shirish Keskar", "suffix": ""}, {"first": "Caiming", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Socher", "suffix": ""}], "dblp_id": "conf/iclr/GotmareKXS19", "year": 2018, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1810.13243"]}, "num": null, "urls": [], "raw_text": "Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243 (2018).", "links": null}, "BIBREF15": {"ref_id": "b15", "title": "A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation", "authors": [{"first": "Cyril", "middle": [], "last": "Goutte", "suffix": ""}, {"first": "Eric", "middle": [], "last": "Gaussier", "suffix": ""}], "dblp_id": null, "year": null, "venue": "Advances in Information Retrieval", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Cyril Goutte and Eric Gaussier. [n.d.]. A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation. In Advances in Information Retrieval.", "links": null}, "BIBREF16": {"ref_id": "b16", "title": "Learning deep representations by mutual information estimation and maximization", "authors": [{"first": "Devon", "middle": [], "last": "Hjelm", "suffix": ""}, {"first": "Alex", "middle": [], "last": "Fedorov", "suffix": ""}, {"first": "Samuel", "middle": [], "last": "Lavoie-Marchildon", "suffix": ""}, {"first": "Karan", "middle": [], "last": "Grewal", "suffix": ""}, {"first": "Phil", "middle": [], "last": "Bachman", "suffix": ""}, {"first": "Adam", "middle": [], "last": "Trischler", "suffix": ""}, {"first": "Yoshua", "middle": [], "last": "Bengio", "suffix": ""}], "dblp_id": "conf/iclr/HjelmFLGBTB19", "year": 2018, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen- tations by mutual information estimation and maximization. In International Conference on Learning Representations.", "links": null}, "BIBREF17": {"ref_id": "b17", "title": "Generative agents for player decision modeling in games", "authors": [{"first": "Christoffer", "middle": [], "last": "Holmgard", "suffix": ""}, {"first": "Antonios", "middle": [], "last": "Liapis", "suffix": ""}, {"first": "Julian", "middle": [], "last": "Togelius", "suffix": ""}, {"first": "Georgios", "middle": ["N"], "last": "Yannakakis", "suffix": ""}], "dblp_id": "conf/fdg/HolmgardLTY14", "year": 2014, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Christoffer Holmgard, Antonios Liapis, Julian Togelius, and Georgios N Yan- nakakis. 2014. Generative agents for player decision modeling in games. (2014).", "links": null}, "BIBREF18": {"ref_id": "b18", "title": "Big code!= big vocabulary: Open-vocabulary models for source code", "authors": [{"first": "Rafael-Michael", "middle": [], "last": "Karampatsis", "suffix": ""}, {"first": "Hlib", "middle": [], "last": "Babii", "suffix": ""}, {"first": "Romain", "middle": [], "last": "Robbes", "suffix": ""}, {"first": "Charles", "middle": [], "last": "Sutton", "suffix": ""}, {"first": "Andrea", "middle": [], "last": "Janes", "suffix": ""}], "dblp_id": "conf/icse/KarampatsisBRSJ20", "year": 2020, "venue": "IEEE/ACM 42nd International Conference on Software Engineering (ICSE)", "volume": "", "issue": "", "pages": "1073--1085", "other_ids": {}, "num": null, "urls": [], "raw_text": "Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and An- drea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for source code. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEE, 1073-1085.", "links": null}, "BIBREF19": {"ref_id": "b19", "title": "T test as a parametric statistic", "authors": [{"first": "Tae", "middle": [], "last": "Kyun", "suffix": ""}, {"first": "Kim", "middle": [], "last": "", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "Korean journal of anesthesiology", "volume": "68", "issue": "6", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Tae Kyun Kim. 2015. T test as a parametric statistic. Korean journal of anesthesi- ology 68, 6 (2015), 540.", "links": null}, "BIBREF20": {"ref_id": "b20", "title": "Adam: A method for stochastic optimization", "authors": [{"first": "P", "middle": [], "last": "Diederik", "suffix": ""}, {"first": "Jimmy", "middle": [], "last": "Kingma", "suffix": ""}, {"first": "", "middle": [], "last": "Ba", "suffix": ""}], "dblp_id": "journals/corr/KingmaB14", "year": 2014, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1412.6980"]}, "num": null, "urls": [], "raw_text": "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).", "links": null}, "BIBREF21": {"ref_id": "b21", "title": "Reformer: The Efficient Transformer", "authors": [{"first": "Nikita", "middle": [], "last": "Kitaev", "suffix": ""}, {"first": "Lukasz", "middle": [], "last": "Kaiser", "suffix": ""}, {"first": "Anselm", "middle": [], "last": "Levskaya", "suffix": ""}], "dblp_id": "conf/iclr/KitaevKL20", "year": 2019, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2019. Reformer: The Efficient Transformer. In International Conference on Learning Representations.", "links": null}, "BIBREF22": {"ref_id": "b22", "title": "Distributed representations of sentences and documents", "authors": [{"first": "Quoc", "middle": [], "last": "Le", "suffix": ""}, {"first": "Tomas", "middle": [], "last": "Mikolov", "suffix": ""}], "dblp_id": "conf/icml/LeM14", "year": 2014, "venue": "International conference on machine learning", "volume": "", "issue": "", "pages": "1188--1196", "other_ids": {}, "num": null, "urls": [], "raw_text": "Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International conference on machine learning. PMLR, 1188-1196.", "links": null}, "BIBREF23": {"ref_id": "b23", "title": "Game data mining competition on churn prediction and survival analysis using commercial game log data", "authors": [{"first": "Eunjo", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Yoonjae", "middle": [], "last": "Jang", "suffix": ""}, {"first": "Dumim", "middle": [], "last": "Yoon", "suffix": ""}, {"first": "Jihoon", "middle": [], "last": "Jeon", "suffix": ""}, {"first": "Seong-Il", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Sang-Kwang", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Dae-Wook", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Pei", "middle": [], "last": "Pei Chen", "suffix": ""}, {"first": "Anna", "middle": [], "last": "Guitart", "suffix": ""}, {"first": "Paul", "middle": [], "last": "Bertens", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:1802.02301"]}, "num": null, "urls": [], "raw_text": "Eunjo Lee, Yoonjae Jang, DuMim Yoon, JiHoon Jeon, Seong-il Yang, Sang-Kwang Lee, Dae-Wook Kim, Pei Pei Chen, Anna Guitart, Paul Bertens, et al. 2018. Game data mining competition on churn prediction and survival analysis using com- mercial game log data. arXiv preprint arXiv:1802.02301 (2018).", "links": null}, "BIBREF24": {"ref_id": "b24", "title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining", "authors": [{"first": "Jinhyuk", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Wonjin", "middle": [], "last": "Yoon", "suffix": ""}, {"first": "Sungdong", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Donghyeon", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Sunkyu", "middle": [], "last": "Kim", "suffix": ""}, {"first": "Chan", "middle": [], "last": "Ho", "suffix": ""}, {"first": "So", "middle": [], "last": "", "suffix": ""}, {"first": "Jaewoo", "middle": [], "last": "Kang", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "Bioinformatics", "volume": "36", "issue": "4", "pages": "1234--1240", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: A pre-trained biomedical lan- guage representation model for biomedical text mining. Bioinformatics 36, 4 (2020), 1234-1240.", "links": null}, "BIBREF25": {"ref_id": "b25", "title": "Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes", "authors": [{"first": "Yuhong", "middle": [], "last": "Li", "suffix": ""}, {"first": "Xiaofan", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Deming", "middle": [], "last": "Chen", "suffix": ""}], "dblp_id": "conf/cvpr/LiZC18", "year": 2018, "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition", "volume": "", "issue": "", "pages": "1091--1100", "other_ids": {}, "num": null, "urls": [], "raw_text": "Yuhong Li, Xiaofan Zhang, and Deming Chen. 2018. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1091-1100.", "links": null}, "BIBREF26": {"ref_id": "b26", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "authors": [{"first": "Yinhan", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Myle", "middle": [], "last": "Ott", "suffix": ""}, {"first": "Naman", "middle": [], "last": "Goyal", "suffix": ""}, {"first": "Jingfei", "middle": [], "last": "Du", "suffix": ""}, {"first": "Mandar", "middle": [], "last": "Joshi", "suffix": ""}, {"first": "Danqi", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Omer", "middle": [], "last": "Levy", "suffix": ""}, {"first": "Mike", "middle": [], "last": "Lewis", "suffix": ""}, {"first": "Luke", "middle": [], "last": "Zettlemoyer", "suffix": ""}, {"first": "Veselin", "middle": [], "last": "Stoyanov", "suffix": ""}], "dblp_id": null, "year": 2019, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. (2019).", "links": null}, "BIBREF27": {"ref_id": "b27", "title": "dom2vec: Unsupervised protein domain embeddings capture domains structure and function providing data-driven insights into collocations in domain architectures", "authors": [{"first": "Brandon", "middle": [], "last": "Damianos P Melidis", "suffix": ""}, {"first": "Wolfgang", "middle": [], "last": "Malone", "suffix": ""}, {"first": "", "middle": [], "last": "Nejdl", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "bioRxiv", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Damianos P Melidis, Brandon Malone, and Wolfgang Nejdl. 2020. dom2vec: Un- supervised protein domain embeddings capture domains structure and function providing data-driven insights into collocations in domain architectures. bioRxiv (2020).", "links": null}, "BIBREF28": {"ref_id": "b28", "title": "Time-Aware User Embeddings as a Service", "authors": [{"first": "Martin", "middle": [], "last": "Pavlovski", "suffix": ""}, {"first": "Jelena", "middle": [], "last": "Gligorijevic", "suffix": ""}, {"first": "Ivan", "middle": [], "last": "Stojkovic", "suffix": ""}, {"first": "Shubham", "middle": [], "last": "Agrawal", "suffix": ""}, {"first": "Shabhareesh", "middle": [], "last": "Komirishetty", "suffix": ""}, {"first": "Djordje", "middle": [], "last": "Gligorijevic", "suffix": ""}, {"first": "Narayan", "middle": [], "last": "Bhamidipati", "suffix": ""}, {"first": "Zoran", "middle": [], "last": "Obradovic", "suffix": ""}], "dblp_id": "conf/kdd/PavlovskiGSAKGB20", "year": 2020, "venue": "SIGKDD", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Martin Pavlovski, Jelena Gligorijevic, Ivan Stojkovic, Shubham Agrawal, Shab- hareesh Komirishetty, Djordje Gligorijevic, Narayan Bhamidipati, and Zoran Obradovic. 2020. Time-Aware User Embeddings as a Service. In SIGKDD. 3194- 3202.", "links": null}, "BIBREF29": {"ref_id": "b29", "title": "Towards deep player behavior models in mmorpgs", "authors": [{"first": "Johannes", "middle": [], "last": "Pfau", "suffix": ""}, {"first": "Jan", "middle": ["David"], "last": "Smeddinck", "suffix": ""}, {"first": "Rainer", "middle": [], "last": "Malaka", "suffix": ""}], "dblp_id": "conf/chiplay/PfauSM18", "year": 2018, "venue": "Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play", "volume": "", "issue": "", "pages": "381--392", "other_ids": {}, "num": null, "urls": [], "raw_text": "Johannes Pfau, Jan David Smeddinck, and Rainer Malaka. 2018. Towards deep player behavior models in mmorpgs. In Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play. 381-392.", "links": null}, "BIBREF30": {"ref_id": "b30", "title": "Imagebert: Cross-modal pre-training with large-scale weak-supervised imagetext data", "authors": [{"first": "Di", "middle": [], "last": "Qi", "suffix": ""}, {"first": "Lin", "middle": [], "last": "Su", "suffix": ""}, {"first": "Jia", "middle": [], "last": "Song", "suffix": ""}, {"first": "Edward", "middle": [], "last": "Cui", "suffix": ""}, {"first": "Taroon", "middle": [], "last": "Bharti", "suffix": ""}, {"first": "Arun", "middle": [], "last": "Sacheti", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {"arXiv": ["arXiv:2001.07966"]}, "num": null, "urls": [], "raw_text": "Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. 2020. Imagebert: Cross-modal pre-training with large-scale weak-supervised image- text data. arXiv preprint arXiv:2001.07966 (2020).", "links": null}, "BIBREF31": {"ref_id": "b31", "title": "Language Models are Unsupervised Multitask Learners", "authors": [{"first": "Alec", "middle": [], "last": "Radford", "suffix": ""}, {"first": "Jeffrey", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Rewon", "middle": [], "last": "Child", "suffix": ""}, {"first": "David", "middle": [], "last": "Luan", "suffix": ""}, {"first": "Dario", "middle": [], "last": "Amodei", "suffix": ""}, {"first": "Ilya", "middle": [], "last": "Sutskever", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language Models are Unsupervised Multitask Learners. (2018). https://d4mucfpksywv.cloudfront.net/better-language-models/language- models.pdf", "links": null}, "BIBREF32": {"ref_id": "b32", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": [{"first": "Colin", "middle": [], "last": "Raffel", "suffix": ""}, {"first": "Noam", "middle": [], "last": "Shazeer", "suffix": ""}, {"first": "Adam", "middle": [], "last": "Roberts", "suffix": ""}, {"first": "Katherine", "middle": [], "last": "Lee", "suffix": ""}, {"first": "Sharan", "middle": [], "last": "Narang", "suffix": ""}, {"first": "Michael", "middle": [], "last": "Matena", "suffix": ""}, {"first": "Yanqi", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Wei", "middle": [], "last": "Li", "suffix": ""}, {"first": "Peter", "middle": ["J"], "last": "Liu", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "Journal of Machine Learning Research", "volume": "21", "issue": "", "pages": "1--67", "other_ids": {}, "num": null, "urls": [], "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21 (2020), 1-67.", "links": null}, "BIBREF33": {"ref_id": "b33", "title": "Using tf-idf to determine word relevance in document queries", "authors": [{"first": "Juan", "middle": [], "last": "Ramos", "suffix": ""}], "dblp_id": null, "year": 2003, "venue": "Proceedings of the first instructional conference on machine learning", "volume": "242", "issue": "", "pages": "29--48", "other_ids": {}, "num": null, "urls": [], "raw_text": "Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, Vol. 242. Citeseer, 29-48.", "links": null}, "BIBREF34": {"ref_id": "b34", "title": "Neural Machine Translation of Rare Words with Subword Units", "authors": [{"first": "Rico", "middle": [], "last": "Sennrich", "suffix": ""}, {"first": "Barry", "middle": [], "last": "Haddow", "suffix": ""}, {"first": "Alexandra", "middle": [], "last": "Birch", "suffix": ""}], "dblp_id": "conf/acl/SennrichHB16a", "year": 2016, "venue": "ACL", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In ACL. 1715-1725.", "links": null}, "BIBREF35": {"ref_id": "b35", "title": "Analyzing human behavior from multiplayer online game logs: A knowledge discovery approach", "authors": [{"first": "Jin", "middle": [], "last": "Kyong", "suffix": ""}, {"first": "Nishith", "middle": [], "last": "Shim", "suffix": ""}, {"first": "Muhammad", "middle": ["A"], "last": "Pathak", "suffix": ""}, {"first": "Colin", "middle": [], "last": "Ahmad", "suffix": ""}, {"first": "Zoheb", "middle": [], "last": "Delong", "suffix": ""}, {"first": "Amogh", "middle": [], "last": "Borbora", "suffix": ""}, {"first": "Jaideep", "middle": [], "last": "Mahapatra", "suffix": ""}, {"first": "", "middle": [], "last": "Srivastava", "suffix": ""}], "dblp_id": null, "year": 2011, "venue": "IEEE Intelligent Systems", "volume": "26", "issue": "", "pages": "85--89", "other_ids": {}, "num": null, "urls": [], "raw_text": "Kyong Jin Shim, Nishith Pathak, Muhammad A Ahmad, Colin DeLong, Zoheb Borbora, Amogh Mahapatra, and Jaideep Srivastava. 2011. Analyzing human behavior from multiplayer online game logs: A knowledge discovery approach. In IEEE Intelligent Systems, Vol. 26. IEEE, 85-89.", "links": null}, "BIBREF36": {"ref_id": "b36", "title": "An inclusive taxonomy of player modeling", "authors": [{"first": "M", "middle": [], "last": "Adam", "suffix": ""}, {"first": "Chris", "middle": [], "last": "Smith", "suffix": ""}, {"first": "Kenneth", "middle": [], "last": "Lewis", "suffix": ""}, {"first": "Gillian", "middle": [], "last": "Hullett", "suffix": ""}, {"first": "Anne", "middle": [], "last": "Smith", "suffix": ""}, {"first": "", "middle": [], "last": "Sullivan", "suffix": ""}], "dblp_id": null, "year": 2011, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Adam M Smith, Chris Lewis, Kenneth Hullett, Gillian Smith, and Anne Sullivan. 2011. An inclusive taxonomy of player modeling. University of California, Santa Cruz, Tech. Rep. UCSC-SOE-11-13 (2011).", "links": null}, "BIBREF37": {"ref_id": "b37", "title": "The Variance of the Adjusted Rand Index", "authors": [{"first": "Douglas", "middle": [], "last": "Steinley", "suffix": ""}, {"first": "Michael", "middle": ["J"], "last": "Brusco", "suffix": ""}, {"first": "Lawrence", "middle": [], "last": "Hubert", "suffix": ""}], "dblp_id": null, "year": 2016, "venue": "Psychological methods", "volume": "21", "issue": "2", "pages": "261--272", "other_ids": {}, "num": null, "urls": [], "raw_text": "Douglas Steinley, Michael J Brusco, and Lawrence Hubert. 2016. The Variance of the Adjusted Rand Index. Psychological methods 21, 2 (2016), 261-272.", "links": null}, "BIBREF38": {"ref_id": "b38", "title": "Ernie 2.0: A continual pre-training framework for language understanding", "authors": [{"first": "Yu", "middle": [], "last": "Sun", "suffix": ""}, {"first": "Shuohuan", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Yukun", "middle": [], "last": "Li", "suffix": ""}, {"first": "Shikun", "middle": [], "last": "Feng", "suffix": ""}, {"first": "Hua", "middle": [], "last": "Hao Tian", "suffix": ""}, {"first": "Haifeng", "middle": [], "last": "Wu", "suffix": ""}, {"first": "", "middle": [], "last": "Wang", "suffix": ""}], "dblp_id": "conf/aaai/SunWLFTWW20", "year": 2020, "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "34", "issue": "", "pages": "8968--8975", "other_ids": {}, "num": null, "urls": [], "raw_text": "Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A continual pre-training framework for language under- standing. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8968-8975.", "links": null}, "BIBREF39": {"ref_id": "b39", "title": "MMORPG player behavior model based on player action categories", "authors": [{"first": "Mirko", "middle": [], "last": "Suznjevic", "suffix": ""}, {"first": "Ivana", "middle": [], "last": "Stupar", "suffix": ""}, {"first": "Maja", "middle": [], "last": "Matijasevic", "suffix": ""}], "dblp_id": "conf/netgames/SuznjevicSM11", "year": 2011, "venue": "10th Annual Workshop on Network and Systems Support for Games", "volume": "", "issue": "", "pages": "1--6", "other_ids": {}, "num": null, "urls": [], "raw_text": "Mirko Suznjevic, Ivana Stupar, and Maja Matijasevic. 2011. MMORPG player behavior model based on player action categories. In 2011 10th Annual Workshop on Network and Systems Support for Games. IEEE, 1-6.", "links": null}, "BIBREF40": {"ref_id": "b40", "title": "Mvan: Multi-view attention networks for real money trading detection in online games", "authors": [{"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Jianshi", "middle": [], "last": "Lin", "suffix": ""}, {"first": "Shize", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "Sha", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Runze", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Peng", "middle": [], "last": "Cui", "suffix": ""}], "dblp_id": "conf/kdd/TaoLZZWFC19", "year": 2019, "venue": "SIGKDD", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jianrong Tao, Jianshi Lin, Shize Zhang, Sha Zhao, Runze Wu, Changjie Fan, and Peng Cui. 2019. Mvan: Multi-view attention networks for real money trading detection in online games. In SIGKDD. 2536-2546.", "links": null}, "BIBREF41": {"ref_id": "b41", "title": "NGUARD: A Game Bot Detection Framework for NetEase MMORPGs", "authors": [{"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Jiarong", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Linxia", "middle": [], "last": "Gong", "suffix": ""}, {"first": "Yifu", "middle": [], "last": "Li", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Zhou", "middle": [], "last": "Zhao", "suffix": ""}], "dblp_id": "conf/kdd/TaoXGLFZ18", "year": 2018, "venue": "SIGKDD", "volume": "", "issue": "", "pages": "811--820", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jianrong Tao, Jiarong Xu, Linxia Gong, Yifu Li, Changjie Fan, and Zhou Zhao. 2018. NGUARD: A Game Bot Detection Framework for NetEase MMORPGs. In SIGKDD. 811-820.", "links": null}, "BIBREF42": {"ref_id": "b42", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "authors": [{"first": "Yi", "middle": [], "last": "Tay", "suffix": ""}, {"first": "Mostafa", "middle": [], "last": "Dehghani", "suffix": ""}, {"first": "Samira", "middle": [], "last": "Abnar", "suffix": ""}, {"first": "Yikang", "middle": [], "last": "Shen", "suffix": ""}, {"first": "Dara", "middle": [], "last": "Bahri", "suffix": ""}, {"first": "Philip", "middle": [], "last": "Pham", "suffix": ""}, {"first": "Jinfeng", "middle": [], "last": "Rao", "suffix": ""}, {"first": "Liu", "middle": [], "last": "Yang", "suffix": ""}, {"first": "Sebastian", "middle": [], "last": "Ruder", "suffix": ""}, {"first": "Donald", "middle": [], "last": "Metzler", "suffix": ""}], "dblp_id": "conf/iclr/Tay0ASBPRYRM21", "year": 2020, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. Long Range Arena: A Benchmark for Efficient Transformers. In International Conference on Learning Representations.", "links": null}, "BIBREF43": {"ref_id": "b43", "title": "Modeling MMORPG Players' Behaviour", "authors": [{"first": "Bogdan", "middle": [], "last": "Okresa", "suffix": ""}, {"first": "\u00d0uric", "middle": [], "last": "", "suffix": ""}, {"first": "Mario", "middle": [], "last": "Konecki", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "Central European Conference on Information and Intelligent Systems", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Bogdan Okresa \u00d0uric and Mario Konecki. 2015. Modeling MMORPG Players' Behaviour. In Central European Conference on Information and Intelligent Systems. Faculty of Organization and Informatics Varazdin, 177.", "links": null}, "BIBREF44": {"ref_id": "b44", "title": "Attention is All you Need", "authors": [{"first": "Ashish", "middle": [], "last": "Vaswani", "suffix": ""}, {"first": "Noam", "middle": [], "last": "Shazeer", "suffix": ""}, {"first": "Niki", "middle": [], "last": "Parmar", "suffix": ""}, {"first": "Jakob", "middle": [], "last": "Uszkoreit", "suffix": ""}, {"first": "Llion", "middle": [], "last": "Jones", "suffix": ""}, {"first": "Aidan", "middle": ["N"], "last": "Gomez", "suffix": ""}, {"first": "Lukasz", "middle": [], "last": "Kaiser", "suffix": ""}, {"first": "Illia", "middle": [], "last": "Polosukhin", "suffix": ""}], "dblp_id": "conf/nips/VaswaniSPUJGKP17", "year": 2017, "venue": "NeurIPS", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS.", "links": null}, "BIBREF45": {"ref_id": "b45", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "authors": [{"first": "Alex", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Amanpreet", "middle": [], "last": "Singh", "suffix": ""}, {"first": "Julian", "middle": [], "last": "Michael", "suffix": ""}, {"first": "Felix", "middle": [], "last": "Hill", "suffix": ""}, {"first": "Omer", "middle": [], "last": "Levy", "suffix": ""}, {"first": "Samuel", "middle": [], "last": "Bowman", "suffix": ""}], "dblp_id": "conf/iclr/WangSMHLB19", "year": 2018, "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP", "volume": "", "issue": "", "pages": "353--355", "other_ids": {}, "num": null, "urls": [], "raw_text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 353-355.", "links": null}, "BIBREF46": {"ref_id": "b46", "title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": [{"first": "Wei", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Bin", "middle": [], "last": "Bi", "suffix": ""}, {"first": "Ming", "middle": [], "last": "Yan", "suffix": ""}, {"first": "Chen", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Jiangnan", "middle": [], "last": "Xia", "suffix": ""}, {"first": "Zuyi", "middle": [], "last": "Bao", "suffix": ""}, {"first": "Liwei", "middle": [], "last": "Peng", "suffix": ""}, {"first": "Luo", "middle": [], "last": "Si", "suffix": ""}], "dblp_id": "conf/iclr/0225BYWXBPS20", "year": 2019, "venue": "International Conference on Learning Representations", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. 2019. StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding. In International Conference on Learning Rep- resentations.", "links": null}, "BIBREF47": {"ref_id": "b47", "title": "An Investigation of Phone-Based Subword Units for End-to-End Speech Recognition", "authors": [{"first": "Weiran", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Guangsen", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Aadyot", "middle": [], "last": "Bhatnagar", "suffix": ""}, {"first": "Yingbo", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "Caiming", "middle": [], "last": "Xiong", "suffix": ""}, {"first": "Richard", "middle": [], "last": "Socher", "suffix": ""}], "dblp_id": "conf/interspeech/WangWBZXS20", "year": 2020, "venue": "Proc. Interspeech 2020", "volume": "", "issue": "", "pages": "1778--1782", "other_ids": {}, "num": null, "urls": [], "raw_text": "Weiran Wang, Guangsen Wang, Aadyot Bhatnagar, Yingbo Zhou, Caiming Xiong, and Richard Socher. 2020. An Investigation of Phone-Based Subword Units for End-to-End Speech Recognition. Proc. Interspeech 2020 (2020), 1778-1782.", "links": null}, "BIBREF48": {"ref_id": "b48", "title": "Deep Behavior Tracing with Multi-level Temporality Preserved Embedding", "authors": [{"first": "Runze", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Hao", "middle": [], "last": "Deng", "suffix": ""}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Qi", "middle": [], "last": "Liu", "suffix": ""}, {"first": "Liang", "middle": [], "last": "Chen", "suffix": ""}], "dblp_id": "conf/cikm/WuDTF0C20", "year": 2020, "venue": "CIKM", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Runze Wu, Hao Deng, Jianrong Tao, Changjie Fan, Qi Liu, and Liang Chen. 2020. Deep Behavior Tracing with Multi-level Temporality Preserved Embedding. In CIKM. 2813-2820.", "links": null}, "BIBREF49": {"ref_id": "b49", "title": "NGUARD+ An Attention-based Game Bot Detection Framework via Player Behavior Sequences", "authors": [{"first": "Jiarong", "middle": [], "last": "Xu", "suffix": ""}, {"first": "Yifan", "middle": [], "last": "Luo", "suffix": ""}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Zhou", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Jiangang", "middle": [], "last": "Lu", "suffix": ""}], "dblp_id": null, "year": 2020, "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)", "volume": "14", "issue": "6", "pages": "1--24", "other_ids": {}, "num": null, "urls": [], "raw_text": "Jiarong Xu, Yifan Luo, Jianrong Tao, Changjie Fan, Zhou Zhao, and Jiangang Lu. 2020. NGUARD+ An Attention-based Game Bot Detection Framework via Player Behavior Sequences. ACM Transactions on Knowledge Discovery from Data (TKDD) 14, 6 (2020), 1-24.", "links": null}, "BIBREF50": {"ref_id": "b50", "title": "Player modeling", "authors": [{"first": "Pieter", "middle": [], "last": "Georgios N Yannakakis", "suffix": ""}, {"first": "Daniele", "middle": [], "last": "Spronck", "suffix": ""}, {"first": "Elisabeth", "middle": [], "last": "Loiacono", "suffix": ""}, {"first": "", "middle": [], "last": "Andr\u00e9", "suffix": ""}], "dblp_id": null, "year": 2013, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Georgios N Yannakakis, Pieter Spronck, Daniele Loiacono, and Elisabeth Andr\u00e9. 2013. Player modeling. (2013).", "links": null}, "BIBREF51": {"ref_id": "b51", "title": "Customer churn prediction in the online new media platform: a case study on juzi entertainment", "authors": [{"first": "Sha", "middle": [], "last": "Yuan", "suffix": ""}, {"first": "Shuotian", "middle": [], "last": "Bai", "suffix": ""}, {"first": "Mengmeng", "middle": [], "last": "Song", "suffix": ""}, {"first": "Zhenyu", "middle": [], "last": "Zhou", "suffix": ""}], "dblp_id": null, "year": 2017, "venue": "2017 International Conference on Platform Technology and Service (PlatCon)", "volume": "", "issue": "", "pages": "1--5", "other_ids": {}, "num": null, "urls": [], "raw_text": "Sha Yuan, Shuotian Bai, Mengmeng Song, and Zhenyu Zhou. 2017. Customer churn prediction in the online new media platform: a case study on juzi enter- tainment. In 2017 International Conference on Platform Technology and Service (PlatCon). IEEE, 1-5.", "links": null}, "BIBREF52": {"ref_id": "b52", "title": "Multi-source Data Multi-task Learning for Profiling Players in Online Games", "authors": [{"first": "Shiwei", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Runze", "middle": [], "last": "Wu", "suffix": ""}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Manhu", "middle": [], "last": "Qu", "suffix": ""}, {"first": "Hao", "middle": [], "last": "Li", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}], "dblp_id": "conf/cig/ZhaoWTQLF20", "year": 2020, "venue": "2020 IEEE Conference on Games (CoG)", "volume": "", "issue": "", "pages": "104--111", "other_ids": {}, "num": null, "urls": [], "raw_text": "Shiwei Zhao, Runze Wu, Jianrong Tao, Manhu Qu, Hao Li, and Changjie Fan. 2020. Multi-source Data Multi-task Learning for Profiling Players in Online Games. In 2020 IEEE Conference on Games (CoG). IEEE, 104-111.", "links": null}, "BIBREF53": {"ref_id": "b53", "title": "Keep You from Leaving: Churn Prediction in Online Games", "authors": [{"first": "Angyu", "middle": [], "last": "Zheng", "suffix": ""}, {"first": "Liang", "middle": [], "last": "Chen", "suffix": ""}, {"first": "Fenfang", "middle": [], "last": "Xie", "suffix": ""}, {"first": "Jianrong", "middle": [], "last": "Tao", "suffix": ""}, {"first": "Changjie", "middle": [], "last": "Fan", "suffix": ""}, {"first": "Zibin", "middle": [], "last": "Zheng", "suffix": ""}], "dblp_id": "conf/dasfaa/ZhengCXTFZ20", "year": 2020, "venue": "International Conference on Database Systems for Advanced Applications", "volume": "", "issue": "", "pages": "263--279", "other_ids": {}, "num": null, "urls": [], "raw_text": "Angyu Zheng, Liang Chen, Fenfang Xie, Jianrong Tao, Changjie Fan, and Zibin Zheng. 2020. Keep You from Leaving: Churn Prediction in Online Games. In International Conference on Database Systems for Advanced Applications. Springer, 263-279.", "links": null}}}, "ner": [{"syntactic": ["mmorpg", "data cleaning", "language model", "recommendation", "training sample", "role-playing game", "massively multiplayer", "compression methods", "gaming experiences", "engineering"], "semantic": ["mmorpg", "online learning", "detection algorithm", "computer games", "gameplay", "personalized recommendation", "language model", "recommendation", "role-playing game", "videogames", "massively multiplayer", "engineering", "integrated data"], "union": ["computer games", "gameplay", "language model", "recommendation", "compression methods", "integrated data", "mmorpg", "online learning", "detection algorithm", "data cleaning", "personalized recommendation", "training sample", "role-playing game", "massively multiplayer", "videogames", "gaming experiences", "engineering"], "enhanced": ["game theory", "interactive computer graphics", "game design", "computational linguistics", "recommender systems", "data compression", "data integration", "internet", "online systems", "signal detection", "data warehouses", "machine learning", "massively multi-player online games", "virtual worlds", "human computer interaction"], "Metrics": [], "ProgLang": [], "Dataset": [], "MathTerm": [], "IT Framework": [], "ISO": [], "Technology": ["bot"], "Terms": ["supervised", "unlabeled", "MMORPG", "unsupervised", "MTC"], "TechName": []}]}