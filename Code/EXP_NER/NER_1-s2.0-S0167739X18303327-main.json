{"paper_id": "1-s2", "header": {"generated_with": "S2ORC 1.0.0", "date_generated": "2024-03-20T17:52:51.101159Z"}, "title": "Scheduling Internet of Things requests to minimize latency in hybrid Fog-Cloud computing", "authors": [{"first": "Raafat", "middle": ["O"], "last": "Aburukba", "suffix": "", "affiliation": {"laboratory": "", "institution": "American University of Sharjah", "location": {"postBox": "PO Box", "postCode": "26666", "settlement": "Sharjah", "country": "United Arab Emirates"}}, "email": "raburukba@aus.edu"}, {"first": "Mazin", "middle": [], "last": "Alikarrar", "suffix": "", "affiliation": {"laboratory": "", "institution": "American University of Sharjah", "location": {"postBox": "PO Box", "postCode": "26666", "settlement": "Sharjah", "country": "United Arab Emirates"}}, "email": ""}, {"first": "Taha", "middle": [], "last": "Landolsi", "suffix": "", "affiliation": {"laboratory": "", "institution": "American University of Sharjah", "location": {"postBox": "PO Box", "postCode": "26666", "settlement": "Sharjah", "country": "United Arab Emirates"}}, "email": ""}, {"first": "Khaled", "middle": [], "last": "El-Fakih", "suffix": "", "affiliation": {"laboratory": "", "institution": "American University of Sharjah", "location": {"postBox": "PO Box", "postCode": "26666", "settlement": "Sharjah", "country": "United Arab Emirates"}}, "email": ""}], "year": "", "venue": null, "identifiers": {}, "abstract": "Delivering services for Internet of Things (IoT) applications that demand real-time and predictable latency is a challenge. Several IoT applications require stringent latency requirements due to the interaction between the IoT devices and the physical environment through sensing and actuation. The limited capabilities of IoT devices require applications to be integrated in Cloud and Fog computing paradigms. Fog computing significantly improves on the service latency as it brings resources closer to the edge. The characteristics of both Fog and Cloud computing will enable the integration and interoperation of a large number of IoT devices and services in different domains.\nThis work models the scheduling of IoT service requests as an optimization problem using integer programming in order to minimize the overall service request latency. The scheduling problem by nature is NP-hard, and hence, exact optimization solutions are inadequate for large size problems. This work introduces a customized implementation of the genetic algorithm (GA) as a heuristic approach to schedule the IoT requests to achieve the objective of minimizing the overall latency. The GA is tested in a simulation environment that considers the dynamic nature of the environment. The performance of the GA is evaluated and compared to the performance of waited-fair queuing (WFQ), priority-strict queuing (PSQ), and round robin (RR) techniques. The results show that the overall latency for the proposed approach is 21.9% to 46.6% better than the other algorithms. The proposed approach also showed significant improvement in meeting the requests deadlines by up to 31%.", "pdf_parse": {"abstract": [{"text": "Delivering services for Internet of Things (IoT) applications that demand real-time and predictable latency is a challenge. Several IoT applications require stringent latency requirements due to the interaction between the IoT devices and the physical environment through sensing and actuation. The limited capabilities of IoT devices require applications to be integrated in Cloud and Fog computing paradigms. Fog computing significantly improves on the service latency as it brings resources closer to the edge. The characteristics of both Fog and Cloud computing will enable the integration and interoperation of a large number of IoT devices and services in different domains.RMSE  confusion matrix false negative", "cite_spans": [], "section": "Abstract", "sec_num": null}, {"text": "This work models the scheduling of IoT service requests as an optimization problem using integer programming in order to minimize the overall service request latency. The scheduling problem by nature is NP-hard, and hence, exact optimization solutions are inadequate for large size problems. This work introduces a customized implementation of the genetic algorithm (GA) as a heuristic approach to schedule the IoT requests to achieve the objective of minimizing the overall latency. The GA is tested in a simulation environment that considers the dynamic nature of the environment. The performance of the GA is evaluated and compared to the performance of waited-fair queuing (WFQ), priority-strict queuing (PSQ), and round robin (RR) techniques. The results show that the overall latency for the proposed approach is 21.9% to 46.6% better than the other algorithms. The proposed approach also showed significant improvement in meeting the requests deadlines by up to 31%.", "cite_spans": [], "section": "Abstract", "sec_num": null}], "body_text": [{"text": "Cisco introduced the concept of Fog computing paradigm in [1] . The term Fog computing is chosen as an analogy where a Fog is a Cloud close to the ground or the edge [2] . Some work refers to Fog as an abbreviation for ''From cOre to edGe'' [3] . In Fog computing, the computation and storage capabilities are deployed in networking devices at different levels in the network architecture. These devices are equipped with schedulers that make decisions whether to serve a request at the Fog nodes or at the Cloud Datacenters (DCs). Such decision is typically based on multiple attributes related to the requests and the resources available at the Fog and Cloud. Fog benefits the Internet of Things (IoT) applications as it brings the computational and networking resources closer to the edge devices. Thus, allowing services and computations to run as close as possible to end users and IoT devices that require computational capabilities in real-time with low latency.", "cite_spans": [{"start": 58, "end": 61, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 166, "end": 169, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 241, "end": 244, "text": "[3]", "ref_id": "BIBREF2"}], "section": "Introduction", "sec_num": "1."}, {"text": "With the high increase of the internet-connected devices and the advent of IoT, the data that needs to be processed and served is becoming voluminous. Cisco estimated that in 2020 there will be 50 billion devices connected to the Internet [1] . These devices will be generating and exchanging data in large volume, at high velocity, and with noticeable heterogeneity, which is known as ''Big Data''. A large amount of the data generated by IoT applications can be classified as latency sensitive where applications require a processing response in real-time (seconds or microseconds). Moreover, the generated data from some IoT devices may not be valid after seconds (data expiry time) from the generation time. Given the nature of such applications, the stringent delay requirement makes processing this data at the Cloud level a potential performance bottleneck. Fog and Cloud computing create a cooperative and comprehensive architecture in which each one completes what the other lacks. The interaction between Fog and Cloud is a promising paradigm that will enable serving billions of IoT devices and applications with low latency. It helps significantly in monitoring and managing massive amount of data generated from the IoT devices. Examples of these applications include industrial automation, transportation, live streaming, real-time and online gaming, augmented reality, connected vehicles, remote patient health monitoring, smart micro grid, and smart traffic [2] [3] [4] .", "cite_spans": [{"start": 239, "end": 242, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 1474, "end": 1477, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 1478, "end": 1481, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 1482, "end": 1485, "text": "[4]", "ref_id": "BIBREF3"}], "section": "Introduction", "sec_num": "1."}, {"text": "The main factor that distinguishes Fog computing from Cloud computing is its closeness to end users. In the Fog, the services can be hosted at edge devices such as access points, routers, switches, base stations, and even end-users' devices. Fog computing, being at the edge of the network, may be used to address the latency concern of Cloud computing. Fog computing has a list of characteristics mentioned in [2, 4, 5] . Table 1 summarizes these characteristics and presents a Cloud-vs-Fog computing comparison.", "cite_spans": [{"start": 411, "end": 414, "text": "[2,", "ref_id": "BIBREF1"}, {"start": 415, "end": 417, "text": "4,", "ref_id": "BIBREF3"}, {"start": 418, "end": 420, "text": "5]", "ref_id": "BIBREF4"}], "section": "Introduction", "sec_num": "1."}, {"text": "The objective of this research is to model the problem of scheduling IoT requests generated by edge devices, and assigning them to the adequate resources available at both the Fog and Cloud. We use the Integer Linear Programming (ILP) technique to model the minimum service time for IoT requests. A heuristic approach using genetic algorithm (GA) is developed to obtain a feasible solution with a good quality in a reasonable computational time. The GA is customized for the proposed model to minimize latency. The work presents an adequate representation of a possible solution (chromosome) and a well-designed crossover operator. In addition, the GA includes a procedure to penalize possible solutions that do not satisfy the problem constraints. This will make the infeasible solutions to become less likely to be selected for producing new offspring chromosomes. The GA solution is provided to a network simulator to assess the impact of this solution on the delay performance. The service latency provided by the hybrid Fog-Cloud architecture that implements GA is compared to other systems with the same architecture but uses traditional scheduling algorithms, such as waited-fair queuing (WFQ), priority-strict queuing (PSQ), and round robin (RR) scheduling.", "cite_spans": [], "section": "Introduction", "sec_num": "1."}, {"text": "This paper is organized as follows: Section 2 presents a review of related work, Section 3 presents the formulation of the ILP model, Section 4 illustrates the proposed adaptive genetic algorithm, Section 5 presents the various simulation results, and Section 6 presents the conclusions and future work.", "cite_spans": [], "section": "Introduction", "sec_num": "1."}, {"text": "The work in [2] [3] [4] [6] [7] [8] [9] [10] [11] [12] [13] shared the same view of Fog computing as a distributed computing paradigm that extends the traditional Cloud computing resources at the edge of the network. Similar to Cloud computing, Fog computing provides ubiquitous computation, storage, networking, and application services in a highly visualized platform at the edge between end devices and Cloud computing data centers. Virtualization is a fundamental technology for Fog computing as it separates physical infrastructures to create various dedicated resources that can run multiple operating systems and multiple applications at the same time on the same resource.", "cite_spans": [{"start": 12, "end": 15, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 16, "end": 19, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 20, "end": 23, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 24, "end": 27, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 28, "end": 31, "text": "[7]", "ref_id": "BIBREF6"}, {"start": 32, "end": 35, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 36, "end": 39, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 40, "end": 44, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 45, "end": 49, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 50, "end": 54, "text": "[12]", "ref_id": "BIBREF11"}, {"start": 55, "end": 59, "text": "[13]", "ref_id": "BIBREF12"}], "section": "Fog computing definition", "sec_num": "2.1."}, {"text": "The work in [14] proposed a formal definition for Fog computing: ''Fog computing is a scenario where a huge number of heterogeneous (wireless and sometimes autonomous) ubiquitous and decentralized devices communicate and potentially cooperate among them and with the network to perform storage and processing tasks without the intervention of third parties. These tasks can be for supporting basic network functions (routing and switching) or new services and applications that run in a sandboxed (isolated and restricted) environment. Users leasing part of their devices to host these services get incentives for doing so''. This definition points out the proximity, wireless, decentralized characteristics of Fog computing. The definition also points out the potential cooperation between Fog computing devices which is a significant property within Fog computing paradigm for load balancing purposes. In this definition, the authors did not focus on addressing the interdependency and interaction between the Fog at the edge and the Cloud datacenter. Each one of these platforms has its own characteristics which are suitable for specific type of use cases to enable new spectrum of applications.", "cite_spans": [{"start": 12, "end": 16, "text": "[14]", "ref_id": "BIBREF13"}], "section": "Fog computing definition", "sec_num": "2.1."}, {"text": "In contrast, the Fog-Cloud interdependency is stressed in [15] where the authors developed a definition that covers the significant properties of Fog computing. Their definition states: ''Fog computing is a geographically distributed computing architecture with a resource pool that consists of one or more ubiquitously connected heterogeneous devices (including edge devices) at the edge of network and not exclusively, but seamlessly backed by Cloud computing services, to collaboratively provide elastic computation, storage and communication (and many other new services and tasks) in isolated (sandboxed) environments to a large scale of clients in proximity''. The definition realized the impact of the collaboration between Cloud computing, Fog computing, and edge devices in very large-scale systems. This latter definition is the most comprehensive and is adopted in this work.", "cite_spans": [{"start": 58, "end": 62, "text": "[15]", "ref_id": "BIBREF14"}], "section": "Fog computing definition", "sec_num": "2.1."}, {"text": "Generally, scheduling is defined as the allocation of tasks to capable resources at a specific time. Usually, a scheduling problem is subject to many constraints and objectives that must be fulfilled. The scheduling problem can be mapped to an optimization problem where the scheduling problem objectives can be, for instance, minimizing latency and maximizing resource utilization. The work in [16] specifies that most of scheduling problems consist of four basic elements:", "cite_spans": [{"start": 395, "end": 399, "text": "[16]", "ref_id": "BIBREF15"}], "section": "Scheduling techniques", "sec_num": "2.2."}, {"text": "1. Resources: are physical/logical devices with the ability to execute or process tasks. 2. Tasks: are physical/logical operations that need to be executed by the resources. 3. Constraints: are conditions that must be considered in scheduling the tasks into the resources. They may be operation-based, task-based, resource-based, or a combination of these. They could also be hard constraints, meaning constraints that must be full-filled or soft constraints can be relaxed. 4. Objectives: are evaluation criteria that need to be measured in order to assess the schedule performance.", "cite_spans": [], "section": "Scheduling techniques", "sec_num": "2.2."}, {"text": "To find a solution to a scheduling problem, there are two broad categories of methods: exact methods and heuristic methods. Exact methods find the absolute optimal solution to the scheduling problem. Examples of exact algorithms include Simplex [17] and Branch-and-Bound [18, 19] . On the other hand, heuristic methods do not guarantee finding the optimal solution. However, they are able to find a solution that has some degree of optimality in a reasonable computation time compared to the time required by an exact method. Examples of heuristic algorithms include: simulated annealing [20, 21] , genetic algorithms [22] [23] [24] , and ant colony algorithms [25] .", "cite_spans": [{"start": 245, "end": 249, "text": "[17]", "ref_id": "BIBREF16"}, {"start": 271, "end": 275, "text": "[18,", "ref_id": "BIBREF17"}, {"start": 276, "end": 279, "text": "19]", "ref_id": "BIBREF18"}, {"start": 588, "end": 592, "text": "[20,", "ref_id": "BIBREF19"}, {"start": 593, "end": 596, "text": "21]", "ref_id": "BIBREF20"}, {"start": 618, "end": 622, "text": "[22]", "ref_id": "BIBREF21"}, {"start": 623, "end": 627, "text": "[23]", "ref_id": "BIBREF22"}, {"start": 628, "end": 632, "text": "[24]", "ref_id": "BIBREF23"}, {"start": 661, "end": 665, "text": "[25]", "ref_id": "BIBREF24"}], "section": "Scheduling techniques", "sec_num": "2.2."}, {"text": "There are few existing studies in the literature in terms of design, algorithms or implementations within Fog computing. Cisco introduced the IOx [26] which is a hosting environment for IoT applications. IOx brings the application execution capability down to the source of IoT data. IOx offers steady and consistent hosting across multiple network infrastructure devices such as Cisco routers, switches, and compute modules.", "cite_spans": [{"start": 146, "end": 150, "text": "[26]", "ref_id": "BIBREF25"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "In [10] , the authors addressed Cloudlet as a special case of Fog computing. It is an intermediate layer located between the Cloud data centers and each mobile device. The edge devices The work in [27] presented a high-level programming model for future Internet applications that are characterized by being geospatially distributed, large-scale, and latency-sensitive. Fog computing resources are allocated for serving the low-latency services while tolerant larger scope services that need aggregation are allocated in the Cloud datacenter. Moreover, the work in [28] proposed a healthcare reference model that consists of four layers: sensing layer that integrates different physical sensors that collect data, networking layer that offers networking support and data transfer in the wired and wireless networks, service layer that creates and manages all types of services aiming to satisfy user requirements, and interface layer that enables the interaction between users and applications. The work also elaborated in splitting the networking layer into two sub-layers namely the Fog layer to handle the local buffering and different connectivity requirements to IoT devices, and the Cloud layer that handles the connectivity to the Fog, user/device/data management, and application services such as dashboard, rule engine, big data analytics, and integration framework.", "cite_spans": [{"start": 3, "end": 7, "text": "[10]", "ref_id": "BIBREF9"}, {"start": 197, "end": 201, "text": "[27]", "ref_id": "BIBREF26"}, {"start": 565, "end": 569, "text": "[28]", "ref_id": "BIBREF27"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "The work in [29] studied web optimization within the context of Fog computing. Fog servers connect the end users to the Internet so that all web requests must pass through the Fog servers on their way to the web servers of the Cloud. There are two possible process flows for the web requests: (1) optimizing webpage for its initial request and (2) optimizing webpage for its subsequent request(s) where all the needed files and web objects are cached and stored locally within the Fog server.", "cite_spans": [{"start": 12, "end": 16, "text": "[29]", "ref_id": "BIBREF28"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "The authors in [30] introduced a placement and migration method for providers of infrastructures that involved the Fog and the Cloud computing resources. It works by planning the migration ahead of time so that it can meet the defined end-toend service latency and at the same time it reduces the network bandwidth consumption.", "cite_spans": [{"start": 15, "end": 19, "text": "[30]", "ref_id": "BIBREF29"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "According to [31] , mobile Cloud is a very similar model to Fog computing in which the resources are shared not only from central resources but also from pervasive mobile devices. Although these devices have heterogeneous resources (e.g. CPUs, bandwidth, content) and support services, they still can share these resources. Based on the key concept of service-oriented utility functions, the authors proposed an architecture and mathematical framework for heterogeneous resource sharing. The heterogeneous resources are most probably measured in dissimilar scales/units (e.g. power, bandwidth, latency). For this reason, the authors adopted a unified framework on their work where all quantities are mapped to time only. They also formulated their model for optimization and found the optimal solution using convex optimization approaches.", "cite_spans": [{"start": 13, "end": 17, "text": "[31]", "ref_id": "BIBREF30"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "In [32] , the authors introduced a new computational framework that enables remote real-time sensing, monitoring, and scalable computing for diagnosis and prognosis in a Fog-based environment. The framework also utilizes wireless sensor networks, Cloud computing, Fog computing and machine learning. The Fog computing-based framework has been used for data-driven machine health and process monitoring in cyber-manufacturing. The objective of Fog cyber-manufacturing is also to provide the foundation to smart manufacturing networks in which manufacturers will have access to on-demand computing infrastructures, mobile applications for cyber-manufacturing, and parallel machine learning tools.", "cite_spans": [{"start": 3, "end": 7, "text": "[32]", "ref_id": "BIBREF31"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "The authors in [33] proposed an approach for distributing event-based across edge and Cloud resources to support IoT applications. They considered an IoT deployment scenario with multiple event streams generated at the edge at high frequency and a user-defined analytics dataflow composed of queries that needs to be executed on these streams, and several edge devices in a private network and public Cloud VMs available to perform the queries. Their goal was to determine a distributed placement of these queries onto the edge and Cloud resources such that the end-to-end latency for performing the event analytics is minimized to support timely decision making. This placement schedule needs to meet constraints such as the throughput capacity supported on edge and Cloud machines by the queries, bandwidth and latency limits of the network, and energy capacity of the edge devices. Another work in [34] proposed a workflow service request and a dynamic minimum response time that considers the same level algorithm to map requests in edge computing. The experimentation showed that the proposed algorithm performed very well in response time and blocking rate. The network traffic problem in Cloud and Edge Computing is a core challenge that is addressed in [35] . The work presented two optimization models that minimize the maximum percentage of the average packet time and the average arriving time for each node. While the second model deals with maximizing the local network link utilization. The work introduced multi-topology routing to ensure the performance of the assigned egress routers with a near-optimal solution based on the proposed heuristic approach. The results show that the proposed algorithm has a lower execution time and better quality of service than other approaches with the aim to satisfy the flexibility and efficiency demands of network traffic problems in cloud and edge computing.", "cite_spans": [{"start": 15, "end": 19, "text": "[33]", "ref_id": "BIBREF32"}, {"start": 901, "end": 905, "text": "[34]", "ref_id": "BIBREF33"}, {"start": 1261, "end": 1265, "text": "[35]", "ref_id": "BIBREF34"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "Security is a major challenge in Fog computing given the nature of the environment. The work in [36] proposed a cybersecurity framework to identify malicious edge devices in the distributed fog computing environment. The framework uses the two-stage Markov model for early prediction of the malicious and legitimate edge devices. The results presented the effectiveness of the proposed framework.", "cite_spans": [{"start": 96, "end": 100, "text": "[36]", "ref_id": "BIBREF35"}], "section": "Fog computing implementations", "sec_num": "2.3."}, {"text": "The work in [37] proposed a budget constraint based scheduling model to minimize execution time while meeting a specified budget for delivering results. They modeled the workflow application as a Directed Acyclic Graph (DAG). The developed genetic algorithm is used to solve the scheduling optimization problem with a cost-fitness and time-fitness. The algorithm was tested in a simulated Grid computing environment.", "cite_spans": [{"start": 12, "end": 16, "text": "[37]", "ref_id": "BIBREF36"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "In [38] , different existing approaches were comparatively examined for scheduling of scientific workflow applications in Grid computing environments. Three algorithms were evaluated; one of them is using Genetic algorithms (GA). The authors also studied the incremental workflow partitioning against the full-DAGgraph scheduling strategy. They demonstrated experiments using real-world scientific applications covering both balanced (symmetric) and unbalanced (asymmetric) workflows.", "cite_spans": [{"start": 3, "end": 7, "text": "[38]", "ref_id": "BIBREF37"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "In [39] , the authors proposed a resource provisioning and scheduling strategy for scientific workflows on Infrastructure as a Service (IaaS) Cloud environments. They presented an algorithm based on the meta-heuristic optimization technique named particle swarm optimization (PSO). This algorithm aims to minimize the overall workflow execution cost while meeting deadline constraints.", "cite_spans": [{"start": 3, "end": 7, "text": "[39]", "ref_id": null}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "The authors in [40] worked on deadline sensitive leases which can be scheduled using traditional backfilling algorithm. However, in the backfilling algorithm one of the leases is selected from the best effort queue which will provide the free resources to schedule the deadline sensitive lease. However, in some scenarios, backfilling algorithm does not provide better scheduling if there are similar types of leases and must be in conjugative in sequence. For this reason, the authors used an algorithm called AHP (Analytic Hierarchy Process) as a decision maker with the backfilling algorithm. AHP helps in choosing a possible best lease from a given best effort queue in order to schedule deadline sensitive leases.", "cite_spans": [{"start": 15, "end": 19, "text": "[40]", "ref_id": "BIBREF39"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "The authors in [41] presented an auto-scaling mechanism for Cloud computing environments. In the authors approach, the Cloud resources are considered as virtual machines (VMs) of various sizes/costs. The jobs are specified as workflows where users specify performance requirements by assigning (soft) deadlines to jobs. The goal is to ensure all jobs are finished within their deadlines at minimum financial cost. They used the Earliest Deadline First (EDF) algorithm to schedule tasks on each VM type. After deadline assignment and instance consolidation, every task is scheduled to a VM type. The approach sorts the tasks by their deadlines for each VM type, and schedule the task with the earliest deadline whenever an instance is available.", "cite_spans": [{"start": 15, "end": 19, "text": "[41]", "ref_id": "BIBREF40"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "In [42] , the authors proposed a Dynamic Critical Path (DCP) based workflow scheduling algorithm that determines efficient mapping of tasks by calculating the critical path in the workflow task graph at every step. It assigns priority to a task in the critical path which is estimated to complete earlier. They compared the performance of their proposed approach with other existing heuristic and meta-heuristic based scheduling strategies for different types and sizes of workflows.", "cite_spans": [{"start": 3, "end": 7, "text": "[42]", "ref_id": "BIBREF41"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "The authors in [43] presented a collaborative architecture of thin clients and conventional desktop or laptop computers, known as thick clients, aiming to improve Cloud access. Such an architecture enables offloading computation-intensive, resourceconsuming tasks up to a powerful computing platform which is Cloud, leaving only simple jobs to the capacity-limited thin client devices such as smartphones and tablets. Additionally, they introduced a genetic approach for task scheduling such that the processing time is minimized. The network needs to support high bandwidth and low latency connection between clients and the Cloud. They compared the performance of their approach with two algorithms: Contention aware Scheduling (CaS) and Greedy for Cost (GfC) algorithm.", "cite_spans": [{"start": 15, "end": 19, "text": "[43]", "ref_id": "BIBREF42"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "In [44] , the authors investigated the collision issues within the Internet of Vehicles (IoV) and proposed a collision prediction and avoidance technique. The proposed technique was based on deep learning that uses genetic algorithm back propagation neural network to forecast the rear-end collision probability in the IoV. The work considered influential factors such as the driver, road, vehicle, and the surrounding external environment. The genetic algorithm was used to demonstrate the accuracy between the predicted and actual values for the Internet of Vehicles. The work also showed that GA can be improved with the use of deep learning. Moreover, the work in [45] presented genetic algorithm in predictive modeling and forecasting the green economic performance for achieving supplier selection in cities.", "cite_spans": [{"start": 3, "end": 7, "text": "[44]", "ref_id": "BIBREF43"}, {"start": 668, "end": 672, "text": "[45]", "ref_id": "BIBREF44"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "In this work, we are approaching the latency sensitivity for IoT applications and requests as a scheduling problem that involves resources in both Fog and Cloud. Previous studies (such as in [5] and [46] ) tried to solve the latency problem by using Fog as a replacement for Cloud. In such environment, the interoperation between Fog and Cloud is very limited. It is also not a realistic solution as Fog devices are resource-limited by nature where the Cloud is resource-rich. In this work, the GA is customized for the proposed ILP problem; namely, an appropriate representation of a possible solution (chromosome) and a well-designed crossover operator are provided. In addition, the GA included a procedure to penalize possible solutions that do not satisfy the problem constraints so that they become less likely to be selected for producing new offspring chromosomes. \u2022 Q j : size of request r j in packets \u2022 IT j : Initiation time of request r j \u2022 D j : Deadline for request r j \u2022 W j : Priority weight factor for request r j . Each request priority is assigned as a normalized fractional weight, W j , to signify its importance with respect to the other requests. All weights should sum up to an exact 1, i.e.", "cite_spans": [{"start": 191, "end": 194, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 199, "end": 203, "text": "[46]", "ref_id": "BIBREF45"}], "section": "Related work using genetic algorithms", "sec_num": "2.4."}, {"text": "\u2211 n j=1 W j = 1.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "Those requests require processing power P i from the set of heterogeneous resources S = {s i |i = 1, . . . , m}. A resource is a physical device where a request is processed. Resource s i might reside close to the edge at the Fog or further from the edge at the Cloud. Each request r j experiences specific delays as follows:", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 Transmission delay, which is the time it takes to push the request packets' bits onto the connection link.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 Queuing delay, which is the time the request data packets are spend in passing through the network routers and switches.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 Propagation delay, which is the time for a signal to propagate through the transmission media before it reaches its destination.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "In this work, the propagation delay is neglected as it is insignificant compared to the other delays.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "The transmission delay is a function of the packet's length and the link transmission rate. It has a deterministic value for fixed packet sizes and transmission rates, and hence, can be evaluated beforehand. On the contrary, the queuing time is stochastic in nature. For this reason, the total delay is modeled as a random variable with an average value given by:", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u03b4 = \u03b4 tr + \u03b4 q (1)", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "where \u03b4 tr is the deterministic transmission delay and \u03b4 q is the average value of the queuing delay.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "Additionally, the actual processing time is denoted as PT ij , which is the time it takes to process each request r j by resource s i . Examples of IoT data processing are data storage, data aggregation and analysis, features extraction, images and video processing, etc. This processing time is defined by the data size of the request and the processing capability of the resource that processes the request. The resource capability is defined in terms of processing speed P i . Moreover, a request might experience additional delay if the resource is busy serving other requests.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "The scheduler as shown in Fig. 1 is to allocate requests in R to the capable resources in S at a specific time with the objective of minimizing latency. The latency of request r j is defined as the round-trip time (RTT) from the moment the request is initiated at the edge to the moment it is completely processed and the results are returned to the requester. The scheduler has the environment knowledge such as the number of resources, resources capabilities, resources availability, resource networking and transmission delays, number of requests, and the requirements of requests.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "The scheduling problem model is built based on the following environment settings. Those settings are identified to present the scope of the problem model and its operating environment. The model assumptions are:", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 A resource as a compute processor, at both the Fog and the Cloud, can process only one request at a time.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 A request can be initiated at any instant of time.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 The data size of each request is defined by the number of packets the request has. Each request may have multiple packets where each data packet is assumed to be of the same size.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "\u2022 Pre-emption is not allowed. If a request starts processing, it must be finished without interruption.", "cite_spans": [], "section": "Environment analysis", "sec_num": "3.1."}, {"text": "Consider a set or n requests, R = { r j |j = 1, . . . , n }", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": ", that are generated from the edge-layer IoT devices and that need to be processed by a set of m resources S = {s i |i = 1, . . . , m}. The resources are distributed among the Fog and the Cloud. The objective is to minimize the overall latency defined as the sum of the RTT for serving all n requests using m available resources, while taking into account the various constraints of the problem. Mathematically, the latency is the difference between the initiation time and the end of service time as presented in Eq. ( 4). The service time includes the starting time, the processing time, and the transmission and queuing times. The overall latency to minimize as the problem objective function, is defined as the weighted sum of the individual requests' latencies as follows in Eq. ( 2):", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "min m \u2211 i=1 n \u2211 j=1 ( LT ij W j X ij ) (2)", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "where LT ij as presented in Eq. ( 4) is the latency incurred when serving the jth request, denoted r j , by the ith resource, denoted s i . Here, W j is the weighting factor of request r j describing its priority, and X ij is a decision variable as presented in Eq. ( 3), that indicates whether request r j is allocated to resource s i . The individual latency of a given request is:", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "EQUATION", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": ")", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "LT ij = ST ij + PT ij + TQT ij -IT j (4)", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "Here, ST ij is the start time of the processing of request r j by resource s i , P ij is the processing time of the request, TQT ij is the transmission and queuing time before request r j reaches the resource s i , and IT j is the initiation time of request r j . The processing time of each request is equal to the request data size (in packets) divided by the resource processing power (in packets per second). Let Q j denote the data size of request r j and P i denote the processing power of resource s i , then the processing time is given by PT ij = Q j /P i . The transmission and queuing time of request r j from the edge layer to resource s i , located in the Fog or the Cloud, is given by Eq. (5) .", "cite_spans": [{"start": 703, "end": 706, "text": "(5)", "ref_id": "BIBREF4"}], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "TQT ij = Q j \u2211 i=1 \u03b4 i (5)", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "A request delay is the summation of its packets delays that will suffer individual delays distributed around a specific mean. This means a request cannot start processing unless all its packets reach the resource.", "cite_spans": [], "section": "Model formulation Objective function:", "sec_num": "3.2."}, {"text": "The objective function defined in Eq. ( 2) is to be minimized subject to sets of constraints. The constraint in Eq. ( 6) presents each request r j must be served by only one resource.", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "m \u2211 i=1 X ij = 1, \u2200r j \u2208 R (6)", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "In the constraint presented in Eq. ( 7), a request r j cannot start processing before getting transmitted to the resource. This amount of time includes the initiation time plus the transmission and queuing time to where the Fog or the Cloud resources reside. In other words, in order to process a request, this request must be created and transmitted to the resource first.", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "ST ij \u2265 IT j + TQT ij , \u2200r j \u2208 R, \u2200s i \u2208 S (7)", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "The constraints presented in Eqs. ( 8) and ( 9) present each resource s i can process only one request at a time. If there are two requests that arrive at the resource at the same time for processing, one of them must start processing after the other one finishes. The order in which these two requests get executed is defined by the parameter \u03b8 ijk . If \u03b8 ijk = 1, resource s i executes request r j then request r k and vice versa. The decision variable \u03b8 ijk is defined as presented in Eqs. ( 8) and ( 9) for \u2200s i \u2208 S, \u2200r j ,", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "EQUATION", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "The constraint presented in Eq. ( 10) takes into account the precedence dependency among requests. A request is not able to start processing unless its precedents finish processing even if they are allocated in different processors. This constraint is presented in Eq. ( 10) for \u2200s i \u2208 S, \u2200r j , r k \u2208 R, r j \u0338 = r k .", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "EQUATION", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "The constraint presented in Eq. ( 11) takes into account that a request must be served before a specific deadline. This constraint is presented in Eq. ( 11) for \u2200s i \u2208 S, \u2200r j \u2208 R.", "cite_spans": [], "section": "Model constraints:", "sec_num": null}, {"text": "LT ij \u2264 D j (11) Table 2 summarizes the nomenclature used in the problem formulation.", "cite_spans": [{"start": 12, "end": 16, "text": "(11)", "ref_id": "BIBREF10"}], "section": "Model constraints:", "sec_num": null}, {"text": "Given the complexity nature of the problem being NP-Hard, exact optimal solutions are not adequate. A full analysis and proof of the complexity of similar class of problem can be found in [46] . Hence, in this work we propose a customized implementation of the Genetics algorithm. Genetic algorithm [22] is a meta-heuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms In a GA, a population of candidate solutions (chromosomes) to an optimization problem is evolved toward better or more fit one. For a scheduling problem, a population chromosome represents a candidate solution to the problem. A solution is feasible if it satisfies the problem constraints; otherwise, it is infeasible. We propose a GA that employs a chromosomal representation, a crossover and mutation operators appropriately designed for the considered problem. In addition, the GA includes a procedure for penalizing infeasible solutions so that they have less probability of selection (or survival).", "cite_spans": [{"start": 188, "end": 192, "text": "[46]", "ref_id": "BIBREF45"}, {"start": 299, "end": 303, "text": "[22]", "ref_id": "BIBREF21"}], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "The algorithm for our implementation is shown in Table 3 . The GA initially defines the problem size, i.e. the number of requests and the available resources, and then it generates a population of POP number of candidate solutions (chromosomes or individuals) to represent the initial population. The next step is to evaluate the fitness F of each chromosome, defined as the inverse of the total latency i.e. F = 1/LT . Afterwards, we select mates or parent individuals (chromosomes) that will undergo crossover and mutation to create a new offspring population from the current population [47] . After crossover and mutation, the fitness value of each created offspring chromosome is evaluated according to the fitness function F. Next, a procedure is applied to determine infeasible chromosomes and then another procedure is applied to penalize some of them. In addition, the best-sofar chromosome, i.e. the one with the least fitness value, of all generated populations is preserved. The GA repeats the above steps until some termination criterion is satisfied as given in more details later. Upon termination, the GA returns the best-so-far chromosome representing a solution to the considered problem.", "cite_spans": [{"start": 590, "end": 594, "text": "[47]", "ref_id": "BIBREF46"}], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "In our proposed algorithm, the operators are implemented as follows:", "cite_spans": [], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "\u2022 Chromosomal Representation: In our GA, a candidate solution (chromosome) to the scheduling problem is a 2dimensional array of X ij elements (or genes), of 0 or 1 integer values, representing a possible allocation of requests to resources as shown in Fig. 2 considering an allocating of n requests within m resources. A gene X ij has the value 1 (0) if request j is assigned (not assigned) to resource i. This 2-dimensional array is stretched vertically, as shown in Fig. 3 , in order to get a 1-dimensional solution array that represents a chromosome. It is important to note that there is also another dimension in the scheduling problem solution which is the execution order of requests that are allocated to the same resource (refer to Eqs. ( 8) and ( 9))). This is an important parameter since it gives the time dimension to the scheduling problem and distinguishes it from the traditional allocation problems. This parameter is not considered as part of the chromosome. However, it is considered in the latency evaluation of each chromosome.", "cite_spans": [], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "\u2022 Fitness Evaluation: For each chromosome of the population, the fitness of the given chromosome is calculated as F = 1/LT where LT is given by the objective function in Eq. ( 2). In this function, the algorithm determines the values of the decision variable in the problem, X ij and \u03b8 ijk .", "cite_spans": [], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "\u2022 Crossover and mutation: Our GA uses the traditional Roulette Wheel method [47] for selecting each two parents from the current population that will mate (crossover) to reproduce a new offspring population. Then, for each two selected chromosomes, a crossover-point (or position) k is selected as in integer value between 2 and the total number of requests j. Thus, the crossover-point k is chosen delicately at a position (as shown in Fig. 3 ) where we not to mix up the requests allocation. Then, and all genes corresponding to requests k to j are swapped to create two new chromosomes. This process is repeated until the number of generated children chromosomes in the new generation is equal to the number of chromosomes in the current generation. Afterwards, standard mutation is applied, where for each randomly selected gene X ij , its value flipped to the opposite value (i.e. from 1 to 0 or the converse).", "cite_spans": [{"start": 76, "end": 80, "text": "[47]", "ref_id": "BIBREF46"}], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "\u2022 Feasibility check and infeasibility penalization: GA checks the feasibility of the new offspring chromosomes after crossover and mutation. More precisely, the GA determines which new chromosomes are feasible, i.e., satisfy constraints ( 6) and (11) . In other words, for each chromosome, it determines that each of its requests is allocated to one and only one resource Eq. ( 6) and if the chromosome satisfies the deadline requirements of Eq. (11) . 50% of the infeasible chromosome that do not satisfy constraints ( 6) and ( 11) are penalized by increasing their fitness values so that they have less probability of survival to the next generation.", "cite_spans": [{"start": 246, "end": 250, "text": "(11)", "ref_id": "BIBREF10"}, {"start": 446, "end": 450, "text": "(11)", "ref_id": "BIBREF10"}], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "\u2022 Elitism and termination criteria: the GA saves the best-sofar chromosome (the one that is feasible and has the least fitness value) of all generated populations. The algorithm stops when the termination criterion is satisfied. According to our experiments, this is done when the fitness of the bestso-far did not change for 4 consecutive iterations or when the maximum number of iterations MAX = 20 (determined as shown in the following Section 5) is reached. ", "cite_spans": [], "section": "Proposed genetic algorithm", "sec_num": "4."}, {"text": "Before using the genetic algorithm in a real-time simulation environment, different GA parameters within the implementation were studied. The most important two parameters within the GA implementation are: population size (POP) and maximum number of iterations (MAX). These two parameters have a direct impact on the solution quality and runtime.", "cite_spans": [], "section": "GA experimentation", "sec_num": "5."}, {"text": "Experiments are conducted to determine the population size (POP) considering both the quality of obtained solution as determined by the overall latency (LT) and the runtime (RT) of the GA. We consider varying size scheduling problems N, where each value of N is a tuple (j/i) of j requests and i resources. More precisely, we consider tuples of values (40/10), (6/20), (80/30), and (100/50) as shown in Figs. 4 and 5 , respectively. For each considered value of N, five different experiments are conducted and the averages of their corresponding LT and RT values are calculated as depicted in Figs. 4 and 5 . For these experiments, the maximum number MAX of iterations of the GA is set to 50. ", "cite_spans": [], "section": "Population size", "sec_num": "5.1."}, {"text": "In order to determine best value for the number of iteration MAX, experiments are conducted considering both the quality of obtained solution (as determined LT) and the runtime RT of the GA. In fact, we use the same values of N as given in the previous section, and vary the values of MAX as shown in Figs. 6 and 7 , respectively. For each combination of a considered value of N and MAX, five different experiments are executed and the average of the corresponding LT and RT values are depicted as shown in the figures. According to these experiments, an increase in MAX refines the solution quality (latency) up to a certain point; however, it usually sharply increases the runtime of the GA. In this work, MAX is set to 20 as it gives good quality solution with a moderate runtime.", "cite_spans": [], "section": "Maximum number of iterations", "sec_num": "5.2."}, {"text": "This section includes a comparison of the proposed GA with respect to an exact algorithm in terms of quality of solution (as determined by LT) and runtime (RT). Namely, we adopt the known Lingo software (Lingo, 2017) that uses a Branch-and-Bound algorithm (B&B) for providing the optimal quality solution of the considered problem. To this end, experiments are conducted using small size problems, as the exact algorithm did not converge to solutions for large size problems. In particular, we problems of the following combinations n requested and m resources are used, (4, 2), (6, 2), (8, 2), (12, 2), (8, 3) , and (10, 3) as shown in Fig. 8 . Figs. 8 and 9 depict the solution quality (latency in mss) and runtime of both Lingo using B&B and GA, respectively, for the conduced experiments. According to these experiments, both the exact and the GA provide solutions of comparable quality; however, as shown in Fig. 9 , even for these small size problems, the runtime of the exact is much more than that of the GA. In fact, the exact algorithm took almost two days to compute a solution for a problem with 10 requests and 3 resources.", "cite_spans": [{"start": 604, "end": 607, "text": "(8,", "ref_id": "BIBREF7"}, {"start": 608, "end": 610, "text": "3)", "ref_id": "BIBREF2"}], "section": "Comparison of GA with exact algorithm", "sec_num": "5.3."}, {"text": "A simulation model from the formulated model is developed using the discrete event simulator. The simulation is built based on Edge-Fog-Cloud 3-layered architecture as shown in Fig. 1 . At the edge layer, requests are generated with a specific distribution for the inter-arrival times. Each generated request is associated with its attributes defined in the model. The requests travel from the edge to the propose GA scheduler that decides on when and where to allocate the requests on the Fog or the Cloud resources. The GA scheduling algorithm is integrated with the discrete event simulator. The GA scheduler receives requests within specific defined time frames. Within any time frame, the GA scheduler can receive any number of requests. Initially, all the resources in the simulator are available. As time passes, and requests are initiated, those requests are allocated to the resources. Hence, some of the resources will be busy processing those initiated requests. This means, the GA generates a scheduling solution and keeps the generated solution knowledge for future scheduling decisions as new requests arrive.", "cite_spans": [], "section": "Simulation and results", "sec_num": "6."}, {"text": "The main resources attributes defined in the model are the processing power and the average delay. The GA solver assumes all requests packets are delayed to be served based on the average historical delay of serving the previously processed requests. Hence, in the simulation environment, the actual delay per packet can be the same as the average value or different based on the distribution used. In the simulation environment for the produced results, the transmission and queuing delay distribution is set to Gaussian with specific mean and variance. When a request is sent to a specific resource to be served and it reaches the resource, if the resource is available and the waiting queue is empty, the request is served right away. Otherwise, the request is pushed into a waiting queue and served as soon as the resource become available.", "cite_spans": [], "section": "Simulation and results", "sec_num": "6."}, {"text": "In this experiment, the GA scheduler performance is compared to other traditional algorithms. These algorithms namely are Waited Fair Queuing, Priority Strict Queuing, and Round Robin [48] . They are compared upon two metrics: the overall average service latency and the number of missed-deadline requests. The population size and maximum number of iterations are set to 60 and 20 respectively as concluded in Sections 5.1 and 5.2. The comparison is performed based on two scheduling modes: static scheduling and dynamic scheduling. In static mode, the inter-arrival time between requests is removed and all requests are assumed to be generated at time 0 as one batch. In dynamic scheduling, requests are generated using a normal distribution and inter-arrival rate.", "cite_spans": [{"start": 184, "end": 188, "text": "[48]", "ref_id": "BIBREF47"}], "section": "Simulation and results", "sec_num": "6."}, {"text": "This experiment involves a set of 16 servers with an average processing speed of 500 packets per second and widely distributed from 50 to 1000 packets per second. The servers' average delays are set to an average of 5 ms per packet. It is also distributed from 1 ms up to 9.7 ms. A total of 100 requests is used in the experiment. These requests are generated at time 0 with no inter-arrival time. The priorities are set to be uniformly distributed from 1 to 16. The deadline requirements are set to be 400 s on average with a variance of 50. Initially, the average data size starts from a small value in a way that makes requests deadline requirements loose and zero requests miss their deadlines. Then, the average data size is increased to observe the impact on the overall latency and number of missed-deadline requests.", "cite_spans": [], "section": "Static scheduling", "sec_num": "6.1."}, {"text": "Figs. 10 and 11 shows the average overall latency and the number of missed-deadline requests versus requests average size. The GA achieved better overall latency than the rest of the algorithms. WFQ and PSQ results are very close to each other since the allocation of requests within resources in both of these algorithms is achieved based on priority, however, the dispatching is different. RR achieved the highest latency time and the reason is the way requests are allocated in a RR fashion without considering requests priorities. It can also be observed that the GA keeps the record clean of missed-deadline requests for longer time than the other algorithms. However, at an average size of 6500 packets, the GA cannot guarantee meeting all requests deadlines as their service latency increases and their deadlines become very critical. It is important to mention that, within all the experimented data sizes using the GA, the latter was able to come up with a feasible schedule solution in which all requests deadlines should be met. However, the simulation results show that it is not guaranteed that all requests will be met even if the evaluated GA scheduling solution is feasible. This can be seen between 6000 and 8000 average data size in Fig. 11 . After a data size of 8000 packets, the problem becomes infeasible and hence the GA cannot find a feasible schedule. This is true in the simulation because the actual delay per packet can be different from the average delay.", "cite_spans": [], "section": "Static scheduling", "sec_num": "6.1."}, {"text": "This experiment setup contains 16 servers and 500 requests with the objective of the experiment to evaluate the latency as requests arrive over time. The requests are generated in a Poisson distribution with an inter-arrival mean of 1 s. The priorities are set to be between 1 and 16 in a uniform distribution. The deadline requirements on average are set to 200 s with a variance of 50. The time frame in which the resources get scheduled is 10 s. The average request data size is normally distributed from 1000 to 10 000 packets.", "cite_spans": [], "section": "Dynamic scheduling", "sec_num": "6.2."}, {"text": "Figs. 12 and 13 show the overall average latency and the number of missed-deadline requests versus the average data size. The GA achieved the best overall latency compared to the other algorithms. WFQ and PSQ results are very close to each other. However, the difference increases by increasing the average data size. On the other hand, Fig. 13 shows that the GA achieved 0 missed-deadline requests if the data size is less than 5000 packets on average. At an average data size of 6000 packets, the GA achieved 0 missed requests, WFQ and PSQ lost almost 10% of the requests and RR lost 6% of the requests. After this point, most of the requests deadline requirements become very critical and some of them even infeasible. For this reason, the GA also starts missing requests when the data size is increased above 6000 packets. However it still achieves better results than the other 3 algorithms.", "cite_spans": [], "section": "Dynamic scheduling", "sec_num": "6.2."}, {"text": "The objective of this section is to evaluate the service latency provided by resources that have cloud and fog computing characteristics. In general, cloud resources are classified as powerful with high processing capabilities, but at the same time they have large average transmission and networking delay. Conversely, fog resources have limited processing power but they provide smaller average delay since they exist closer to the edge. This experiment gives a clear notion about cloud and fog resources from a design perspective, whether it is more beneficial to put very powerful resources at the cloud or to put much less powerful resources closer to requests sources in the fog.", "cite_spans": [], "section": "Comparison of cloud-only and hybrid Fog-Cloud architectures", "sec_num": "6.3."}, {"text": "In order to be able to serve a set of requests with a minimized latency, three parameters are considered in the formulated model as follows:", "cite_spans": [], "section": "Comparison of cloud-only and hybrid Fog-Cloud architectures", "sec_num": "6.3."}, {"text": "\u2022 The ratio of average delays, \u03b4 f /\u03b4 c .", "cite_spans": [], "section": "Comparison of cloud-only and hybrid Fog-Cloud architectures", "sec_num": "6.3."}, {"text": "\u2022 The ratio of processing speeds, P f /P c \u2022 The ratio of resource numbers, N f /N c The impact of each parameter on the service latency is studied independently by fixing two of them and varying only one. The latency that is achieved by varying these parameters is evaluated against a system that has a Cloud characteristic. This cloud setup has a set of 4 super cloud servers with a very high processing capability of 5000 packets per second. However, the average delays for these servers set to be relatively high as 10 ms per packet. To evaluate the latency, a total of 500 requests are used in this set of experiments. Their arrivals follow a Poisson distribution with a mean inter-arrival time of 1 s. The latency is studied versus the average data size which will be changed from 1000 packets to 10 000 packets.", "cite_spans": [], "section": "Comparison of cloud-only and hybrid Fog-Cloud architectures", "sec_num": "6.3."}, {"text": "In this experiment, the number of Fog servers is set to four times the number of Cloud server, N f /N c = 4. Their processing power is only 10% or their Cloud peers, P f /P c = 10%. The average delay ratio, \u03b4 f /\u03b4 c , is changed to 1%, 10%, 20%, 50% and 85%. Fig. 14 presents the latency results, and as observed, increasing the average delay ratio from 1% to 85% increases the latency of the Fog until it reaches a point where it crosses the Cloud latency. This is interpreted as the Fog servers are far from the edge devices and closer to the Cloud servers. In this case, the Fog servers provide as poor latency as the one provided by Cloud.", "cite_spans": [], "section": "Impact of the Ratio of Average Delays:", "sec_num": null}, {"text": "In this experiment, the number of Fog servers is set to four times the number of Cloud servers, N f /N c = 4. Their average delay, \u03b4 f /\u03b4 c is 10%. The processing power, P f /P c , is changed to 3%, 5%, 7%, 10% and 20%. Fig. 15 shows the latency results. As observed by the results presented in Fig. 15 , decreasing the processing speed ratio from 20% to 3% increases the latency of Fog until it reaches a point where it crosses the Cloud latency. This is interpreted as fog servers have slow processing capabilities.", "cite_spans": [], "section": "Impact of the Ratio of Processing Speed:", "sec_num": null}, {"text": "In this case, even if these resources are closer to edge than the Cloud resources, the Fog provide high latency due to the slow processing capabilities. ", "cite_spans": [], "section": "Impact of the Ratio of Processing Speed:", "sec_num": null}, {"text": "In the experiment presented in Fig. 16 , the Fog servers' average delay is set to 10% of the Cloud average delay. Also, the processing power capabilities of the Fog servers are set to 10% of the Cloud servers' capabilities. The number of Fog resources relative to Cloud is varied to 100%, 150%, 200%, 300%, 400%, 600%, and 800%. Fig. 16 shows the latency results for this experiment. As observed from the experiment presented in Fig. 16 , decreasing the number of resources ratio from 800% to 100% increases the latency of Fog until it reaches a point where it crosses the Cloud latency. This can be interpreted as having Fog servers with very few resources. Even if these resources are closer to edge and have a good processing capability, having fewer resources affects the latency in a negative way.", "cite_spans": [], "section": "Impact of the Ratio of Resource Numbers:", "sec_num": null}, {"text": "Another experiment is conducted with the objective is to find the breaking points in terms of these three parameters, average delay, processing power and number of resources. The average data size is set to 5000 packets. Fig. 17 shows the results of the experiment as crossing points between fog latency and cloud latency. For instance, for a specific average delay ratio and processing power ratio between Fog and Cloud, the graph tells the number of servers' ratio in which Fog outperforms Cloud or vice versa.", "cite_spans": [], "section": "Impact of the Ratio of Resource Numbers:", "sec_num": null}, {"text": "In the last three experiments, it was observed that the latency of Fog computing at some point crosses the Cloud computing latency. This happens in three cases where Fog computing has high average delay as presented in Fig. 14 , has resources with low processing power as presented in Fig. 15 , and has few number of servers presented in Fig. 16 .", "cite_spans": [], "section": "Impact of the Ratio of Resource Numbers:", "sec_num": null}, {"text": "Another experiment is conducted with the objective is to find the breaking points for the three ratios with the average size of 5000 packets. Fig. 17 shows the results of the experiment. As presented in Fig. 17 , the experiment shows where a set of Fog servers would provide better latency than a set of Cloud servers. For instance, for a specific average delay ratio and processing power ratio between Fog and Cloud, the graph shows the number of servers' ratio in which Fog outperforms Cloud or vice versa.", "cite_spans": [], "section": "Impact of the Ratio of Resource Numbers:", "sec_num": null}, {"text": "This work modeled the scheduling problem for IoT requests to minimize latency in hybrid Fog-Cloud computing using integer linear program. The latency is addressed in this work as the Round-Trip Time (RTT) for processing an IoT request from the moment it is initiated to the moment it is completely processed and the results are returned back to the requester. This latency includes many delay components such as transmission delay, routing or queuing delay, propagation delay, processing time, waiting time in case the resources are busy.", "cite_spans": [], "section": "Conclusion and future research", "sec_num": "7."}, {"text": "The model is solved and validated using Lingo software to illustrate its solution and behavior. Lingo is set to use Branch-and-Bound as an exact algorithm for solving the model. All scheduling problems that were solved using Lingo were small-sized problems since the problem by nature is an NP-hard problem. Hence, exact methods are not adequate to solve large size problems. In this work, we developed a customized implementation of the Genetic Algorithms (GA) as a heuristic approach to find feasible solutions with a good quality in a reasonable computational time. The GA is studied and evaluated on different problems with different sizes in order to estimate the effects of the model with different parameters such as population size and maximum number of iterations. After developing the GA, a comprehensive comparison is performed between the exact solutions obtained from Lingo and the heuristic solutions obtained from the GA.", "cite_spans": [], "section": "Conclusion and future research", "sec_num": "7."}, {"text": "The optimized service latency obtained from the GA is also compared to other non-optimized scheduling techniques (such as WFQ, PRI, and RR). The results of the proposed approach showed significant improvement in the overall latency from 21.9% and up to 46.6% better solutions than the other algorithms. The proposed approach also succeeded in meeting the requests deadlines by up to 31% with better performance than the other algorithms.", "cite_spans": [], "section": "Conclusion and future research", "sec_num": "7."}, {"text": "The last set of experiments in Section 6.3 showed the significance of integrating Fog computing with Cloud computing. Fog computing is generally characterized by having small communication delay and wide spatial coverage. This allows using small-size low-power Fog computing resources and it provides better service latency than using only Cloud computing. The experiments showed typical parameters of where Fog outperforms Cloud in terms of resources average delay, processing power and number of resources.", "cite_spans": [], "section": "Conclusion and future research", "sec_num": "7."}, {"text": "Future work will extend the model to include critical request scheduling and to allow pre-emption. Other heuristic-based techniques will also be evaluated. Moreover, multiple objective functions can be included to maximize resource utilization and minimize latency.", "cite_spans": [], "section": "Conclusion and future research", "sec_num": "7."}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Cisco delivers vision of fog computing to accelerate value from billions of connected devices", "authors": [{"first": "S", "middle": [], "last": "Antonio", "suffix": ""}], "dblp_id": null, "year": 2014, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Antonio, Cisco delivers vision of fog computing to accelerate value from billions of connected devices, 2014, Internet: https://newsroom. cisco.com/press-release-content?type=webcontent&articleId=1334100, Jan. 29, (Accessed 24 January 2017).", "links": null}, "BIBREF1": {"ref_id": "b1", "title": "Fog computing and its role in the internet of things", "authors": [{"first": "F", "middle": [], "last": "Bonomi", "suffix": ""}, {"first": "R", "middle": [], "last": "Milito", "suffix": ""}, {"first": "J", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "S", "middle": [], "last": "Addepalli", "suffix": ""}], "dblp_id": "conf/sigcomm/BonomiMZA12", "year": 2012, "venue": "Proc. of 1st Edition of MCC Workshop on Mobile Cloud Comput", "volume": "", "issue": "", "pages": "13--16", "other_ids": {}, "num": null, "urls": [], "raw_text": "F. Bonomi, R. Milito, J. Zhu, S. Addepalli, Fog computing and its role in the internet of things, in: Proc. of 1st Edition of MCC Workshop on Mobile Cloud Comput., 2012, pp. 13-16.", "links": null}, "BIBREF2": {"ref_id": "b2", "title": "Assessment of the Suitability of Fog Computing in the Context of Internet of Things", "authors": [{"first": "S", "middle": [], "last": "Sarkar", "suffix": ""}, {"first": "S", "middle": [], "last": "Chatterjee", "suffix": ""}, {"first": "S", "middle": [], "last": "Misra", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "IEEE Trans. on Cloud Computing", "volume": "99", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Sarkar, S. Chatterjee, S. Misra, Assessment of the Suitability of Fog Computing in the Context of Internet of Things, in: IEEE Trans. on Cloud Computing, Vol. 99, Oct. 2015, p. 1.", "links": null}, "BIBREF3": {"ref_id": "b3", "title": "Fog computing", "authors": [{"first": "P", "middle": ["V"], "last": "Patil", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "IJCA Proc. on Nat. Conf. on Recent Trends in Mobile and Cloud Computing", "volume": "", "issue": "", "pages": "1--6", "other_ids": {}, "num": null, "urls": [], "raw_text": "P.V. Patil, Fog computing, in: IJCA Proc. on Nat. Conf. on Recent Trends in Mobile and Cloud Computing, 2015, pp. 1-6.", "links": null}, "BIBREF4": {"ref_id": "b4", "title": "Improving web sites performance using edge servers in fog computing architecture", "authors": [{"first": "J", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "D", "middle": [], "last": "Chan", "suffix": ""}, {"first": "M", "middle": [], "last": "Prabhu", "suffix": ""}, {"first": "P", "middle": [], "last": "Natarajan", "suffix": ""}, {"first": "H", "middle": [], "last": "Hu", "suffix": ""}, {"first": "F", "middle": [], "last": "Bonomi", "suffix": ""}], "dblp_id": "conf/sose/ZhuCPNHB13", "year": 2013, "venue": "Service Oriented System Engineering (SOSE)", "volume": "", "issue": "", "pages": "320--323", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Zhu, D. Chan, M. Prabhu, P. Natarajan, H. Hu, F. Bonomi, Improving web sites performance using edge servers in fog computing architecture. in: Service Oriented System Engineering (SOSE), 2013 IEEE 7th International Symposium on, 2013, pp. 320-323.", "links": null}, "BIBREF5": {"ref_id": "b5", "title": "Iot, from cloud to fog computing", "authors": [{"first": "M", "middle": [], "last": "Abdelshkour", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Abdelshkour, Iot, from cloud to fog computing, 2015, Internet: http: //blogs.cisco.com/perspectives/iot-from-cloud-to-fog-computing, Mar. 25, (Accessed 18 February 2017.", "links": null}, "BIBREF6": {"ref_id": "b6", "title": "Fog Computing architecture to enable consumer centric Internet of Things services", "authors": [{"first": "S", "middle": ["K"], "last": "Datta", "suffix": ""}, {"first": "C", "middle": [], "last": "Bonnet", "suffix": ""}, {"first": "J", "middle": [], "last": "Haerri", "suffix": ""}], "dblp_id": "conf/isce/DattaBH15", "year": 2015, "venue": "Int. Symposium on Consumer Electronics", "volume": "", "issue": "", "pages": "1--2", "other_ids": {}, "num": null, "urls": [], "raw_text": "S.K. Datta, C. Bonnet, J. Haerri, Fog Computing architecture to enable consumer centric Internet of Things services, in: Int. Symposium on Consumer Electronics, 2015, pp. 1-2.", "links": null}, "BIBREF7": {"ref_id": "b7", "title": "Fog computing and smart gateway based communication for cloud of things", "authors": [{"first": "M", "middle": [], "last": "Aazam", "suffix": ""}, {"first": "E", "middle": ["N"], "last": "Huh", "suffix": ""}], "dblp_id": "conf/ficloud/AazamH14", "year": 2014, "venue": "Int. Conf. on Future Internet of Things and Cloud", "volume": "", "issue": "", "pages": "464--470", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Aazam, E.N. Huh, Fog computing and smart gateway based communi- cation for cloud of things, in: Int. Conf. on Future Internet of Things and Cloud, 2014, pp. 464-470.", "links": null}, "BIBREF8": {"ref_id": "b8", "title": "Security and privacy issues of fog computing: A survey", "authors": [{"first": "S", "middle": [], "last": "Yi", "suffix": ""}, {"first": "Z", "middle": [], "last": "Qin", "suffix": ""}, {"first": "Q", "middle": [], "last": "Li", "suffix": ""}], "dblp_id": "conf/wasa/YiQL15", "year": 2015, "venue": "Int. Conf. on Wireless Algorithms, Syst. and Application", "volume": "", "issue": "", "pages": "685--695", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Yi, Z. Qin, Q. Li, Security and privacy issues of fog computing: A survey, in: Int. Conf. on Wireless Algorithms, Syst. and Application, 2015, pp. 685-695.", "links": null}, "BIBREF9": {"ref_id": "b9", "title": "Fog computing: A cloud to the ground support for smart things and machine-to-machine networks", "authors": [{"first": "I", "middle": [], "last": "Stojmenovic", "suffix": ""}], "dblp_id": null, "year": 2014, "venue": "Telecommunication Networks and Application Conf", "volume": "", "issue": "", "pages": "117--122", "other_ids": {}, "num": null, "urls": [], "raw_text": "I. Stojmenovic, Fog computing: A cloud to the ground support for smart things and machine-to-machine networks, in: Telecommunication Networks and Application Conf., 2014, pp. 117-122.", "links": null}, "BIBREF10": {"ref_id": "b10", "title": "A survey of fog computing: concepts, applications and issues", "authors": [{"first": "S", "middle": [], "last": "Yi", "suffix": ""}, {"first": "C", "middle": [], "last": "Li", "suffix": ""}, {"first": "Q", "middle": [], "last": "Li", "suffix": ""}], "dblp_id": "conf/mobihoc/YiLL15", "year": 2015, "venue": "Proc. of 2015 Workshop on Mobile Big Data", "volume": "", "issue": "", "pages": "37--42", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Yi, C. Li, Q. Li, A survey of fog computing: concepts, applications and issues, in: Proc. of 2015 Workshop on Mobile Big Data, 2015, pp. 37-42.", "links": null}, "BIBREF11": {"ref_id": "b11", "title": "Policy-driven security management for fog computing: Preliminary framework and a case study", "authors": [{"first": "C", "middle": [], "last": "Dsouza", "suffix": ""}, {"first": "G", "middle": [], "last": "Ahn", "suffix": ""}, {"first": "M", "middle": [], "last": "Taguinod", "suffix": ""}], "dblp_id": "conf/iri/DSouzaAT14", "year": 2014, "venue": "IEEE 15th Int. Conf. on Information Reuse and Integrate", "volume": "", "issue": "", "pages": "16--23", "other_ids": {}, "num": null, "urls": [], "raw_text": "C. Dsouza, G. Ahn, M. Taguinod, Policy-driven security management for fog computing: Preliminary framework and a case study, in: IEEE 15th Int. Conf. on Information Reuse and Integrate, 2014, pp. 16-23.", "links": null}, "BIBREF12": {"ref_id": "b12", "title": "Key ingredients in an IoT recipe: Fog Computing, Cloud computing, and more Fog Computing", "authors": [{"first": "M", "middle": [], "last": "Yannuzzi", "suffix": ""}, {"first": "R", "middle": [], "last": "Milito", "suffix": ""}, {"first": "R", "middle": [], "last": "Serral-Graci\u00e0", "suffix": ""}, {"first": "D", "middle": [], "last": "Montero", "suffix": ""}, {"first": "M", "middle": [], "last": "Nemirovsky", "suffix": ""}], "dblp_id": "conf/camad/YannuzziMSMN14", "year": 2014, "venue": "IEEE 19th Int. Workshop on Computer Aided Modeling and Design of Communication Links and Networks", "volume": "", "issue": "", "pages": "325--329", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Yannuzzi, R. Milito, R. Serral-Graci\u00e0, D. Montero, M. Nemirovsky, Key ingredients in an IoT recipe: Fog Computing, Cloud computing, and more Fog Computing, in: IEEE 19th Int. Workshop on Computer Aided Modeling and Design of Communication Links and Networks, 2014, pp. 325-329.", "links": null}, "BIBREF13": {"ref_id": "b13", "title": "Finding your way in the fog: Towards a comprehensive definition of fog computing", "authors": [{"first": "L", "middle": [], "last": "Vaquero", "suffix": ""}, {"first": "L", "middle": [], "last": "Rodero-Merino", "suffix": ""}], "dblp_id": null, "year": 2014, "venue": "ACM SIGCOMM Computer Communication Review", "volume": "", "issue": "", "pages": "27--32", "other_ids": {}, "num": null, "urls": [], "raw_text": "L. Vaquero, L. Rodero-Merino, Finding your way in the fog: Towards a comprehensive definition of fog computing, in: ACM SIGCOMM Computer Communication Review, 2014, pp. 27-32.", "links": null}, "BIBREF14": {"ref_id": "b14", "title": "Fog computing: Platform and applications", "authors": [{"first": "S", "middle": [], "last": "Yi", "suffix": ""}, {"first": "Z", "middle": [], "last": "Hao", "suffix": ""}, {"first": "Z", "middle": [], "last": "Qin", "suffix": ""}, {"first": "Q", "middle": [], "last": "Li", "suffix": ""}], "dblp_id": "conf/hotweb/YiHQL15", "year": 2015, "venue": "3rd IEEE Workshop on Hot Topics in Web Syst. and Technol", "volume": "", "issue": "", "pages": "73--78", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Yi, Z. Hao, Z. Qin, Q. Li, Fog computing: Platform and applications, in: 3rd IEEE Workshop on Hot Topics in Web Syst. and Technol., 2015, pp. 73-78.", "links": null}, "BIBREF15": {"ref_id": "b15", "title": "Agent-based approach for dynamic scheduling in content-based networks", "authors": [{"first": "R", "middle": [], "last": "Aburukba", "suffix": ""}, {"first": "H", "middle": [], "last": "Ghenniwa", "suffix": ""}, {"first": "W", "middle": [], "last": "Shen", "suffix": ""}], "dblp_id": "conf/icebe/AburukbaGS06", "year": 2006, "venue": "IEEE Int. Conf. on e-Bus. Eng", "volume": "", "issue": "", "pages": "425--432", "other_ids": {}, "num": null, "urls": [], "raw_text": "R. Aburukba, H. Ghenniwa, W. Shen, Agent-based approach for dynamic scheduling in content-based networks, in: IEEE Int. Conf. on e-Bus. Eng., 2006, pp. 425-432.", "links": null}, "BIBREF16": {"ref_id": "b16", "title": "Computational Techniques of the Simplex Method", "authors": [{"first": "I", "middle": [], "last": "Maros", "suffix": ""}], "dblp_id": null, "year": 2002, "venue": "", "volume": "61", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "I. Maros, Computational Techniques of the Simplex Method, Vol. 61, first ed., Springer, New York, 2002.", "links": null}, "BIBREF17": {"ref_id": "b17", "title": "Branch and Bound Algorithms-Principles and Examples", "authors": [{"first": "J", "middle": [], "last": "Clausen", "suffix": ""}], "dblp_id": null, "year": 1999, "venue": "", "volume": "", "issue": "", "pages": "1--30", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Clausen, Branch and Bound Algorithms-Principles and Examples, De- partment of Computer Science in University of Copenhagen, 1999, pp. 1-30.", "links": null}, "BIBREF18": {"ref_id": "b18", "title": "Branch-and-bound methods: A survey", "authors": [{"first": "E", "middle": [], "last": "Lawler", "suffix": ""}, {"first": "D", "middle": [], "last": "Wood", "suffix": ""}], "dblp_id": null, "year": 1966, "venue": "Oper. Res", "volume": "14", "issue": "", "pages": "699--719", "other_ids": {}, "num": null, "urls": [], "raw_text": "E. Lawler, D. Wood, Branch-and-bound methods: A survey, Oper. Res. 14 (1966) 699-719.", "links": null}, "BIBREF19": {"ref_id": "b19", "title": "Simulated annealing and genetic algorithms for optimal regression testing", "authors": [{"first": "N", "middle": [], "last": "Mansour", "suffix": ""}, {"first": "K", "middle": [], "last": "El-Fakih", "suffix": ""}], "dblp_id": null, "year": 1999, "venue": "J. Softw. Maint", "volume": "11", "issue": "", "pages": "19--34", "other_ids": {}, "num": null, "urls": [], "raw_text": "N. Mansour, K. El-Fakih, Simulated annealing and genetic algorithms for optimal regression testing, J. Softw. Maint. 11 (1999) 19-34.", "links": null}, "BIBREF20": {"ref_id": "b20", "title": "Optimization by simulated annealing", "authors": [{"first": "S", "middle": [], "last": "Kirkpatrick", "suffix": ""}, {"first": "C", "middle": ["D"], "last": "Gelatt", "suffix": ""}, {"first": "M", "middle": ["P"], "last": "Vecchi", "suffix": ""}], "dblp_id": null, "year": 1983, "venue": "Science", "volume": "220", "issue": "", "pages": "671--680", "other_ids": {}, "num": null, "urls": [], "raw_text": "S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, Optimization by simulated annealing, Science 220 (1983) 671-680.", "links": null}, "BIBREF21": {"ref_id": "b21", "title": "Genetic algorithms and machine learning", "authors": [{"first": "D", "middle": ["E"], "last": "Goldberg", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Holland", "suffix": ""}], "dblp_id": "conf/colt/Grefenstette93", "year": 1988, "venue": "Mach. Learn", "volume": "3", "issue": "", "pages": "95--99", "other_ids": {}, "num": null, "urls": [], "raw_text": "D.E. Goldberg, J.H. Holland, Genetic algorithms and machine learning, Mach. Learn. 3 (1988) 95-99.", "links": null}, "BIBREF22": {"ref_id": "b22", "title": "A genetic algorithm for the job shop problem", "authors": [{"first": "F", "middle": ["D"], "last": "Croce", "suffix": ""}, {"first": "R", "middle": [], "last": "Tadei", "suffix": ""}, {"first": "G", "middle": [], "last": "Volta", "suffix": ""}], "dblp_id": null, "year": 1995, "venue": "Comput. Oper. Res", "volume": "22", "issue": "", "pages": "15--24", "other_ids": {}, "num": null, "urls": [], "raw_text": "F.D. Croce, R. Tadei, G. Volta, A genetic algorithm for the job shop problem, Comput. Oper. Res. 22 (1995) 15-24.", "links": null}, "BIBREF23": {"ref_id": "b23", "title": "A budget constrained scheduling of workflow applications on utility grids using genetic algorithms", "authors": [{"first": "J", "middle": [], "last": "Yu", "suffix": ""}, {"first": "R", "middle": [], "last": "Buyya", "suffix": ""}], "dblp_id": null, "year": 2006, "venue": "Workflows in Support of Large-Scale Sci. Workshop", "volume": "", "issue": "", "pages": "1--10", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Yu, R. Buyya, A budget constrained scheduling of workflow applications on utility grids using genetic algorithms, in: Workflows in Support of Large-Scale Sci. Workshop, 2006, pp. 1-10.", "links": null}, "BIBREF24": {"ref_id": "b24", "title": "Positive Feedback As a Search Strategy", "authors": [{"first": "M", "middle": [], "last": "Dorigo", "suffix": ""}, {"first": "V", "middle": [], "last": "Maniezzo", "suffix": ""}, {"first": "A", "middle": [], "last": "Colorni", "suffix": ""}], "dblp_id": null, "year": 1991, "venue": "", "volume": "", "issue": "", "pages": "91--107", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Dorigo, V. Maniezzo, A. Colorni, Positive Feedback As a Search Strategy, Technical Report, Politecnico di Milano, 1991, 91-016.", "links": null}, "BIBREF25": {"ref_id": "b25", "title": "Cisco, Cisco IOx", "authors": [], "dblp_id": null, "year": 2017, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "Cisco, Cisco IOx, Internet: http://www.cisco.com/c/en/us/products/cloud- systems-management/iox/index.html (Accessed 8 April 2017).", "links": null}, "BIBREF26": {"ref_id": "b26", "title": "Mobile fog: A programming model for large-scale applications on the internet of things", "authors": [{"first": "K", "middle": [], "last": "Hong", "suffix": ""}, {"first": "D", "middle": [], "last": "Lillethun", "suffix": ""}, {"first": "U", "middle": [], "last": "Ramachandran", "suffix": ""}, {"first": "B", "middle": [], "last": "Ottenw\u00e4lder", "suffix": ""}, {"first": "B", "middle": [], "last": "Koldehofe", "suffix": ""}], "dblp_id": "conf/sigcomm/HongLROK13", "year": 2013, "venue": "Proceedings of the second ACM SIGCOMM workshop on Mobile cloud computing", "volume": "", "issue": "", "pages": "15--20", "other_ids": {}, "num": null, "urls": [], "raw_text": "K. Hong, D. Lillethun, U. Ramachandran, B. Ottenw\u00e4lder, B. Koldehofe, Mobile fog: A programming model for large-scale applications on the internet of things, in: Proceedings of the second ACM SIGCOMM workshop on Mobile cloud computing, 2013, pp. 15-20.", "links": null}, "BIBREF27": {"ref_id": "b27", "title": "Towards fog-driven IoT eHealth: Promises and challenges of IoT in medicine and healthcare", "authors": [{"first": "B", "middle": [], "last": "Farahani", "suffix": ""}, {"first": "F", "middle": [], "last": "Firouzi", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}, {"first": "M", "middle": [], "last": "Badaroglu", "suffix": ""}, {"first": "N", "middle": [], "last": "Constant", "suffix": ""}, {"first": "K", "middle": [], "last": "Mankodiya", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "Future Gener. Comput. Syst", "volume": "78", "issue": "", "pages": "659--676", "other_ids": {}, "num": null, "urls": [], "raw_text": "B. Farahani, F. Firouzi, V. Chang, M. Badaroglu, N. Constant, K. Mankodiya, Towards fog-driven IoT eHealth: Promises and challenges of IoT in medicine and healthcare, Future Gener. Comput. Syst. 78 (2018) 659-676.", "links": null}, "BIBREF28": {"ref_id": "b28", "title": "Improving web sites performance using edge servers in fog computing architecture", "authors": [{"first": "J", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "D", "middle": [], "last": "Chan", "suffix": ""}, {"first": "M", "middle": [], "last": "Prabhu", "suffix": ""}, {"first": "P", "middle": [], "last": "Natarajan", "suffix": ""}, {"first": "H", "middle": [], "last": "Hu", "suffix": ""}, {"first": "F", "middle": [], "last": "Bonomi", "suffix": ""}], "dblp_id": "conf/sose/ZhuCPNHB13", "year": 2013, "venue": "IEEE 7th Int. Symposium on Service Oriented System Engineering", "volume": "", "issue": "", "pages": "320--323", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Zhu, D. Chan, M. Prabhu, P. Natarajan, H. Hu, F. Bonomi, Improving web sites performance using edge servers in fog computing architecture, in: IEEE 7th Int. Symposium on Service Oriented System Engineering, 2013, pp. 320-323.", "links": null}, "BIBREF29": {"ref_id": "b29", "title": "MigCEP: operator migration for mobility driven distributed complex event processing", "authors": [{"first": "B", "middle": [], "last": "Ottenw\u00e4lder", "suffix": ""}, {"first": "B", "middle": [], "last": "Koldehofe", "suffix": ""}, {"first": "K", "middle": [], "last": "Rothermel", "suffix": ""}, {"first": "U", "middle": [], "last": "Ramachandran", "suffix": ""}], "dblp_id": "conf/debs/OttenwalderKRR13", "year": 2013, "venue": "Proc. of 7th ACM Int. Conf. on Distributed Event-Based Syst", "volume": "", "issue": "", "pages": "183--194", "other_ids": {}, "num": null, "urls": [], "raw_text": "B. Ottenw\u00e4lder, B. Koldehofe, K. Rothermel, U. Ramachandran, MigCEP: op- erator migration for mobility driven distributed complex event processing, in: Proc. of 7th ACM Int. Conf. on Distributed Event-Based Syst., 2013, pp. 183-194.", "links": null}, "BIBREF30": {"ref_id": "b30", "title": "Service-oriented heterogeneous resource sharing for optimizing service latency in mobile cloud", "authors": [{"first": "T", "middle": [], "last": "Nishio", "suffix": ""}, {"first": "R", "middle": [], "last": "Shinkuma", "suffix": ""}, {"first": "T", "middle": [], "last": "Takahashi", "suffix": ""}, {"first": "N", "middle": [], "last": "Mandayam", "suffix": ""}], "dblp_id": null, "year": 2013, "venue": "Proc. of the 1st Int. Workshop on Mobile Cloud Computing & Networking", "volume": "", "issue": "", "pages": "19--26", "other_ids": {}, "num": null, "urls": [], "raw_text": "T. Nishio, R. Shinkuma, T. Takahashi, N. Mandayam, Service-oriented heterogeneous resource sharing for optimizing service latency in mobile cloud, in: Proc. of the 1st Int. Workshop on Mobile Cloud Computing & Networking, 2013, pp. 19-26.", "links": null}, "BIBREF31": {"ref_id": "b31", "title": "A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing", "authors": [{"first": "D", "middle": [], "last": "Wu", "suffix": ""}, {"first": "S", "middle": [], "last": "Liu", "suffix": ""}, {"first": "L", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "J", "middle": [], "last": "Terpenny", "suffix": ""}, {"first": "R", "middle": [], "last": "Gao", "suffix": ""}, {"first": "T", "middle": [], "last": "Kurfess", "suffix": ""}, {"first": "J", "middle": [], "last": "Guzzo", "suffix": ""}], "dblp_id": null, "year": 2017, "venue": "J. Manuf. Syst", "volume": "43", "issue": "1", "pages": "25--34", "other_ids": {}, "num": null, "urls": [], "raw_text": "D. Wu, S. Liu, L. Zhang, J. Terpenny, R. Gao, T. Kurfess, J. Guzzo, A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing, J. Manuf. Syst. 43 (1) (2017) 25-34, Transactions of the SME.", "links": null}, "BIBREF32": {"ref_id": "b32", "title": "Distributed scheduling of event analytics across edge and cloud", "authors": [{"first": "R", "middle": [], "last": "Ghosh", "suffix": ""}, {"first": "Y", "middle": [], "last": "Simmhan", "suffix": ""}], "dblp_id": null, "year": 2016, "venue": "CoRR", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "R. Ghosh, Y. Simmhan, Distributed scheduling of event analytics across edge and cloud, CoRR (2016) (1608.01537).", "links": null}, "BIBREF33": {"ref_id": "b33", "title": "Low-latency orchestration for workflow-oriented service function chain in edge computing", "authors": [{"first": "G", "middle": [], "last": "Sun", "suffix": ""}, {"first": "Y", "middle": [], "last": "Li", "suffix": ""}, {"first": "Y", "middle": [], "last": "Li", "suffix": ""}, {"first": "D", "middle": [], "last": "Liao", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "Future Gener. Comput. Syst", "volume": "85", "issue": "", "pages": "116--128", "other_ids": {}, "num": null, "urls": [], "raw_text": "G. Sun, Y. Li, Y. Li, D. Liao, V. Chang, Low-latency orchestration for workflow-oriented service function chain in edge computing, Future Gener. Comput. Syst. 85 (2018) 116-128.", "links": null}, "BIBREF34": {"ref_id": "b34", "title": "Efficient algorithm for traffic engineering in cloud-of-things and edge computing", "authors": [{"first": "J", "middle": [], "last": "Sun", "suffix": ""}, {"first": "S", "middle": [], "last": "Sun", "suffix": ""}, {"first": "K", "middle": [], "last": "Li", "suffix": ""}, {"first": "D", "middle": [], "last": "Liao", "suffix": ""}, {"first": "A", "middle": ["K"], "last": "Sangaiah", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "Comput. Electr. Eng", "volume": "69", "issue": "", "pages": "610--627", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Sun, S. Sun, K. Li, D. Liao, A.K. Sangaiah, V. Chang, Efficient algorithm for traffic engineering in cloud-of-things and edge computing, Comput. Electr. Eng. 69 (2018) 610-627.", "links": null}, "BIBREF35": {"ref_id": "b35", "title": "A cybersecurity framework to identify malicious edge device in fog computing and cloud-of-things environments", "authors": [{"first": "A", "middle": ["S"], "last": "Sohal", "suffix": ""}, {"first": "R", "middle": [], "last": "Sandhu", "suffix": ""}, {"first": "S", "middle": ["K"], "last": "Sood", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}], "dblp_id": null, "year": 2018, "venue": "Comput. Secur", "volume": "74", "issue": "", "pages": "340--354", "other_ids": {}, "num": null, "urls": [], "raw_text": "A.S. Sohal, R. Sandhu, S.K. Sood, V. Chang, A cybersecurity framework to identify malicious edge device in fog computing and cloud-of-things environments, Comput. Secur. 74 (2018) 340-354.", "links": null}, "BIBREF36": {"ref_id": "b36", "title": "Scheduling scientific workflow applications with deadline and budget constraints using genetic algorithms", "authors": [{"first": "J", "middle": [], "last": "Yu", "suffix": ""}, {"first": "R", "middle": [], "last": "Buyya", "suffix": ""}], "dblp_id": null, "year": 2006, "venue": "Sci. Program", "volume": "14", "issue": "", "pages": "217--230", "other_ids": {}, "num": null, "urls": [], "raw_text": "J. Yu, R. Buyya, Scheduling scientific workflow applications with deadline and budget constraints using genetic algorithms, Sci. Program. 14 (2006) 217-230.", "links": null}, "BIBREF37": {"ref_id": "b37", "title": "Scheduling of scientific workflows in the ASKALON grid environment", "authors": [{"first": "M", "middle": [], "last": "Wieczorek", "suffix": ""}, {"first": "R", "middle": [], "last": "Prodan", "suffix": ""}, {"first": "T", "middle": [], "last": "Fahringer", "suffix": ""}], "dblp_id": null, "year": 2005, "venue": "ACM SIGMOD Record", "volume": "34", "issue": "", "pages": "56--62", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Wieczorek, R. Prodan, T. Fahringer, Scheduling of scientific workflows in the ASKALON grid environment, in: ACM SIGMOD Record, Vol. 34, 2005, pp. 56-62.", "links": null}, "BIBREF39": {"ref_id": "b39", "title": "Deadline sensitive lease scheduling in cloud computing environment using AHP", "authors": [{"first": "S", "middle": ["C"], "last": "Nayak", "suffix": ""}, {"first": "C", "middle": [], "last": "Tripathy", "suffix": ""}], "dblp_id": null, "year": 2016, "venue": "J. King Saud Univ. Comput. Inf. Sci", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "S.C. Nayak, C. Tripathy, Deadline sensitive lease scheduling in cloud computing environment using AHP, J. King Saud Univ. Comput. Inf. Sci. (2016).", "links": null}, "BIBREF40": {"ref_id": "b40", "title": "Auto-scaling to minimize cost and meet application deadlines in cloud workflows", "authors": [{"first": "M", "middle": [], "last": "Mao", "suffix": ""}, {"first": "M", "middle": [], "last": "Humphrey", "suffix": ""}], "dblp_id": "conf/sc/MaoH11", "year": 2011, "venue": "Int. Conference of High Performance Computing, Networking, Storage and Analysis", "volume": "", "issue": "", "pages": "1--12", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Mao, M. Humphrey, Auto-scaling to minimize cost and meet application deadlines in cloud workflows, in: Int. Conference of High Performance Computing, Networking, Storage and Analysis, 2011, pp. 1-12.", "links": null}, "BIBREF41": {"ref_id": "b41", "title": "A dynamic critical path algorithm for scheduling scientific workflow applications on global grids", "authors": [{"first": "M", "middle": [], "last": "Rahman", "suffix": ""}, {"first": "S", "middle": [], "last": "Venugopal", "suffix": ""}, {"first": "R", "middle": [], "last": "Buyya", "suffix": ""}], "dblp_id": "conf/eScience/RahmanVB07", "year": 2007, "venue": "IEEE Int. Conference of E-Science and Grid Computing", "volume": "", "issue": "", "pages": "35--42", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Rahman, S. Venugopal, R. Buyya, A dynamic critical path algorithm for scheduling scientific workflow applications on global grids, in: IEEE Int. Conference of E-Science and Grid Computing, 2007, pp. 35-42.", "links": null}, "BIBREF42": {"ref_id": "b42", "title": "An adaptive procedure for task scheduling optimization in mobile cloud computing", "authors": [{"first": "P", "middle": [], "last": "Hung", "suffix": ""}, {"first": "E", "middle": [], "last": "Huh", "suffix": ""}], "dblp_id": null, "year": 2015, "venue": "Math. Probl. Eng", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "P. Hung, E. Huh, An adaptive procedure for task scheduling optimization in mobile cloud computing, Math. Probl. Eng. (2015).", "links": null}, "BIBREF43": {"ref_id": "b43", "title": "A rear-end collision prediction scheme based on deep learning in the Internet of Vehicles", "authors": [{"first": "C", "middle": [], "last": "Chen", "suffix": ""}, {"first": "H", "middle": [], "last": "Xiang", "suffix": ""}, {"first": "T", "middle": [], "last": "Qiu", "suffix": ""}, {"first": "C", "middle": [], "last": "Wang", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhou", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}], "dblp_id": null, "year": 2017, "venue": "J. Parallel Distrib. Comput", "volume": "117", "issue": "", "pages": "192--204", "other_ids": {}, "num": null, "urls": [], "raw_text": "C. Chen, H. Xiang, T. Qiu, C. Wang, Y. Zhou, V. Chang, A rear-end collision prediction scheme based on deep learning in the Internet of Vehicles, J. Parallel Distrib. Comput. 117 (2017) 192-204.", "links": null}, "BIBREF44": {"ref_id": "b44", "title": "An integrated neutrosophic ANP and VIKOR method for achieving sustainable supplier selection: A case study in importing field", "authors": [{"first": "M", "middle": [], "last": "Abdel-Baset", "suffix": ""}, {"first": "V", "middle": [], "last": "Chang", "suffix": ""}, {"first": "A", "middle": [], "last": "Gamal", "suffix": ""}, {"first": "F", "middle": [], "last": "Smarandache", "suffix": ""}], "dblp_id": null, "year": 2019, "venue": "Comput. Ind", "volume": "106", "issue": "", "pages": "94--110", "other_ids": {}, "num": null, "urls": [], "raw_text": "M. Abdel-Baset, V. Chang, A. Gamal, F. Smarandache, An integrated neutro- sophic ANP and VIKOR method for achieving sustainable supplier selection: A case study in importing field, Comput. Ind. 106 (2019) 94-110.", "links": null}, "BIBREF45": {"ref_id": "b45", "title": "The quadratic assignment problem", "authors": [{"first": "R", "middle": ["E"], "last": "Burkard", "suffix": ""}, {"first": "E", "middle": [], "last": "C\u00b8ela", "suffix": ""}, {"first": "L", "middle": [], "last": "Pitsoulis", "suffix": ""}], "dblp_id": null, "year": 1998, "venue": "Handbook of Combinatorial Optimization", "volume": "", "issue": "", "pages": "241--339", "other_ids": {}, "num": null, "urls": [], "raw_text": "R.E. Burkard, E. C\u00b8ela, L. Pitsoulis, The quadratic assignment problem, in: Handbook of Combinatorial Optimization, in: Computer-aided chemical engineering, Kluwer Academic Publishers, Dordrecht, 1998, pp. 241-339.", "links": null}, "BIBREF46": {"ref_id": "b46", "title": "Modelling of a roulette wheel selection operator in genetic algorithms using generalized nets", "authors": [{"first": "T", "middle": [], "last": "Pencheva", "suffix": ""}, {"first": "K", "middle": [], "last": "Atanassov", "suffix": ""}, {"first": "A", "middle": [], "last": "Shannon", "suffix": ""}], "dblp_id": null, "year": 2009, "venue": "Int. J Bioautom", "volume": "13", "issue": "", "pages": "257--264", "other_ids": {}, "num": null, "urls": [], "raw_text": "T. Pencheva, K. Atanassov, A. Shannon, Modelling of a roulette wheel selection operator in genetic algorithms using generalized nets, Int. J Bioautom. 13 (2009) 257-264.", "links": null}, "BIBREF47": {"ref_id": "b47", "title": "Computer Networking: A Top-Down Approach", "authors": [{"first": "J", "middle": ["F"], "last": "Kurose", "suffix": ""}, {"first": "W", "middle": ["R"], "last": "Keith", "suffix": ""}], "dblp_id": null, "year": 2009, "venue": "", "volume": "", "issue": "", "pages": "", "other_ids": {}, "num": null, "urls": [], "raw_text": "J.F. Kurose, W.R. Keith, Computer Networking: A Top-Down Approach, fourth ed., Addison Wesley, 2009.", "links": null}}}, "ner": [{"syntactic": ["computing paradigm", "genetic algorithms", "confusion matrices", "heuristic approaches", "optimization", "internet of things", "cloud computing", "internet", "optimization problems", "simulation environments"], "semantic": ["computing paradigm", "genetic algorithms", "confusion matrices", "heuristic approaches", "optimization", "internet of things", "internet", "optimization problems", "cloud computing"], "union": ["computing paradigm", "genetic algorithms", "heuristic approaches", "confusion matrices", "optimization", "internet of things", "internet", "simulation environments", "optimization problems", "cloud computing"], "enhanced": ["grid computing", "artificial intelligence", "heuristic programming", "image resolution", "mathematics", "architecture types", "computer science", "circuit simulation", "correlation analysis", "computer systems"], "Metrics": ["confusion matrix", "false negative"], "ProgLang": [], "Dataset": [], "MathTerm": [], "IT Framework": [], "ISO": [], "Technology": [], "Terms": ["RMSE", "computing", "Cloud"], "TechName": []}]}