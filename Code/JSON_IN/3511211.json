{
    "paper_id": "3511211",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-03-20T17:52:48.439581Z"
    },
    "title": "Unified Holistic Memory Management Supporting Multiple Big Data Processing Frameworks over Hybrid Memories",
    "authors": [
        {
            "first": "Jiacheng",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Onur",
            "middle": [],
            "last": "Mutlu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Xiaobing",
            "middle": [],
            "last": "Feng",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Harry",
            "middle": [],
            "last": "Guoqing",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "L",
            "middle": [],
            "last": "Chen",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "H",
            "middle": [],
            "last": "Cui",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "C",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "G",
            "middle": [
                "H"
            ],
            "last": "Xu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "J",
            "middle": [],
            "last": "Zigman",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed",
    "pdf_parse": {
        "abstract": [
            {
                "text": "To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy inefficient. Emerging non-volatile memory (NVM) technologies offer high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages and executed on top of a managed",
                "cite_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Modern Big Data computing exemplified by systems such as Spark and QuickCached is extremely memory intensive. Lack of memory can lead to a range of severe functional and performance issues including out-of-memory crashes, significantly degraded efficiency, or even loss of data upon node failures. Relying completely on DRAM to satisfy the memory need of a data center is costly in many different ways-e.g., large-volume DRAM is expensive and energy inefficient; furthermore, DRAM's relatively small capacity dictates that a large number of machines is often needed just to provide sufficient memory, resulting in underutilized CPU resources for workloads that cannot be easily parallelized.",
                "cite_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Emerging non-volatile memory (NVM), such as phase change memory (PCM) [49, 79, 89] , resistive random-access memory (RRAM) [78] , Spin-transfer torque memory (STT-MRAM) [46] or 3D XPoint [5] , is a promising technology that, has large memory capacity, energy efficiency and low per-GB cost, making them a supplement to traditional DRAM. NVM is on the memory bus and can be accessed via load/store instructions, enabling direct manipulation of persistent data in memory. There are two representative usages of NVM. The first is to leverage its persistence feature to ensure the persistent data structures are crash consistent and resume executions in the event of a failure [14, 21, 23, 24, 26, 27, 41, 42, 47, 48, 51, 52, 55, 56, 64, 71, 73, 81-84, 95, 96] . The second is as a supplement to traditional DRAM devices, i.e., to build the hybrid memory architecture, which benefits from both access speed of DRAM and the large capacity, low power consumption,and low per-GB cost of NVM. Our proposed approach falls into the second category. Systems with hybrid memories have received much attention [9, 11, 13, 15, 18, 25, 34, 43, 48, 49, 54, 60, 62, 63, 69-71, 76, 77, 80, 85-87, 90, 93, 94] recently from both academia and industry. The benefit of mixing NVM with DRAM for Big Data systems is obvious-NVM's high capacity makes it possible to fulfill the Unified Holistic Memory Management Supporting Multiple Big Data 2:3 high memory requirement of a Big Data workload with a small number of compute nodes, holding the promise of significantly reducing the costs of both hardware and energy in large data centers.",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 74,
                        "text": "[49,",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 75,
                        "end": 78,
                        "text": "79,",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 79,
                        "end": 82,
                        "text": "89]",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 123,
                        "end": 127,
                        "text": "[78]",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 169,
                        "end": 173,
                        "text": "[46]",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 187,
                        "end": 190,
                        "text": "[5]",
                        "ref_id": null
                    },
                    {
                        "start": 673,
                        "end": 756,
                        "text": "[14, 21, 23, 24, 26, 27, 41, 42, 47, 48, 51, 52, 55, 56, 64, 71, 73, 81-84, 95, 96]",
                        "ref_id": null
                    },
                    {
                        "start": 1097,
                        "end": 1190,
                        "text": "[9, 11, 13, 15, 18, 25, 34, 43, 48, 49, 54, 60, 62, 63, 69-71, 76, 77, 80, 85-87, 90, 93, 94]",
                        "ref_id": null
                    }
                ],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Although using NVM for Big Data systems is a promising direction, the idea has not yet been fully explored. Adding NVM na\u00efvely would lead to large performance penalties due to its significantly increased access latency and reduced bandwidth-e.g. the latency of an NVM read is 2-4\u00d7 larger than that of a DRAM read and NVM's bandwidth is about 1/8-1/3 of that of DRAM [30, 75] . Hence, a critical research question that centers around all hybrid-memory-related research is how to perform intelligent data allocation and migration between DRAM and NVM so that we can maximize the overall energy efficiency while minimizing the performance overhead? To answer this question in the context of Big Data processing, there are two major challenges.",
                "cite_spans": [
                    {
                        "start": 366,
                        "end": 370,
                        "text": "[30,",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 371,
                        "end": 374,
                        "text": "75]",
                        "ref_id": "BIBREF75"
                    }
                ],
                "section": "Problems",
                "sec_num": "1.1"
            },
            {
                "text": "A common approach to managing hybrid memories is to modify the OS or hardware to (1) monitor access frequency of physical memory pages and (2) move the hot (frequently accessed) data into DRAM. This approach works well for native language applications where data stays in the memory location it is allocated into. However, in managed languages, the garbage collector keeps changing the data layout in memory by copying objects to different physical memory pages, which breaks the bonding between data and physical memory address. Most Big Data systems are written in such managed languages, e.g., Java and Scala, for the quick development cycle and rich community support they provide. Managed languages are executed on top of a managed runtime such as the JVM, which employs a set of sophisticated memory management techniques such as garbage collection. As a traditional garbage collector is not aware of hybrid memories, allocating and migrating hot/cold pages at the OS level can easily lead to interference between these two different levels of memory management.",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 84,
                        "text": "(1)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "section": "Challenge #1: Working with Garbage Collection (GC).",
                "sec_num": "1.1.1"
            },
            {
                "text": "Working with Application-Level Memory Subsystems. Modern Big Data systems all contain sophisticated memory subsystems that perform various memory management tasks at the application level. For instance, Apache Spark [6] uses resilient distributed datasets (RDDs) as its data abstraction. An RDD is a distributed data structure that is partitioned across different servers. At a low level, each RDD partition is an array of Java objects, each representing a data tuple. RDDs are often immutable but can exhibit diverse lifetime behavior. For example, developers can explicitly persist RDDs in memory for memorization or fault tolerance. Such RDDs are long-lived while RDDs storing intermediate results are short-lived.",
                "cite_spans": [
                    {
                        "start": 216,
                        "end": 219,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    }
                ],
                "section": "Challenge #2:",
                "sec_num": "1.1.2"
            },
            {
                "text": "An RDD can be at one of many storage levels (e.g., memory, disk, unmaterialized, etc.). Spark further allows developers to specify, with annotations, where an RDD should be allocated, e.g., in the managed heap or native memory. Objects allocated natively are not subject to GC, leading to increased efficiency. However, data processing tasks, such as shuffle, join, map, or reduce, are performed over the managed heap. A native-memory-based RDD cannot be directly processed unless it is first moved into the heap. Hence, where to allocate an RDD depends on when and how it is processed. For example, a frequently accessed RDD should be placed in DRAM while a native-memory-based RDD would not be frequently used and placing it in NVM would be desirable. Clearly, efficiently using hybrid memories requires appropriate coordination between these orthogonal data placement polices, i.e., the heap, native memory, or disk, vs. NVM or DRAM.",
                "cite_spans": [],
                "section": "Challenge #2:",
                "sec_num": "1.1.2"
            },
            {
                "text": "Another instance is QuickCached, which is a pure Java implementation of Memcached server based on QuickServer [3] . In particular, it is an in-memory key-value store for small chunks of arbitrary data (strings, objects) from results of database calls, API calls, or page rendering. QuickCached leverages ConcurrentHashMap and So f tRe f erence for managing its memory, i.e., ConcurrentHashMap for storing the key-value data, and So f tRe f erence for automatically clearing data at the discretion of the garbage collector in response to memory demand.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 113,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "section": "Challenge #2:",
                "sec_num": "1.1.2"
            },
            {
                "text": "In summary, the key challenges in supporting hybrid memories for Big Data processing lie in how to develop runtime system techniques that can make memory allocation/migration decisions that match how data is actually used in an application. Although techniques such as Espresso [80] and Write Rationing [11] support NVM for managed programs, neither of them was designed for Big Data processing whose data usage is greatly different than that of regular, non-data-intensive Java applications [65, 66] .",
                "cite_spans": [
                    {
                        "start": 278,
                        "end": 282,
                        "text": "[80]",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 303,
                        "end": 307,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 492,
                        "end": 496,
                        "text": "[65,",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 497,
                        "end": 500,
                        "text": "66]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "section": "State of the Art.",
                "sec_num": "1.1.3"
            },
            {
                "text": "For example, Espresso defines a new programming model that can be used by the developer to allocate objects in persistent memory. However, real-world developers would be reluctant to completely re-implement their systems from scratch using such a new model. Shoaib et al. [11] introduced the Write Rationing GC, which moves the objects that experience a large/small number of writes into DRAM/NVM to prolong NVM's lifetime. Write Rationing pioneers the work of using the GC to migrate objects based on their access patterns. However, Big Data systems make heavy use of immutable datasets-for example, in Spark, most RDDs are immutable. Placing all immutable RDDs into NVM can incur a large overhead as many of these RDDs are frequently read and an NVM read is 2-4\u00d7 slower than a DRAM read.",
                "cite_spans": [
                    {
                        "start": 272,
                        "end": 276,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "State of the Art.",
                "sec_num": "1.1.3"
            },
            {
                "text": "1.2.1 Our Insight. We analyzed two representative big data systems, i.e., Spark for data processing and QuickCached for data store, and we observed that even they exhibit diverse memory behaviors, we have opportunities to share the common memory management strategy in JVM. Our observations are as follows:",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "\u2022 Spark applications have two unique characteristics that can greatly aid hybrid memory management. First, they perform bulk object creation, and data objects exhibit strong epochal behavior and clear access patterns. For example, Spark developers program with RDDs, each of which contains objects with exactly the same access/lifetime patterns. Exploiting these patterns at the runtime would make it much easier for Big Data applications to enjoy the benefits of hybrid memory.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Second, the data access and lifetime patterns are often statically observable in the user program. For example, an RDD is a coarse-grained data abstraction in Spark and the access patterns of different RDDs can often be inferred from the way they are created and used in the program (Section 2).",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "\u2022 QuickCached has one unique characteristic that can aid hybrid memory management. It uses a huge hash table for storing the key-value pairs, and when processing each query request, it would create a large number temporary objects which would be frequently accessed in a very short period of time. Therefore, the lifetime of frequently accessed data is short, and the data with long lifetime are infrequently accessed.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Hence, unlike regular, non-data-intensive applications for which profiling is often needed to understand the access patterns of individual objects, we can develop a simple static analysis for a Big Data application to infer the access pattern of each coarse-grained data collection, in which all objects share the same pattern. This observation aligns well with prior work (e.g., Facade [66] or Yak [65] ) that requires simple annotations to specify epochs to perform efficient garbage collection for Big Data systems. The static analysis does not incur any runtime overhead, yet it can produce precise enough data access information for the runtime system to perform effective allocation and migration.",
                "cite_spans": [
                    {
                        "start": 387,
                        "end": 391,
                        "text": "[66]",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 399,
                        "end": 403,
                        "text": "[65]",
                        "ref_id": "BIBREF65"
                    }
                ],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:5 1.2.2 Panthera. Based on our extensive experience with Big Data applications, we propose Panthera, which divides a mess of data objects into several data collections according to application's semantics and infers the coarse-grained data usage behavior by lightweight static program analysis and dynamic data usage monitoring. Panthera leverages garbage collection to migrate data between DRAM and NVM, incurring almost no runtime overhead.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "We select two big data processing frameworks in this article. First, we focus on Apache Spark as it is the de-facto data-parallel framework deployed widely in industry. Spark hosts a range of applications in machine learning, graph analytics, stream processing, and so on, making it worthwhile to build a specialized runtime system, which can provide immediate benefit to all applications running atop. Furthermore, to demonstrate the generality of our approach, Panthera is built also on QuickCached, a Java implementation of Memcached, and Section 4 provides a detailed discussion of Panthera's applicability.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Panthera enhances both the JVM and Spark/QuickCached with two major innovations. First, based on the observation that access patterns in a Big Data application can be identified statically, we develop two static analyzers (Section 3) for Spark and QuickCached, respectively. In particular, the Spark analyzer analyzes a Spark program to infer a memory tag (i.e., NVM or DRAM) for each RDD variable based on the variable's location and the way it is used in the program, and the QuickCached analyzer analyzes the QuickCached source code to identify the huge global hash table, and then infer its corresponding memory tags. These tags indicate which memory the objects should be allocated in.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Second, we develop a new semantics-aware and physical-memory-aware generational GC (Section 4). Our static analysis instruments the Spark program and QuickCached to pass the inferred memory tags down to the runtime system, which uses these tags to make allocation/migration decisions. Since our GC is based on a high-performance generational GC in OpenJDK, Panthera's heap has two spaces, representing a young and an old generation. We place the entire young generation in DRAM while splitting the old generation into a small DRAM component and a large NVM component. The insight driving this design is based on a set of key observations (discussed in Section 2 in detail) we make over the lifetimes and access patterns of RDDs in representative Spark executions and the QuickCached objects:",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "\u2022 Most objects are allocated initially in the young generation. Since they are frequently accessed during initialization, placing them in DRAM enables fast access to them. \u2022 Long-lived objects in Spark can be roughly classified into two categories: (1) long-lived RDDs that are frequently accessed during data transformation (e.g., cached for iterative algorithms) and ( 2) long-lived RDDs that are cached primarily for fault tolerance. The first category of RDDs should be placed in the DRAM component of the old generation because they have long lifespans and DRAM provides desirable performance for frequent access to them. The second category should be placed in the NVM component of the old generation because they are infrequently accessed and hence NVM's large access latency has relatively small impact on overall performance. \u2022 For Spark programs, there are also short-lived RDDs that store temporary, intermediate results. These RDDs die and are then reclaimed in the young generation quickly, leading to frequent accesses to this area. This is another reason why we place the young generation within DRAM. \u2022 For QuickCached, there is only one long-lived object, i.e., ConcurrentHashMap for storing the key-value data. Among the hash table, only a small fraction is frequently accessed for a specific request, which will be identified at runtime with negligible overhead. Thus, the ConcurrentHashMap should be placed in the NVM component of the old generation, except the identified frequently accessed fraction. \u2022 For QuickCached, a number of temporary objects would be created to process a specific request. These objects are allocated in the young generation, and should be placed in DRAM enabling fast accesses.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "Based on these observations, we modified both the minor and major GC, which allocate and migrate data objects, based on their RDD types and the semantic information inferred by our static analysis, into the spaces that best fit their lifetimes and access patterns. Our runtime system also monitors the transformations invoked over RDD objects to perform runtime (re)assessment of RDDs' access patterns. Even if the static analysis does not accurately predict an RDD's access pattern and the RDD gets allocated in an undesirable space, Panthera can still migrate the RDD from one space to another using the major GC.",
                "cite_spans": [],
                "section": "Our Contributions",
                "sec_num": "1.2"
            },
            {
                "text": "We have evaluated Panthera extensively with Spark applications, including graph computing (GraphX), machine learning (MLlib) and other iterative in-memory computing applications (Table 4 ), and QuickCached using Yahoo! Cloud Serving Benchmark (YCSB) [22] . Results with various heap sizes and DRAM ratios demonstrate that Panthera makes effective use of hybrid memories-overall, the Panthera-enhanced JVM reduces the memory energy by 22%-34% with only a 1%-9% execution time overhead for QuickCached, and reduces the memory energy by 32%-53% with only less than 1% execution time overhead on average for Spark, whereas Write Rationing [11] that moves read-only RDD objects into NVM incurs a 41% time overhead.",
                "cite_spans": [
                    {
                        "start": 250,
                        "end": 254,
                        "text": "[22]",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 635,
                        "end": 639,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "Results.",
                "sec_num": "1.2.3"
            },
            {
                "text": "This section provides necessary background on Apache Spark [6] and QuickCached [3] with motivating examples that illustrate the access patterns in a Spark program.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 62,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 79,
                        "end": 82,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "section": "BACKGROUND AND MOTIVATION",
                "sec_num": "2"
            },
            {
                "text": "Spark is a data-parallel system that supports acyclic data flow and in-memory computing. The major data representation used by Spark is resilient distributed dataset (RDD) [91] , which represents a read-only collection of tuples. An RDD is a distributed memory abstraction partitioned in the cluster. Each partition is an array of data items of the same type. Each node maintains an RDD partition, which is essentially a multi-layer Java data structure-a top RDD object references a Java array, which, in turn, references a set of tuple objects such as key-value pairs. Figure 1 shows the heap structure for an example RDD where each element is a pair of a string (key) and a compact buffer (value).",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 176,
                        "text": "[91]",
                        "ref_id": "BIBREF91"
                    }
                ],
                "section": "Spark Basics",
                "sec_num": "2.1"
            },
            {
                "text": "A Spark pipeline consists of a sequence of transformations and actions over RDDs. A transformation produces a new RDD from a set of existing RDDs; examples are map, reduce, or join. An action is a function that computes statistics from an RDD, such as an aggregation. Spark leverages lazy evaluation for efficiency, that is, a transformation may not be evaluated until an action is performed later on the resulting RDD. Before data processing starts, the dependences between RDDs are first extracted from the transformations to form a lineage graph, which can be used to conduct lazy evaluation and RDD recomputation upon node failures.",
                "cite_spans": [],
                "section": "Spark Basics",
                "sec_num": "2.1"
            },
            {
                "text": "With lazy evaluation, a transformation only creates a (top-level) RDD object without materializing the RDD (i.e., the point at which its internal array and actual data tuples are created). Recomputing all RDDs is time-consuming when the lineage is long or when it branches out, and hence, Spark allows developers to cache certain RDDs in memory (by using the API persist) . Developers can specify a storage level for a persisted RDD, e.g., in memory or on disk, in the serialized or deserialized form, and the like. RDDs that are not explicitly persisted are temporary RDDs that will be garbage-collected when they are no longer used, while persisted RDDs are materialized and never collected.",
                "cite_spans": [],
                "section": "Spark Basics",
                "sec_num": "2.1"
            },
            {
                "text": "The Spark scheduler examines the lineage graph to build a DAG of stages for execution. The lineage (transformation)-based dependences are classified into \"narrow\" and \"wide\". A narrow dependence exists from a parent to a child RDD if each partition of the parent is used by at most one partition of the child RDD. By contrast, a wide dependence exists when each partition of the parent RDD may be used by multiple child partitions. Distinguishing these two types of dependences makes it possible for Spark to determine whether a shuffle is necessary. For example, for narrow dependences shuffling is not necessary, while for wide dependences it is.",
                "cite_spans": [],
                "section": "Spark Basics",
                "sec_num": "2.1"
            },
            {
                "text": "A Spark pipeline is split into a set of stages based on shuffles (and thus wide dependences)-each stage ends at a shuffle that writes RDDs onto the disk and the next stage starts by reading data from disk files. Transformations that exhibit narrow dependences are grouped into the same stage and executed in parallel.",
                "cite_spans": [],
                "section": "Spark Basics",
                "sec_num": "2.1"
            },
            {
                "text": "An RDD is, at a low level, an array of Java objects, which are managed by the semantics-agnostic GC in the JVM. RDDs often exhibit predictable lifetime and memory-access patterns. Our goal is to pass these patterns down to the GC, which can exploit such semantic information for efficient data placement. We provide a concrete example to illustrate these patterns.",
                "cite_spans": [],
                "section": "RDD Characteristics",
                "sec_num": "2.2"
            },
            {
                "text": "Figure 2 (a) shows the Spark program for PageRank [19] , which is a well-known graph algorithm used widely by search engines to rank web pages. The program iteratively computes the rank of each vertex based on the contributions of its in-neighbors. Three RDDs can be seen from its source code: links representing edges from the input graph, contribs containing contributions from incoming edges of each vertex, and ranks that maps each vertex to its page rank. links is a static map computed from the input while contribs and ranks are recomputed per iteration of the loop.",
                "cite_spans": [
                    {
                        "start": 50,
                        "end": 54,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    }
                ],
                "section": "RDD Characteristics",
                "sec_num": "2.2"
            },
            {
                "text": "In addition to these three developer-defined RDDs visible in the program, Spark generates many invisible RDDs to store intermediate results during execution. A special type of intermediate RDD is ShuffledRDD. Each iteration of the loop in the example forms a stage that ends at a shuffle, writing shuffled data into different disk files. In the beginning of the next stage, Spark creates a ShuffledRDD as input for the stage. Unlike other intermediate RDDs that are never materialized, ShuffledRDDs are immediately materialized because they contain data read freshly out of disk files. However, since they are not persisted, they will be collected when the stage is completed.",
                "cite_spans": [],
                "section": "RDD Characteristics",
                "sec_num": "2.2"
            },
            {
                "text": "In summary, (1) persisted RDDs are materialized at the moment the method persist is called and (2) non-persisted RDDs are not materialized unless they are ShuffleRDDs or an action is invoked on them.",
                "cite_spans": [],
                "section": "RDD Characteristics",
                "sec_num": "2.2"
            },
            {
                "text": "Figure 2 (b) shows the set of RDDs that exists within a stage (i.e., iteration) and their dependences. Suppose each RDD has three partitions (on three nodes). The dashed edges represent wide dependences (i.e., shuffles) due to the reduction on Line 17. There are totally eight RDDs generated in each iteration. ShuffledRDD [8], which stems from the reduction on Line 17, is transformed to ranks via a map transformation. ranks joins with links to form CoGroupedRDD [3] , which is then processed by four consecutive map functions, i.e., f 4 -f 7 , producing contribs at the end. For unmaterialized (blue) RDDs, the sequence of transformations (e.g., f 4 \u2022 . . . \u2022 f 7 ) is applied to each record from the source RDD in a streaming manner via iterators to produce a final record.",
                "cite_spans": [
                    {
                        "start": 465,
                        "end": 468,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "section": "Example",
                "sec_num": "2.3"
            },
            {
                "text": "For links and contribs, the developer invokes the method persist to materialize these RDDs. The storage levels indicate that links is cached in memory throughout the execution (as it is used in each iteration) while contribs generated in each iteration is kept in memory but will be serialized to disk upon memory pressure. ranks is not explicitly persisted. Hence, it is not materialized until the execution reaches Line 20 where action count is invoked on the RDD object.",
                "cite_spans": [],
                "section": "Example",
                "sec_num": "2.3"
            },
            {
                "text": "The lifetime patterns of these different RDDs fall into two categories. Non-persisted intermediate RDDs are short-lived as their data objects are generated only during a pipelined execution.",
                "cite_spans": [],
                "section": "Example",
                "sec_num": "2.3"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:9",
                "cite_spans": [],
                "section": "Example",
                "sec_num": "2.3"
            },
            {
                "text": "Persisted RDDs are long-lived and stay in memory/on disk until the end of the execution. Their access patterns are, however, more diverse. Objects in an intermediate RDD are accessed at most once during streaming. Objects in a persisted RDD can exhibit different types of behavior. For RDDs like links that are used in each iteration, their objects are frequently accessed. In contrast, RDDs like contribs are persisted primarily for speeding up recovery from faults, and hence, their objects are rarely used after generated.",
                "cite_spans": [],
                "section": "Example",
                "sec_num": "2.3"
            },
            {
                "text": "The different characteristics of DRAM and NVM make them suitable for different types of datasets. DRAM has low capacity and fast access speed, while NVM has large capacity but slow speed. Hence, DRAM is a good choice for storing small-sized, frequently accessed datasets, while largesized, infrequently accessed datasets fit naturally into NVM. The clear distinction in the lifespans and access patterns of different RDDs makes it easy for them to be placed into different memories suitable for their behavior. For example, intermediate (blue) RDDs are never materialized. Their objects are created individually during streaming and then quickly collected by the GC. These objects are allocated in the young generation and will eventually die there. As a result, the memory used as young generation is frequently reused by these short-lived objects, which cause very high read/write frequency to this part of memory. This motivates our design choice of placing the young generation in DRAM, which matches the conclusion of previous works [11, 76] .",
                "cite_spans": [
                    {
                        "start": 1038,
                        "end": 1042,
                        "text": "[11,",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1043,
                        "end": 1046,
                        "text": "76]",
                        "ref_id": "BIBREF76"
                    }
                ],
                "section": "Design Choices",
                "sec_num": "2.4"
            },
            {
                "text": "Persisted RDDs, in contrast, have all their data objects created at the same time, and thus need large storage space. Since they are kept alive indefinitely, they should be allocated directly in the old generation. One category of persisted RDDs includes those that are frequently accessed, like links; they need to be placed in DRAM. Another category includes RDDs that are rarely accessed and cached for fault tolerance, like contribs, these RDDs should be placed in NVM. This behavioral difference motivates our choice of splitting the old generation into a DRAM and an NVM component.",
                "cite_spans": [],
                "section": "Design Choices",
                "sec_num": "2.4"
            },
            {
                "text": "We perform what we suggest on a system with 128-GB memory using Spark-based PageRank as the benchmark. For this experiment, we allocate 120-GB memory for the Spark and reserve 8-GB memory for the OS and other services. For the 120-GB Spark memory, 32-GB are DRAM and others are NVM. (We varied the DRAM ratios in evaluation section.) Figure 2 (c) shows the performance and energy consumption normalized to a system with 120 GB of DRAM. Compared to using only 32-GB DRAM, adding 88-GB NVM to the system provides modest performance benefit (15%) but leads to 16% higher energy consumption, without proper data placement across DRAM and NVM (see Unmanaged, Section 7.2). After applying Panthera, RDD links and contribs are placed into DRAM and NVM, respectively. With such careful placement of data across DRAM and NVM, we find that (1) performance increases by 42% compared to using only a 32-GB DRAM, and becomes at the same level of the performance of using 120-GB DRAM; (2) energy consumption is 9% less than using only a 32-GB DRAM, and 54% less than using a 120-GB DRAM. We conclude that careful data placement between DRAM and NVM can provide the performance of large DRAM system, while keeping the energy consumption at the level of a small DRAM system.",
                "cite_spans": [],
                "section": "Design Choices",
                "sec_num": "2.4"
            },
            {
                "text": "QuickCached is a pure Java implementation of Memcached server based on QuickServer, and it serves as an in-memory key-value store for small chunks of arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.",
                "cite_spans": [],
                "section": "QuickCached Basics",
                "sec_num": "2.5"
            },
            {
                "text": "QuickCached supports different backends for organizing and managing the key-value data, and its default backend leverages ConcurrentHashMap and So f tRe f erence, i.e., ConcurrentHashMap for storing the key-value data, and So f tRe f erence for automatically clearing data at the discretion of the garbage collector in response to memory demand [3] . When processing each query request, QuickCached would create a large number of frequently accessed temporary objects, which will be destroyed when current query request is finished. Therefore, the lifetime of these frequently accessed data is short. In comparison, the ConcurrentHashMap provides full-lifecyle service and has long lifetime, however, only a small fraction in ConcurrentHashMap would be frequently accessed when processing one query request. Therefore, we have an opportunity to identify the data structure of ConcurrentHashMap statically, and identify its frequently accessed objects at runtime. Correspondingly, the ConcurrentHashMap should be placed in the NVM component of the old generation, and the identified frequently accessed objects should be placed in the DRAM component of the old generation.",
                "cite_spans": [
                    {
                        "start": 345,
                        "end": 348,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "section": "QuickCached Basics",
                "sec_num": "2.5"
            },
            {
                "text": "Based on our observation of memory access patterns, we developed a simple static analysis that extracts necessary semantic information for efficient data placement. For Spark, the access patterns of RDDs can often be identified from the program using them. Our analysis automatically infers, for each persisted RDD visible in the program, whether it should be allocated in DRAM or NVM. This information is then passed down to the runtime system for appropriate data allocation. For QuickCached, our analysis takes user-annotated source codes as input and automatically generates code for runtime data placement, with the details discussed below.",
                "cite_spans": [],
                "section": "STATIC INFERENCE OF MEMORY TAGS",
                "sec_num": "3"
            },
            {
                "text": "Static Analysis. In a Spark program, the developer can invoke persist with a particular storage level on an RDD to materialize the RDD, as illustrated in Figure 2 . We piggyback on the storage levels to further determine if a persisted RDD should be placed into DRAM or NVM. In particular, Panthera statically analyzes the program to infer a memory tag (i.e., DRAM or NVM) for each persist call. Each of the ten existing storage levels (e.g., MEMORY_ONLY), except for OFF_HEAP and DISK_ONLY, is expanded into two sub-levels, annotated with NVM and DRAM, respectively (e.g., MEMORY_ONLY_DRAM and MEMORY_ONLY_NVM). OFF_HEAP is translated directly into OFF_HEAP_NVM because RDDs placed in native memory are rarely used, while DISK_ONLY does not carry any memory tag.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "Our static analysis performs inference based on the def-use information w.r.t. each RDD variable declared in the program as well as the loop(s) in which the variable is defined/used. Our key insight is that if the variable is defined in each iteration of a computational loop, most of the RDD instances represented by the variable are not used frequently. This is because Spark RDDs are often immutable and hence, every definition of the RDD variable creates a new RDD instance at run time, leaving the old RDD instance cached and unused. Hence, we tag the variable \"NVM\", instructing the runtime system to place these RDDs in NVM. An example is the contribs variable in Figure 2 (a), which is defined in every iteration of the loop-although the variable is also used in each iteration, the use refers to the most recent RDD instance created in the last iteration while the instances created in all the other past iterations are left unused.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "By contrast, if a variable is used-only (i.e., never defined) in the loop, such as links, we create a tag \"DRAM\" for it since only one instance of the RDD exists and is repeatedly used. Panthera analyzes not only RDD variables on which persist is explicitly called, but also those on which actions are invoked, such as the ranks variable in Figure 2 (a). The tag inferred for an RDD variable (say v) is passed, at the materialization point of every RDD instance (v refers to), into the runtime system via automatically instrumented calls to auxiliary (native) methods provided by the Panthera JVM. We piggyback on a tracing GC to propagate this tag from the RDD object down to each data object contained in the RDD-when the GC runs, it moves objects with the same tag together into the same (DRAM or NVM) region (see Section 4).",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "One constraint that needs to be additionally considered is the location of the loop relative to the location of the materialization point of the RDD. We analyze the loop only if the materialization point precedes or is in the loop. Otherwise, whether the variable is used or defined in the loop does not matter as the RDD has not been materialized yet. For instance, although the ranks variable is defined in the loop that starts at Line 17, it does not get materialized until Line 20 after the loop finishes. Hence, its behavior in the loop does not affect its memory tag, which should actually depend on its def-use in the loops, if any, after Line 20.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "If no loop exists in a program, the program has only one iteration and all RDDs receive an \"NVM\" tag as none of them are repeatedly accessed. If there are multiple loops to be considered for an RDD variable, we tag it \"DRAM\" if there exists one loop in which the variable is used-only and that loop follows or contains the materialization point of the RDD. The variable receives an \"NVM\" tag otherwise. If all persisted RDDs receive an \"NVM\" tag at the end of the analysis, we change the tags of all RDDs to \"DRAM\"-the goal is to fully utilize DRAM by first placing RDDs in DRAM. Once DRAM capacity is exhausted, the remaining RDDs, including those with a \"DRAM\" tag, will be placed in NVM.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "Note that our analysis infers tags only for the RDD variables explicitly declared in the program. Intermediate RDDs produced during execution are not materialized and thus do not receive memory tags from our analysis. We discuss how to handle them in Section 4.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "The memory tag of an RDD variable is a static approximation of its access pattern, which may not reflect the behavior of all RDD instances represented by the variable at run time. However, user code for data processing often has a simple batch-transformation logic. Hence, the static information inferred from our analysis is often good enough to help the runtime make an accurate placement decision for the RDD. In case the statically inferred tags do not precisely capture the RDD's access information, Panthera has the ability to move RDDs between NVM and DRAM (within the old generation) based on their access frequencies, when a full-heap GC occurs. The dynamic data migration frequency is a good indicator for the accuracy of the static analysis. Section 4 provides a full discussion for this mechanism and Section 7.5 evaluate the accuracy of the static analysis and the overhead of dynamic migration. ShuffledRDD. Recall from Section 2 that, in addition to the RDDs on which persist is explicitly invoked, ShuffledRDDs, which are created from disk files after a shuffle, are also materialized. These RDDs are often the input of a stage but invisible in the program code. The challenge here is where to place them. Our insight is that their placement should depend on the other materialized RDDs that are transformed from (i.e., depend on) them in the same stage.",
                "cite_spans": [],
                "section": "Spark Analyzer",
                "sec_num": "3.1"
            },
            {
                "text": "For example, in Figure 2 (b), the input of the stage are two sets of ShuffledRDDs: [1] and [8] . ShuffledRDD [1] is the RDD represented by links and our static analysis already infers tag \"DRAM\" for it. ShuffledRDD [8] results from the reduction in the previous stage. Because ShuffledRDD [8] transitively produces MapPartitionRDD [7] (represented by contribs) and MapPartitionRDD [7] has a memory tag \"NVM\" inferred by our static analysis, we tag ShuffledRDD [8] \"NVM\" as well.",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 86,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 91,
                        "end": 94,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 109,
                        "end": 112,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 289,
                        "end": 292,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 331,
                        "end": 334,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 381,
                        "end": 384,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "section": "Dealing with",
                "sec_num": null
            },
            {
                "text": "The main reason is that RDDs belonging to the same stage may share many data objects for optimization purposes. For example, a map transformation that only changes the values (of keyvalue pairs) in RDD A may generate a new RDD B that references the same set of key objects as in A. If B has already received a memory tag from our static analysis, it is better to assign the same tag to A so that these shared objects do not receive inconsistent tags and would not need to be moved from one memory to another when B is generated from A. This is especially beneficial when the transformation is in a computational loop-a large number of objects would be moved if A and B have different memory tags.",
                "cite_spans": [],
                "section": "Dealing with",
                "sec_num": null
            },
            {
                "text": "Figure 3 depicts our alogrithm which assigns the same tag to A and B. We add support that scans the lineage graph at the beginning of each stage to propagate the memory tag backward, starting from the lowest materialized RDD in the graph that has received a tag from our analysis. Conflicts may occur during the propagation-an RDD encountered during the backward traversal may have an existing tag that is different from the tag being propagated. To resolve conflicts, we define the following priority order: DRAM > NVM, which means that upon a conflict, the resulting tag is always DRAM. This is because our goal is to minimize the NVM-induced overhead; RDDs with a \"DRAM\" tag inferred will be frequently used and putting them in NVM would cause large performance degradation.",
                "cite_spans": [],
                "section": "Dealing with",
                "sec_num": null
            },
            {
                "text": "For QuickCached, the core object is the storage object, i.e., the hash table ConcurrentHashMap, and our QuickCached analyzer requires users annotate this object using the following syntax, @CoreHashObject, e.g.,",
                "cite_spans": [],
                "section": "QuickCached Analyzer",
                "sec_num": "3.2"
            },
            {
                "text": "According to the observation that only a small fraction of the hash table will be frequently accessed during the processing of one query request, thus the annotation will guide Panthera to place the hash table in NVM, meanwhile keep only the frequently accessed fraction in DRAM. In particular, the QuickCached analyzer identifies the annotated hash table ConcurrentHashMap and infer its memory tag as \"NVM\" statically.",
                "cite_spans": [],
                "section": "@CoreHashObject ConcurrentHashMap hashTable;",
                "sec_num": null
            },
            {
                "text": "However, different with Spark applications, the frequently accessed data in ConcurrentHashMap cannot be statically identified, since it is determined by the incoming requests at runtime. Thus, the static analysis is inefficient to infer meaningful tags for the objects stored in ConcurrentHashMap. To address this problem, the QuickCached analyzer introduced a dynamic mechanism to distinguish the frequently accessed data and tag it as \"DRAM\" at runtime. Since data accesses are highly skewed in real-world workloads [16] , the frequently accessed data can be identified by monitoring the data access patterns at runtime. In particular, the analyzer automatically inserts some instrumentation codes at the callsites of the \u0434et method which is the interface for accessing the annotated ConcurrentHashMap. The instrumented codes are shown in Figure 4 , serving to leverage a simple LRU strategy to tag the most recently accessed value data as \"DRAM\".",
                "cite_spans": [
                    {
                        "start": 518,
                        "end": 522,
                        "text": "[16]",
                        "ref_id": "BIBREF15"
                    }
                ],
                "section": "@CoreHashObject ConcurrentHashMap hashTable;",
                "sec_num": null
            },
            {
                "text": "In Spark, the RDD is an abstraction which is an array of Java objects at a low level, and such semantics facilitates the memory tags analysis and passing in Panthera. However, QuickCached lacks of such RDD abstraction, therefore, to share the same memory tag passing mechanism with Spark, we synthesize an RDD to wrap the objects that need to be managed by Panthera.",
                "cite_spans": [],
                "section": "@CoreHashObject ConcurrentHashMap hashTable;",
                "sec_num": null
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:13 In particular, the instrumented codes work as follows. First, rdd_indicator(\"DRAM\") ((Line 1)) declares the synthesized RDD in QuickCached, which will behave as the RDD in Spark. Second, we allocate a fixed-size auxiliary object array A\u0434\u0434DramV alues which will be wrapped in the synthesized RDD, to aggregate the most recently accessed value data together (Lines 2 and 3). Finally, the most recently visited elements would be copied into A\u0434\u0434DramV alues when they are accessed (Line 10).",
                "cite_spans": [],
                "section": "@CoreHashObject ConcurrentHashMap hashTable;",
                "sec_num": null
            },
            {
                "text": "With the synthesized RDD, Panthera provides a unified memory-tag-passing mechanism which can support both Spark and QuickCached, as will be discussed in Section 4.2.",
                "cite_spans": [],
                "section": "@CoreHashObject ConcurrentHashMap hashTable;",
                "sec_num": null
            },
            {
                "text": "While our static analysis (Section 3) determines where RDDs should be allocated, this information has to be communicated down to the runtime system, which recognizes only objects, not RDDs. Hence, our goal is to develop a new GC that, when placing/moving data objects, is aware of (1) the high-level semantics about where (DRAM or NVM) these RDDs should be placed and ( 2) the lowlevel information about the RDDs to which these objects belong.",
                "cite_spans": [],
                "section": "THE PANTHERA GARBAGE COLLECTOR",
                "sec_num": "4"
            },
            {
                "text": "We have implemented our new collection algorithm in OpenJDK 8 (build jdk8u76-b02) [8] . In particular, we have modified the object allocator, the interpreter, the two JIT compilers (C1 and Opto), and the Parallel Scavenge collector.",
                "cite_spans": [
                    {
                        "start": 82,
                        "end": 85,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    }
                ],
                "section": "THE PANTHERA GARBAGE COLLECTOR",
                "sec_num": "4"
            },
            {
                "text": "Heap Design. The Panthera GC is based on the Parallel Scavenge collector, which is the default GC in OpenJDK8. The collector divides the heap into a young and an old generation. As discussed earlier in Section 1, Panthera places the young generation in DRAM and splits the old generation into a DRAM component and an NVM component. The off-heap native memory is placed entirely in NVM. We reserve two unused bits, referred to as MEMORY_BITS, from the header of each object to indicate whether the object should be allocated into DRAM (01) or NVM (10) . The default value for these bits is 00-objects that do not receive a tag have this default value. They will be promoted to the NVM component of the old generation if they live long enough. Figure 5 illustrates the heap structure and our allocation policies.",
                "cite_spans": [
                    {
                        "start": 546,
                        "end": 550,
                        "text": "(10)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Allocation Policies. As discussed in Section 3, each materialized RDD carries a memory tag that comes from our static analysis or lineage-based tag propagation. However, at a low level, an RDD is a structure of objects, as illustrated in Figure 1 , and these objects are created at different points in the execution. Our goal is to place all objects belonging to the same logical RDD-including the top object, the array object, tuple objects, and other objects reachable from tuples-together in the space suggested by the RDD's memory tag, because these objects likely have the same access pattern and lifetime.",
                "cite_spans": [],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "However, this is rather challenging-our static analysis infers a memory tag for each top RDD object (whose type is a subtype of org.apache.spark.rdd.RDD) in the user program and we do not know what other objects belong to this RDD by just analyzing the user program. Statically identifying what objects belong to a logical data structure would require precise context-sensitive static analysis of both user and system code, which is difficult to do due to Spark's extremely large codebase and the scalability issues of static analysis.",
                "cite_spans": [],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Our idea to solve this problem is that instead of attempting to allocate all objects of an RDD directly into the space (say S) suggested by the RDD's tag, we allocate only the array object into S upon its creation. This is much easier to do-Panthera instruments each materialization point (e.g., before a call to persist or a Spark action) in the user program to pass the tag down to the runtime system without needing to analyze the Spark system code. Since the array is created at materialization, the runtime system can just use the tag to determine where to place it. All other objects in the RDD are not immediately allocated in S due to the aforementioned difficulties in finding their allocation sites. They are instead allocated in the young generation. Later, we use the GC to move these objects into S as tracing is performed.",
                "cite_spans": [],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Another important reason why we first allocate the array object into S is because the array is often much larger than the top and tuple objects. It is much more efficient to allocate it directly into the space it belongs to rather than allocating it somewhere else and moving it later.",
                "cite_spans": [],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "Table 1 shows our allocation policies for different types of objects in an RDD. For RDDs with tag \"DRAM\", array objects are allocated directly into the DRAM component of the old generation if it has enough space. Otherwise, they have to be allocated in the NVM component. For RDDs with tag \"NVM\", array objects are allocated directly into the NVM component. Intermediate RDDs without tags are all allocated in the young generation (DRAM). Most of them end up dying there and never get promoted, while a small number of objects that eventually become old enough will be promoted to the NVM space of the old generation. Top RDD objects and data tuple objects, as discussed earlier, are all allocated into the young generation and moved later by the GC to the spaces containing their corresponding arrays.",
                "cite_spans": [],
                "section": "Design Overview",
                "sec_num": "4.1"
            },
            {
                "text": "This subsection describes our implementation techniques and various optimizations.",
                "cite_spans": [],
                "section": "Implementation and Optimization",
                "sec_num": "4.2"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:15 ",
                "cite_spans": [],
                "section": "Implementation and Optimization",
                "sec_num": "4.2"
            },
            {
                "text": "Tags. Right before each materialization point (i.e., the invocation of persist or a Spark action), our analysis inserts a call to a native method rdd_indicator(rdd, tag), with the RDD's top object (rdd) and the inferred memory tag (tag) as the arguments. This method first sets a thread-local state variable to DRAM or NVM, according to the tag, informing the current thread that a large array for an RDD will be allocated soon. Next, rdd_indicator sets the MEMORY_BITS of the top object rdd based on tag. Regardless of where it currently is, this top object will eventually be moved by the GC to the space corresponding to tag.",
                "cite_spans": [],
                "section": "Passing",
                "sec_num": "4.2.1"
            },
            {
                "text": "The thread then transitions into a \"wait\" state, waiting for this large array. In this state, the first allocation request for an array whose length exceeds a user-defined threshold (i.e., a million used in our experiments) is recognized as the RDD array. Panthera then allocates the array directly into the space indicated by tag. To implement this, we modified both the fast allocation path, assembly code generated by the JIT compiler, and the slow path, functions implemented in C++. After this allocation, the state variable is reset and the thread exits the wait state. If tag is null, the array is allocated in the young generation, preferably through the thread-local allocation buffer (TLAB), and the MEMORY_BITS of the top object remains as the default value (00).",
                "cite_spans": [],
                "section": "Passing",
                "sec_num": "4.2.1"
            },
            {
                "text": "There are two major challenges in how to move objects: crossgeneration migration and object compaction. As Panthera piggybacks on a generational GC where a minor GC is triggered when JVM is unable to allocate space for a new object, objects in the young generation that survive several minor GCs are deemed \"long-lived\" and moved into the old generation. The major GC is triggered when the old generation is full. We leverage this opportunity to move together objects that belong to the same logical RDD-as discussed earlier, these objects might not have been allocated in the same space initially.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "Minor GC. To do this, we modified the minor collection algorithm in the Parallel Scavenge GC on which Panthera is built. The existing minor GC contains three tasks: root-task, which performs object tracing from the roots (e.g., stack and global variables); old-to-young-task, which scans references from objects in the old generation to those in the young generation to identify (directly or transitively) reachable objects; and steal-task, which performs work stealing for load balancing. To support our object migration, we split old-to-young-task into a DRAM-to-young-task and NVMto-young-task, which find objects that should be moved into the DRAM and NVM parts of the old generation, respectively.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "For these two tasks, we modified the tracing algorithm to propagate the tag-for example, scanning a reference from a DRAM-based RDD array (with tag \"DRAM\") to a tuple object (in the young generation) propagates the tag to the tuple object (by setting its MEMORY_BITS). Hence, when tracing is done, all objects reachable from the array have their MEMORY_BITS set to the same value as that of the array. In the original GC algorithm, an object does not get promoted from the young to the old generation until it survives several minor GCs (in this article, we use the threshold of 15). In Panthera, however, we move the objects whose MEMORY_BITS is set as 01 (10) in tracing immediately to DRAM (NVM) space in the old generation, We refer to this mechanism as eager promotion. Objects whose MEMORY_BITS is not set, 00, in tracing belong to intermediate RDDs or are control objects not associated with any RDDs. The migration of these objects follows the original algorithm, that is, they will be moved only if they survive several minor GCs. During the eager promotion, if there is a lack of free DRAM space for the old generation, Panthera will put the corresponding objects into NVM and let major GC adjust the data layout during execution.",
                "cite_spans": [
                    {
                        "start": 657,
                        "end": 661,
                        "text": "(10)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "Furthermore, we also need to move RDD top objects to the appropriate part of the old generation. These top objects, whose MEMORY_BITS was set by the instrumented call to rdd_indicator at their materialization points, are visited when root-task is executed because these objects are referenced directly by stack variables. We modified the root-task algorithm to identify objects with the set MEMORY_BITS. These RDD top objects will also be moved to (the DRAM (01) or NVM (10) space of) the old generation by the minor GC.",
                "cite_spans": [
                    {
                        "start": 470,
                        "end": 474,
                        "text": "(10)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "Major GC. When a major GC runs, it performs memory compaction by moving objects together (in the old generation) to reduce fragmentation and improve locality. We modified the major GC to guarantee that compaction does not occur across the boundary between DRAM and NVM. Furthermore, when the major GC performs a full-heap scan, Panthera re-assesses, for each RDD array object, where the object should actually be placed based on the RDD's runtime access frequency. This frequency is measured by counting, using instrumentation, how many times a method (e.g., map or reduce) has been invoked on this RDD object. The RDDs are ranked based on the access frequency. The most frequently accessed RDDs will be migrated to DRAM if they are misplaced in NVM. If there isn't enough DRAM space for these high-ranking RDDs, Panthera will evict the RDDs with lower access frequencies from the DRAM.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "We maintain a hash table that maps each RDD object to the number of calls made on t he object. Our static analysis inserts, at each such call site, a JNI (Java Native Interface) call that invokes a native JVM method to increment the call frequency for the RDD object. Frequently (infrequently) accessed array objects are moved from the NVM (DRAM) space to the DRAM (NVM) space within the old generation and all objects reachable from these arrays are moved as well. Their MEMORY_BITS will be updated accordingly. At the end of each major GC, the frequency for each RDD is reset.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "The DRAM space of the old generation can be quickly filled up as it is much smaller than the NVM space. When the DRAM space is full, the minor GC moves all objects from the young generation to the NVM space of the old generation regardless of their memory tags.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "Conflicts. If an object is reachable from multiple references and different tags are propagated through them, a conflict occurs. As discussed earlier, we resolve conflicts by giving \"DRAM\" higher priority than \"NVM\". As long as the object receives \"DRAM\" from any reference, it is a DRAM object and will be moved to the DRAM space of the old generation.",
                "cite_spans": [],
                "section": "Object Migration.",
                "sec_num": "4.2.2"
            },
            {
                "text": "In OpenJDK, the heap is divided into many cards, each representing a region of 512 bytes. Every object can take one or more cards, and the write barrier maintains a card table that marks certain cards dirty upon reference writes. The card table can be used to efficiently identify references during tracing. For example, upon a.f = b, the card that contains the object referenced by a is set to dirty. When a minor GC runs, the old-to-young scavenge task cleans a card if the target objects of the (old-to-young) references contained in the memory region represented by the card have been copied to the old generation. However, if a card contains two large arrays (say A and B)-e.g., A ends in the middle of the card while B starts there immediately-significant inefficiencies can result when they are scanned by two different GC threads. The card would remain dirty even if all objects referenced by A and B have been moved from the young to the old generation-neither thread could clean the card due to its unawareness of the status of the array scanned by another thread. This would cause every minor GC to scan every element of each array in the dirty card until a major GC occurs. This is a serious problem for Big Data applications that make heavy use of large arrays. Shared cards exist pervasively when these arrays are frequently allocated and deallocated. Frequent scanning of such cards with multiple threads can incur a large overhead on NVM due to its higher read latency and reduced bandwidth. We implemented a simple optimization that adds an alignment padding for the allocation of each RDD array to make the end of the array align with the end of a card. Although this leads to space inefficiencies, the amount of wasted space is small (e.g., less than 512 bytes for each array of hundreds of megabytes) while card sharing among arrays is completely eliminated, resulting in substantial reduction in GC time.",
                "cite_spans": [],
                "section": "Card Optimization.",
                "sec_num": "4.2.3"
            },
            {
                "text": "Multiple Big Data 2:17",
                "cite_spans": [],
                "section": "Unified Holistic Memory Management Supporting",
                "sec_num": null
            },
            {
                "text": "As described in Section 3, the core idea behind Panthera is to statically infer RDDs' memory tags and pass them to the runtime system to instruct objects migration. In order to analyze the runtime behavior of the objects more precisely, we propose the profiling-guided optimization (PGO) in this section.",
                "cite_spans": [],
                "section": "PROFILING-GUIDED OPTIMIZATION",
                "sec_num": "5"
            },
            {
                "text": "Figure 6 shows the proportion of DRAM memory accesses to total memory accesses in old generation. Compared with Unmanaged, Panthera increased the proportion of DRAM access by 16.7% on average, which demonstrates that Panthera's strategy can effectively increase DRAM access proportion. In this section, we design a new experiment to analyze the access behaviors of objects in the old generation with a fine granularity.",
                "cite_spans": [],
                "section": "Memory Access Distribution",
                "sec_num": "5.1"
            },
            {
                "text": "We divide the old generation space into chunks of 1-GB size, and plot the access behaviors for each chunk in Figure 7 , using four applications, i.e., LR, TC, GraphX-CC, and MLlib. In particular, the horizontal axis uses Chunk ID to represent all the chunks, and the vertical axis represents the access proportion for each chunk, i.e., the number of accesses on the chunk divided by the number of accesses on all chunks. The chunks in Figure 7 are divided into four categories, (1) red, frequently accessed chunks and placed in DRAM, (2) yellow, infrequently accessed chunks but placed in DRAM, (3) blue, frequently accessed chunks but placed in NVM, and (4) green, infrequently accessed chunks and placed in NVM. The red and green dots show the chunks that are correctly recognized and placed. The yellow and blue dots show the chunks that are wrongly placed. From the figure, we can see that, comparing with Unmanaged, Panthera can place more frequently accessed chunks on DRAM and infrequently accessed chunks on NVM. Take the MLlib (Figure 7(d) ) for example, among the top-10 frequently-accessed chunks, the Unmanaged approach allocates two chunks on DRAM, while Panthera allocates five chunks on DRAM. However, there still exist some frequently accessed chunks that are placed on NVM, as shown by the blue dots, and some infrequently accessed chunks that are placed on DRAM, as shown by the yellow dots, which are undesirable. The reason of the data misplacement is that Panthera without PGO treats the RDD as a whole and can't distinguish the access frequency difference within the RDDs.",
                "cite_spans": [],
                "section": "Memory Access Distribution",
                "sec_num": "5.1"
            },
            {
                "text": "In summary, our key finding on Spark is that the data belonging to the same RDD do not always exhibit similar access patterns. This finding motivates us to introduce finer-grained chunk placement decision into Panthera. Therefore, we only need to perform chunk-based profiling to the data that belonging to the annotated RDD, rather than profiling for all data objects during the program execution. Therefore, we integrate the coarse-grained RDD-level analysis globally and fine-grained chunk-based profiling for some special RDDs. This would obtain precise memory access patterns in a lightweight manner.",
                "cite_spans": [],
                "section": "Memory Access Distribution",
                "sec_num": "5.1"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:19",
                "cite_spans": [],
                "section": "Memory Access Distribution",
                "sec_num": "5.1"
            },
            {
                "text": "As RDDs are multi-layer Java data structures, and Panthera without PGO analyzes the behavior at the granularity of RDD, thus all objects belonging to one RDD would be identified to have the same access pattern and lifetime. However, some objects might be shared by multiple RDDs that are identified to have different behaviors, and it brings an opportunity for more precisely allocating the objects across DRAM/NVM.",
                "cite_spans": [],
                "section": "Opportunity for Runtime Optimization",
                "sec_num": "5.2"
            },
            {
                "text": "As discussed in Section 3.1, we discussed an example of object sharing, and statically, we proposed an algorithm and attempted to assign the same memory tag to the RDDs sharing the objects as shown in Figure 3 . However, the attempt might fail, and there might bring conflicts when inferring the tag for the objects from different RDDs. For example, in Figure 8 , RDD B is generated from RDD A, B and A receive the tag of \"NVM\" and \"DRAM\", respectively. Therefore, when B is generated, the data objects that are shared by A and B will be moved into \"NVM\", even if some of them belong to A which is tagged \"DRAM\".",
                "cite_spans": [],
                "section": "Opportunity for Runtime Optimization",
                "sec_num": "5.2"
            },
            {
                "text": "Based on the observation, we have the opportunity to dynamically switch the memory tag for the shared objects, i.e., using the tag of \"NVM\" when accessing B, and \"DRAM\" when accessing A. For this purpose, we need to refine the analysis, from the granularity of RDDs to chunks, and leverage the runtime behaviors of the objects.",
                "cite_spans": [],
                "section": "Opportunity for Runtime Optimization",
                "sec_num": "5.2"
            },
            {
                "text": "To characterize the behaviors at the granularity of chunks rather than RDDs and allocate more frequently accessed chunks DRAM, we propose a profile-guided approach, which works as follows: First, we collect the memory access distribution for all the chunks by using VTune, with the chunk size of CS, as shown in Figure 7 . Second, from the access distribution, we select the top-K frequently accessed chunks, where K is determined by the DRAM capacity of old generation C and the chunk size CS using the equation of K = C/CS. Finally, the IDs for the K chunks are passed to the JVM, and when JVM starts, we bind the top-K chunks to \"DRAM\", and other chunks to \"NVM\", by invoking the mbind system call. to determine the memory address for each chunk. For example, we assume chunk size is 1GB. When a 64-GB heap is used and DRAM to memory ratio is 1/3 and the nursery space is 1/6 of the heap size, there is 10.66-GB DRAM in old generation. Then, the IDs for the top 11 frequently accessed chunks are passed to the JVM where the top-10 chunks and the first 0.66 GB of the 11th chunk will be bound into \"DRAM\". Figure 9 shows the enhanced Panthera framework with the profile-guided optimization, for executing the example in Figure 8 . In particular, in Spark analyzer, the RDDs B and A are tagged as \"DRAM\" and \"NVM\", respectively. When JVM starts, the profiled K chunks would be bound to DRAM, and other chunks to NVM. Therefore, from the perspective of RDD, B would be still allocated to the DRAM part of the old generation, and A would be still allocated to the NVM part of the old generation. However, with our underlying chunk binding, only the profiled hot K chunks of A and B would be actually allocated to DRAM.",
                "cite_spans": [],
                "section": "Optimization",
                "sec_num": "5.3"
            },
            {
                "text": "Therefore, we leverage the profiled access frequency of the memory chunks, and refine the statically determined DRAM/NVM partition, to allocate only the real hot chunks on DRAM. Figure 10 shows the results of applying the PGO to Panthera. From this figure, we see that, for the Panthera with PGO, most of the frequently accessed chunks are correctly marked with \"DRAM\" tags and placed in DRAM. Compared to the Unmanaged and Panthera without PGO, Panthera with PGO significantly improved the accumulated access proportion on DRAM of Old generation from 19.55% and 25.2% to 53.5%.",
                "cite_spans": [],
                "section": "Optimization",
                "sec_num": "5.3"
            },
            {
                "text": "Panthera's design includes two key individual mechanisms, i.e., analyzing the data access patterns which is framework-dependent such as the Spark Analyzer and QuickCached Analyzer in Section 3, and a set of framework-independent APIs that makes pretenuring, migration, and dynamic monitoring easy for any in-memory big data system using large arrays as backbone data structures. In our design, the clear and predictable data access patterns are connections between different on-top frameworks and the enhanced JVM.",
                "cite_spans": [],
                "section": "DISCUSSION ON APPLICABILITY AND GENERALITY",
                "sec_num": "6"
            },
            {
                "text": "The enhanced JVM is general since the data placement and migration mechanism provided by the Panthera runtime system can be employed to manage memory for any Big Data systems that have clear and predictable data access patterns. Examples include Apache Hadoop, Apache Flink, or database systems such as Apache Cassandra.",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "Panthera determines the data placement and migration via three general policies. These policies are integrated into the enhanced JVM thus they are generally applicable to other JVM-based big data systems.",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 First, some data structures can be pre-tenured with some tags according to their behaviors that can be statically determined. Panthera would allocate these data structures directly into the space indicated by the corresponding tags.",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:21",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "\u2022 Second, some data structures are required to collect their behaviors and determine their placement at runtime. In our current Panthera implementation, the placement of these data structures depends on their access frequencies and lifetimes, thus Panthera would collect these runtime characteristics and make the placement decision during the application execution. Furthermore, Panthera can be extended to integrate new memory access characteristics and new policies. \u2022 Third, some data structures are required to make finer-grained placement decision. For these data structures, Panthera leverages a finer-grained chunk-based memory access profiling approach that enables placements of different chunk at different memory spaces.",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "However, Panthera currently does not support applications whose data access patterns cannot be distinguished clearly, e.g., applications with random memory access.",
                "cite_spans": [],
                "section": "General Memory Management Policies",
                "sec_num": "6.1"
            },
            {
                "text": "The memory access patterns are obtained via user annotations together with static analyzers.",
                "cite_spans": [],
                "section": "Framework-Specific Access Pattern Annotations/Analyzers",
                "sec_num": "6.2"
            },
            {
                "text": "Panthera provides two major APIs, one for pre-tenuring data structures with tags and a second for dynamic monitoring and migration. The first API takes as input an array and a tag, performing data placement as discussed earlier in Section 4. The tag can come from the developer's annotations in the program or from a static analysis that is designed specifically for the framework to be optimized.",
                "cite_spans": [],
                "section": "Framework-Specific Access Pattern Annotations/Analyzers",
                "sec_num": "6.2"
            },
            {
                "text": "To illustrate, consider Apache Hadoop where both a map worker and a reduce worker may need to hold large data structures in memory. Some of these data structures are loaded from HDFS as immutable input, while others are frequently accessed. In the case of HashJoin, which is a building block for SQL engines, one input table is loaded entirely in memory while the second table is partitioned across map workers. If map workers are executed in separate threads, they all share the first table and join their own partitions of the second table with it. The first table is long-lived and frequently accessed. Hence, it should be tagged DRAM and placed in the DRAM space of the old generation, while different partitions of the second table can be placed in the young generation and they will die there quickly.",
                "cite_spans": [],
                "section": "Framework-Specific Access Pattern Annotations/Analyzers",
                "sec_num": "6.2"
            },
            {
                "text": "Panthera's second API takes as input a data structure object to track the number of calls made on the object. If this API is used to track the access frequency of the data structure, the data structure (and all objects reachable from it) would not be pre-tenured (as specified by the first API), but rather, they are subject to dynamic migration performed in the major GC. We can use this API to dynamically monitor certain objects and migrate them if their access patterns are not easy to predict statically.",
                "cite_spans": [],
                "section": "Framework-Specific Access Pattern Annotations/Analyzers",
                "sec_num": "6.2"
            },
            {
                "text": "Use of these above two APIs enables a flexible allocation/migration mechanism that allows certain parts of the data structure (e.g., for which memory tags can be easily inferred) to be pretenured and other parts to be dynamically migrated. Furthermore, the framework-specific memory access pattern analyzers would perform def-use analysis to propagate the user annotations to the runtime system. In Section 3, we demonstrated two individual analyzers for Spark and Quick-Cached, respectively, which are not easily reusable to other new frameworks.",
                "cite_spans": [],
                "section": "Framework-Specific Access Pattern Annotations/Analyzers",
                "sec_num": "6.2"
            },
            {
                "text": "When extending Panthera to a new framework, we need to consider the following key issues:",
                "cite_spans": [],
                "section": "Apply Panthera to a New Framework",
                "sec_num": "6.3"
            },
            {
                "text": "First, we might need to introduce new framework-specific annotations so that the runtime system would have the knowledge about the key data structures. For example, as described in Section 3, to apply Panthera to QuickCached, the @CoreHashObject annotation is introduced to illustrate that this is the data structure for the global hash table in QuickCached. In particular, the annotations are designed together with the static analyzers and deliver application-level knowledge to the analyzers.",
                "cite_spans": [],
                "section": "Apply Panthera to a New Framework",
                "sec_num": "6.3"
            },
            {
                "text": "Second, we need to design a new framework-specific static analyzer to expose the memory access patterns. For example, a new static QuickCached analyzer is developed to identify the core data structure based on user's annotations and allocate an auxiliary array. The insight behind is that only a small fraction of the hashtable is frequently accessed, and the insight guides us to introduce the auxiliary array to hold these frequently accessed data. Meanwhile, the auxiliary array would be tagged by the analyzer so that it can be pre-promoted into the DRAM space of the old generation. Note that the analyzers can leverage user annotations and some framework-specific heuristics to obtain more precise memory access patterns.",
                "cite_spans": [],
                "section": "Apply Panthera to a New Framework",
                "sec_num": "6.3"
            },
            {
                "text": "With the framework-specific annotations and analyzers, the underlying runtime mechanism would leverage the second APIs and dynamically determine the data structure placement and migrations, without any framework-specific modifications. For example, for QuickCached, the runtime system would collect the frequently accessed data to the auxiliary array, thus these data would be allocated on DRAM.",
                "cite_spans": [],
                "section": "Apply Panthera to a New Framework",
                "sec_num": "6.3"
            },
            {
                "text": "We have added/modified 9,186 lines of C++ code in OpenJDK (build jdk8u76-b02) to implement the Panthera GC, and written 979 lines of Scala code to implement the static analysis for Spark and 762 lines of Java code for QuickCached.",
                "cite_spans": [],
                "section": "EVALUATION",
                "sec_num": "7"
            },
            {
                "text": "Most of the prior works on hybrid memories used simulators for experiments. However, none of them support Java applications well. We cannot execute managed-runtime-based distributed systems on these simulators. There also exist emulators such as Quartz [75] and PMEP [29] that support emulation of NVM for large programs using commodity multi-socket (NUMA) hardware, but neither Quartz nor PMEP could run OpenJDK. These emulators require developers to use their own libraries for NVM allocation, making it impossible for the Panthera GC to migrate objects without re-implementing the entire allocator and GC from scratch using these libraries.",
                "cite_spans": [
                    {
                        "start": 253,
                        "end": 257,
                        "text": "[75]",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 267,
                        "end": 271,
                        "text": "[29]",
                        "ref_id": "BIBREF28"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "As observed in [10] and [75] , NUMA's remote memory latency is close to NVM's latency, and hence, researchers have used a NUMA architecture as the baseline to measure emulation accuracy. Following this observation, we built our own emulator on NUMA machines to emulate hybrid memories for JVM-based Big Data systems.",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 19,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 24,
                        "end": 28,
                        "text": "[75]",
                        "ref_id": "BIBREF75"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "We followed Quartz [75] when implementing our emulator. Quartz has two major components: (1) it uses the thermal control register to limit the DRAM bandwidth; and (2) it creates a daemon thread for each application process and inserts delay instructions to emulate the NVM latency. For example, if an application's CPU stall time is S, Quartz scales the CPU stall time to S \u00d7 NVM_latency DRAM_latency to emulate the latency effect of NVM. For (1), we used the same thermal control register to limit the read/write bandwidth. Like Quartz, we currently do not support different bandwidths for reads and writes. For (2), we followed Quartz's observation to use the latency of NUMA's remote memory to model NVM's latency.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 23,
                        "text": "[75]",
                        "ref_id": "BIBREF75"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "An alternative approach to emulating NVM's latency is to instrument loads/stores during JIT compilation, injecting a software-created delay at each load/store. The limitation of this approach, however, is that it does not account for caching effects and memory-level parallelism.",
                "cite_spans": [],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "We used one CPU to run all the computation, the memory local to the CPU as DRAM, and the remote memory as NVM. In particular, DRAM and NVM are emulated, respectively, using two local and two remote memory channels. The performance specifications of the emulated NVM are Unified Holistic Memory Management Supporting Multiple Big Data 2:23 the same as those used in [75] , reported in Table 2 . To emulate NVM's slow write speed, we used the thermal control register to limit the bandwidth of remote memory-the emulated NVM is full duplex with 10 GB/s for read and write bandwidth each. The remote memory's latency in our setting is 2.5\u00d7 of that of the local memory.",
                "cite_spans": [
                    {
                        "start": 365,
                        "end": 369,
                        "text": "[75]",
                        "ref_id": "BIBREF75"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "Energy Estimation. We followed Lee et al. [49] to estimate energy for NVM. We used Micron's DDR4 device specifications [61] to model DRAM's power. NVM's energy has a static and dynamic component. The static component is negligible compared with DRAM [50] . The dynamic component consists of the energy consumed by reads and writes. PCM array reads consume about 2.1\u00d7 larger energy than DRAM due to its need for high temperature operation [49] .",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 46,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 119,
                        "end": 123,
                        "text": "[61]",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 250,
                        "end": 254,
                        "text": "[50]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 438,
                        "end": 442,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "NVM writes consume much more energy than DRAM writes. Upon a row-buffer miss, the energy consumed by each write has three components: (1) an array write that evicts data from the row buffer into the bank array, (2) an array read that fetches data from the bank array to the row buffer, and (3) a row buffer write that writes new data from the CPU last level cache to the row buffer. Assuming the row-buffer miss ratio is 0.5, we computed these three components separately by considering the row buffer's write energy (1.02 pJ/bit), size (i.e., 8K bits for DRAM [61] , 32-bitwide partial writeback to NVM [49] ) and miss rate (0.5), as well as the array's write-back energy (16.8pJ/bit \u00d7 7.6% for NVM) and read energy (2.47pJ/bit for NVM). The factor of 7.6% is due to Lee et al. 's optimization [49] that writes only 7.6% of the dirty words back to the NVM array.",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 137,
                        "text": "(1)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 561,
                        "end": 565,
                        "text": "[61]",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 604,
                        "end": 608,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 795,
                        "end": 799,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "CPU's uncore events, collected with VTune [7] , were employed to compute the numbers of reads and writes. In particular, the events we used were UNC_M_CAS_COUNT.RD and UNC_M_CAS_COUNT.WR. VTune can also distinguish reads and writes from/to local and remote memories.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 45,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "section": "NVM Emulation and Hardware Platform",
                "sec_num": "7.1"
            },
            {
                "text": "We set up a small cluster to run Spark with one master node and one slave node-these two servers have a special Intel chipset with a \"scalable memory buffer\" that can be tuned to produce the 2.5\u00d7 latency for remote memory accesses, which matches NVM's read/write latency. Since our focus is not on distributed computing, this cluster is sufficient for us to execute real workloads on Spark and understand their performance over hybrid memories. Table 3 reports the hardware configurations of the Spark master and Spark slave nodes. Each node has two 8-core CPU and the Parallel Scavenge collector on which Panthera was built creates 16 GC threads in each GC to perform parallel tracing and compaction.",
                "cite_spans": [],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "The negative impact of the GC latency increases with the number of compute nodes. As reported in [57] , a GC run on a single node can hold up the entire cluster-when a node requests a data partition from another server that is running GC, the requesting node cannot do anything until the GC is done on the second node. Since Panthera can significantly improve the GC performance on NVM, we expect Panthera to provide even greater benefit when Spark is executed on a large NVM cluster. System Configurations. Each CPU has a 128-GB DRAM. We reserved 8 GB of DRAM for the OS and the maximum amount of DRAM that can be used for Spark is 120 GB. We experimented with two different heap sizes for the Spark-running JVM (64 GB and 120 GB) and three different DRAM sizes (1/4, 1/3, and 100% of the heap size; the rest of the heap is NVM). For QuickCached, we experiemnted a 64-GB heap size with two different DRAM sizes (1/3 and 100% of the heap size). The configuration with 100% DRAM was used as a baseline to compute the overhead of Panthera under hybrid memories.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 101,
                        "text": "[57]",
                        "ref_id": "BIBREF56"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "Prior works on NVM often used smaller DRAM ratios in their configurations. For example, Write Rationing [11] used 1-GB DRAM and 32-GB NVM in their experiments. However, as we deal with Big Data systems, it would not be possible for us to use a very small DRAM ratio-in our experiments, a regular RDD consumes 10-30-GB memory, and hence, we had to make DRAM large enough to hold at least one RDD.",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 108,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "The nursery space is placed entirely in DRAM. We have experimented with several different sizes (1/4, 1/5, 1/6, and 1/7 of the heap size) for the nursery space. The performance differences between the 1/4, 1/5, and 1/6 configurations were marginal (even under the original JVM), while the configuration of 1/7 led to worse performance. We ended up using 1/6 in our experiments for both Spark and QuickCached to achieve good nursery performance and simultaneously leave more DRAM to the old generation.",
                "cite_spans": [],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "Programs and Datasets. For Spark, We selected a diverse set of seven programs. Table 4 lists these programs, the datasets used to run them and their memory footprints. These are representative programs for a wide variety of tasks including data mining, machine learning, graph and text analytics. PR, KM, LR, and TC run directly on Spark; CC and SSSP are graph programs running on GraphX [33] , which is a distributed graph engine built over Spark; BC is a program in MLib, a machine learning library built on top of Spark. We used real-world datasets to run all the seven programs. Note that although the sizes of these input datasets are not very large, there can be large amounts of intermediate data generated during the computation.",
                "cite_spans": [
                    {
                        "start": 388,
                        "end": 392,
                        "text": "[33]",
                        "ref_id": "BIBREF32"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "To evaluate the performance of QuickCached, we use the Yahoo! Cloud Serving Benchmark (YCSB) [22] . YCSB is a benchmark suite commonly used to evaluate the performance of cloud storage services. We run its A, B, C, D, and F workloads after loading the databases with 30 million records. Each record is 1 KB by default. For each workload, we perform 10 million operations.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 97,
                        "text": "[22]",
                        "ref_id": "BIBREF21"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "Baselines. Our initial goal was to compare Panthera with both Espresso [80] and Write Rationing [11] . However, neither of them is publicly available. Espresso proposes a programming model for developers to develop new applications. Applying it to Big Data systems would mean that we need to rewrite each allocation site, which is clearly not practical. In addition, Espresso does not migrate objects based on their access patterns.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 75,
                        "text": "[80]",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 96,
                        "end": 100,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:25 The Write Rationing GC has two implementations: Kingsguard-Nursery (KN) and Kingsguard-Writes (NW). KN places the young generation in DRAM and the old generation in NVM. KW also places the young generation in DRAM. Different from KN, KW monitors object writes and dynamically migrates write-intensive objects into DRAM. Although we could not directly compare Panthera with these two GCs, we have implemented similar algorithms in OpenJDK. Under KW, almost all persisted RDDs were quickly moved to NVM. The frequent NVM reads from these RDDs, together with write barriers used to monitor object writes, incurred an average of 41% performance overhead for our benchmarks. This is because Big Data applications exhibit different characteristics from regular, non-data-intensive Java applications.",
                "cite_spans": [],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "KN appears to be a good baseline at the first sight. However, implementing it na\u00efvely in the Parallel Scavenge collector can lead to non-trivial overhead-the reduced bandwidth in NVM can create a huge impact on the performance of a multi-threaded program; this is especially the case for Parallel Scavenge that attempts to fully utilize the CPU resources to perform parallel object scanning and compaction.",
                "cite_spans": [],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "To obtain a better baseline, we placed the young generation in DRAM and supported the old generation with a mix of DRAM and NVM. In particular, we divided the virtual address space of the old generation into a number of chunks, each with 1 GB, and used a probability to determine whether a chunk should be mapped to DRAM or NVM. The probability is derived from the DRAM ratio in the system. For example, in a system where the DRAM-to-memory ratio is 1/4 (1/4 DRAM), each chunk is mapped to DRAM with 1/4 probability and to NVM with 3/4 probability. Note that this is common practice [32, 77] to utilize the combined bandwidth of DRAM and NVM. We refer to this configuration as unmanaged, which outperforms both KN and KW for our benchmarks.",
                "cite_spans": [
                    {
                        "start": 583,
                        "end": 587,
                        "text": "[32,",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 588,
                        "end": 591,
                        "text": "77]",
                        "ref_id": "BIBREF77"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "There are also some OS-level-based data migration works, such as Thermostat [9] , Translation Ranger [86] , and HeteroOS [43] . We tried to port these frameworks to our emulated NVM platform, but none of these works fit for the benchmarks we used. For example, when running on Thermostat, the Spark applications always get stuck during the execution and the HeteroOS targets at the hybrid memory in virtualized environments instead of running on the bare-metal machines as Panthera does. Translation Ranger targets at speeding up the virtual-to-physical memory address translation by actively coalescing fragmented pages. It's an orthogonal optimization to Panthera, so we didn't include it in the evaluations. In order to evaluate the OS-level hybrid memory management policy, we utilize the kernel LRU (Least Recently Used) based paging system to do the data migration management. We created a ramdisk on the emulated NVM and mount it as the swap partition. We tuned the performance of the paging system to the best according to the state-of-art works [12, 59] . Under this settings, the NVM works as a secondary memory, similar with the Optane DC Memory Mode [37] . The kernel evicts the least recently used data to NVM and keeps the most recently used data in DRAM. As Figure 11 shows, the OS-level management policy is much worse than the unmanaged. Compared to unmanaged, the slowdown of OS-level management can reach to up 6.20\u00d7. This is because the GC always messes up the data layout placed by the OS, which cause much more useless data migration overhead, as we described in the Section 1. Hence, we utilize the unmanaged as baseline in the subsequent evaluations.",
                "cite_spans": [
                    {
                        "start": 76,
                        "end": 79,
                        "text": "[9]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 101,
                        "end": 105,
                        "text": "[86]",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 121,
                        "end": 125,
                        "text": "[43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 1054,
                        "end": 1058,
                        "text": "[12,",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1059,
                        "end": 1062,
                        "text": "59]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 1162,
                        "end": 1166,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    }
                ],
                "section": "Experiment Setup",
                "sec_num": "7.2"
            },
            {
                "text": "Figure 12 reports the overall performance and energy results of Spark when a 64-GB heap is used and thenDRAM-to-memory ratio is 1/3 (1/3 DRAM). The performance and energy results of each configuration are normalized w.r.t. those of the 64-GB DRAM-only version. The energy results in our experiments include the energy consumption of Panthera runtime, but do not include the energy consumption of the static analyzer and profiling tools. Compared to the DRAM-only version, the unmanaged version reduces energy by 26.7% with a 21.4% execution time overhead. In contrast, Panthera reduces energy by 32.3% at a 4.3% execution time overhead.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "When the heap size is 120 GB (not shown in Figure 12 , but summarized later in Figure 14 and Figure 15 ), the unmanaged version reduces energy by 39.7% at a 19.3% execution time overhead. In contrast, Panthera reduces energy by 47.0% with less than 1% execution time overhead. Clearly, Unified Holistic Memory Management Supporting Multiple Big Data 2:27 considering the RDD semantics in data placement provides significant benefits in both energy and performance.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "GC Performance. To understand the GC performance, we broke down the running time of each program into the mutator and GC time; these results (under the 64-GB heap) are shown in Figure 13 . Compared to the baseline, the unmanaged version introduces performance overhead of 60.4% and 6.9% in the GC and computation, respectively; while for Panthera these two overheads are, respectively, 4.7% and 4.5%. Under the 120-GB heap, the GC performance overhead of the unmanaged version and Panthera are, respectively, 58.0% and 3.1%. Note that, due to large amounts of intermediate data generated, the GC is frequently triggered for these programs.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "Since the GC is a memory-intensive workload, inappropriate data placement can lead to significantly higher memory access time and thus a large penalty. The penalty comes from two major sources. First, NVM's limited bandwidth (which is about 1/3 of that of DRAM) has a large negative impact on the performance of Parallel Scavenge, which launches 16 threads to perform parallel tracing and object copying in each (nursery and full-heap) GC. Given this high degree of parallelism, the performance of the nursery GC is degraded significantly when scanning objects in NVM. Second, object tracing is a read-intensive task, which suffers badly from NVM's higher read latency.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "Panthera improves the GC performance by pretenuring frequently accessed RDD objects in DRAM and performing optimizations including eager promotion (Section 4.2.2) and card padding (Section 4.2.3). Eager promotion reduces the cost of (old-to-young) tracing in each minor GC, while card padding eliminates unnecessary array scans in NVM, which are sensitive to both latency and bandwidth. A further breakdown shows that eager promotion, alone, contributes an average of 9% of the total GC performance improvement. The contribution of card padding is much more significant-without this optimization, the GC time increases by 60% due to the impact of NVM's substantially limited bandwidth and increased latency on the performance of parallel card scanning. In fact, this impact is so large that the other optimizations would not work well when card padding is disabled.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "Varying Heaps and Ratios. To understand the impact of the heap sizes and DRAM ratios (DRAM to total memory), we have conducted experiments with two heap sizes (64 GB, 120GB) and two DRAM ratios (1/3, 1/4) on four programs PR, LR, CC, and BC. Figure 14 reports the time results of these configurations. Panthera's time overheads are, on average, 9.5%, 3.4%, 2.1%, and 0%, respectively, under the four configurations (64 GB, 1/4), (64 GB, 1/3), (120 GB, 1/4), and (120 GB, 1/3). The overheads for the unmanaged version are 25.9%, 20.9%, 23.9%, and 19.3%, respectively, under these same four configurations. We make two interesting observations. First, Panthera is more sensitive to the DRAM ratio than the heap size. The time overhead can be reduced by almost 10% when the DRAM ratio increases from 1/4 to 1/3. The reason is that more frequently accessed RDDs are moved to DRAM, reducing the memory latency and bandwidth bound of NVM. Another observation is that the unmanaged version is much less sensitive to DRAM ratio-the time overhead is reduced by only 5% when the DRAM ratio increases to 1/3. This is because arbitrary data placement leaves much of the frequently accessed data in NVM, making CPUs stall heavily when accessing NVM.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "Figure 15 depicts the energy results for the two heaps and two DRAM/NVM ratios. For the 64-GB heap, the unmanaged version reduces energy by an average of 32.2% and 26.5%, respectively, under the 1/4 and 1/3 DRAM ratio, while Panthera reduces energy by 36.0% and 32.7% under these same ratios. The energy reductions for the 120-GB heap are much more significant-the unmanaged version reduces energy by 45.7% and 39.7%, respectively, under the 1/4 and 1/3 DRAM ratios, while the energy reduction under Panthera increases to 51.7% and 47.0% for these two ratios.",
                "cite_spans": [],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "We also evaluate the prices of NVM and DRAM to show the hardware cost savings that benefit from using the hybrid memory. The price of the cheapest NVM is $7.85 per GB and the cheapest DRAM is $16.61 per GB [35] . Compared with DRAM-only, using the hybrid memories with DRAM ratio 1/3 can reduce 35.2% hardware costs, and even reduce 39.6% when with DRAM ratio 1/4. ",
                "cite_spans": [
                    {
                        "start": 206,
                        "end": 210,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    }
                ],
                "section": "Performance and Energy of Panthera without PGO",
                "sec_num": "7.3"
            },
            {
                "text": "NVM has high latency and low bandwidth. In general, the performance penalty caused by high latency increases with the number of memory accesses. For the same number of memory accesses, NVM incurs higher performance penalty for applications that have instantaneous bandwidth requirements which are beyond NVM's bandwidth. Figure 17 ",
                "cite_spans": [],
                "section": "Memory Access Analysis",
                "sec_num": "7.4"
            },
            {
                "text": "As discussed in Section 4.2, Panthera performs lightweight method-level monitoring on RDD objects to detect misplaced RDDs for dynamic migration. This subsection provides a closer examination of dynamic migration's overhead.",
                "cite_spans": [],
                "section": "Overhead of Monitoring and Migration",
                "sec_num": "7.5"
            },
            {
                "text": "As we monitor only method calls invoked on RDD objects, we find dynamic monitoring overhead is negligible, i.e., it is less than 1% across our benchmarks. For example, for PageRank, only about 300 calls were observed on all RDD objects in a 20-minute execution. The second column of Table 5 reports the number of calls monitored for each application. For GraphX applications, which has thousands of RDD calls, the monitoring overheads are still less than 1%.",
                "cite_spans": [],
                "section": "Overhead of Monitoring and Migration",
                "sec_num": "7.5"
            },
            {
                "text": "Dynamic migration (performed by the major GC) rarely occurs in our experiments, as can be seen from the third column of Table 5 . There are two main reasons. First, the frequency of a major collection is very low because a majority of objects die young and most of the collection work We observed that only two RDDs (during the executions of CC and SSSP) were migrated dynamically. Note that both CC and SSSP are GraphX applications. Each iteration of the processing creates new RDDs representing the updated graph and persists them. At the end of each iteration, the RDDs representing the old graph are explicitly unpersisted. Our static analysis, due to lack of support for the unpersist call, marks both old and new graph RDDs as hot data and generates a DRAM tag for all them. These RDD objects are then allocated in DRAM and their data objects are promoted eagerly to the DRAM space of the old generation. The RDD objects representing the old graphs, if they can survive a major GC, are migrated to the NVM space of the old generation due to their low access frequency.",
                "cite_spans": [],
                "section": "Overhead of Monitoring and Migration",
                "sec_num": "7.5"
            },
            {
                "text": "To have better understanding of the individual contributions of pretenuring and dynamic migration, we have disabled the monitoring and migration and rerun the entire experiments. The performance difference was negligible (i.e., less than 1%). Hence, we conclude that most of Panthera's benefit stems from pretenuring, which improves the performance of both the mutator and the GC. However, dynamic monitoring and migration increases the generality of Panthera's optimizations, making Panthera applicable to applications with diverse access characteristics.",
                "cite_spans": [],
                "section": "Overhead of Monitoring and Migration",
                "sec_num": "7.5"
            },
            {
                "text": "To examine the effectiveness of our profiling-guided optimization, we have evaluated Panthera with PGO enabled using the benchmarks listed in Table 4 . We experimented with two heap sizes , 64 GB and 120 GB, and the nursery space is 1/6 of the heap size while 1/3 of the heap is DRAM. We compared Panthera w/ PGO against w/o PGO, and also against Unmanaged. Note our profiling is applied offline, thus it would not introduce extra overhead.",
                "cite_spans": [],
                "section": "Performance and Energy of PGO",
                "sec_num": "7.6"
            },
            {
                "text": "Figure 18 shows the overall performance and energy results when a 64-GB heap is used. The results are normalized w.r.t. those of the DRAM-only version. Compared with Panthera w/o PGO, the PGO can reduce the energy by 5.8% with 3.9% less execution time on average. Compared with 64-GB DRAM-only version, Panthera w/ PGO can achieve 36.4% energy reduction at 0.2% execution time overhead. However, we can see that some applications, such as LR, Graphx-CC, and Graphx-SSSP, can only get marginal benefits from PGO. There are two basic reasons. First, the access patterns in the RDDs of these applications are uniform and there is no need to divide the RDDs into finer chunks. In this situation, Panthera w/o PGO can recognize and migrate the data to correct place, as shown in Figure 7 . Second, some memory accesses patterns, e.g., streaming, on the RDDs are easy to be recognized. The hardware and OS prefetching mechanisms work well for the data. In this case, even the Panthera w/ PGO can do a better data placement than Panthera w/o Unified Holistic Memory Management Supporting Multiple Big Data 2:31 PGO by recognizing and migrating chunks with higher access frequency to DRAM, it can't get too many benefits.",
                "cite_spans": [],
                "section": "Performance and Energy of PGO",
                "sec_num": "7.6"
            },
            {
                "text": "Our PGO can be decoupled from Panthera and be integrated into the unmanaged version, and Figure 19 shows the overall performance and energy results when PGO is implemented into unmanaged version. The results are normalized to the 64-GB DRAM-only version. Compared with Unmanaged w/o PGO, PGO reduces energy cost by 9.6% with 8.7% less execution time on average. Furthermore, compared with the 64-GB DRAM-only version, Unmanaged w/ PGO can achieve 33.8% energy reduction at 11.8% execution time overhead while Unmanaged w/o PGO reduces energy by 26.7% with a 21.4% execution time overhead.",
                "cite_spans": [],
                "section": "Performance and Energy of PGO",
                "sec_num": "7.6"
            },
            {
                "text": "Figure 20 shows the overall performance and energy results when a 120-GB heap is used. The results are normalized to DRAM-only version. With PGO, Unmanaged reduces energy by 44.8% with a 10.2% execution time overhead while Panthera reduces 50.1% energy with less than 1% execution time overhead on average. PGO Results for QuickCached. Figure 21 reports the performance and energy results when employing PGO to QuickCached. We used the same experiment configuration as described in Section 7.2. The results are normalized w.r.t. those of the DRAM-only version. Compared with Panthera w/o PGO, the PGO reduced 2.9% execution time on average, while the energy consumption was almost the same. The benefits of PGO for QuickCached is less than for Spark, because: (1) the PGO was introduced to do finer-grained chunk placement and improve the performance further based on the static coarse-grained RDD-level analysis. For QuickCached, Panthera inferred the memory tag for each object at runtime, which is fine-grained already; and (2) the NVM access ratio for QuickCached is less than for Spark, e.g., accounting less than 10% of all the memory access, because of the biased object access characteristics as described in 2.5.",
                "cite_spans": [],
                "section": "Performance and Energy of PGO",
                "sec_num": "7.6"
            },
            {
                "text": "How Far from the Ideal. For hybrid memories with managed runtime, the ideal solution is to place each object correctly according to its hotness, and adjust the placement dynamically considering the cost to do object migration and the benefit of migration. As Big Data systems usually have billions of objects in their heap during a normal execution, it is hard to profile the execution to record where each object should be placed, and when to do migration. Panthera introduced a simplified observation that we can develop a simple static analysis to infer the access pattern of each coarse-grained data collection, where all objects share the same pattern. In addition, this simple assumption is accurate enough as stated in 7.5.",
                "cite_spans": [],
                "section": "Discussion",
                "sec_num": "7.7"
            },
            {
                "text": "The Effects of the Emulated NVM Specifications. Due to the limitations of Quartz, although most of the emulated latency/bandwidth results match the specifications used in the previous research [75] , there is still some differences with the real NVM hardware, e.g., Optane DC [37] . There are two major differences between the emulated NVM and real NVM hardware. First, the emulated NVM doesn't show the asymmetry of NVM in read/write latency and bandwidth. Second, the emulated specifications is not exactly the same with the real hardware. For example, the read/write latencies of our emulated NVM are 300 ns/300 ns against the 305 ns/94 ns of the Optane DC [37] . However, we emphasize that different NVM technologies have different specifications and the emulated NVM already shows the performance characteristics of NVM and the performance difference between the DRAM and NVM. Our comprehensive evaluations on the emulated NVM can show the negligible overhead and high accuracy of our proposed static/dynamic analysis and the effectiveness of our data migration policy. For example, we tuned the emulated NVM read/ write latency from 300 ns/300 ns to 120 ns/120 ns, the baseline, Unmanaged, still has a significant performance degradation by suffering from the limited read/write bandwidth. Under these settings, Panthera still outperforms the baseline 11.6% on average. With a fixed 300-ns read/write latency, we also tuned the read/write bandwidth from 5 GB/s to 12 GB/s (12 GB/s is the bandwidth limit of our server QPI), Panthera always outperforms the baseline from 32.3% to 11.9% on average.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 197,
                        "text": "[75]",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 276,
                        "end": 280,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 660,
                        "end": 664,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    }
                ],
                "section": "Discussion",
                "sec_num": "7.7"
            },
            {
                "text": "Hybrid Memories for Managed Runtime. To our knowledge, Panthera is the first practical work to optimize data layout in hybrid memories for managed-runtime-based distributed Big Data platforms. Existing efforts [11, 17, 32, 39, 40, 67, 72, 76, 80] that attempt to support persistent Java focus on regular applications or need to rebuild the platforms.",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 214,
                        "text": "[11,",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 215,
                        "end": 218,
                        "text": "17,",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 219,
                        "end": 222,
                        "text": "32,",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 223,
                        "end": 226,
                        "text": "39,",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 227,
                        "end": 230,
                        "text": "40,",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 231,
                        "end": 234,
                        "text": "67,",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 235,
                        "end": 238,
                        "text": "72,",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 239,
                        "end": 242,
                        "text": "76,",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 243,
                        "end": 246,
                        "text": "80]",
                        "ref_id": "BIBREF80"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Inoue and Nakatani [36] identify code patterns in Java applications that can cause cache misses in L1 and L2. Gao et al. [31] propose a framework including support from hardware, the OS, and the runtime to extend NVM's lifetime. Two recent works close to Panthera are Espresso [80] and Write Rationing [11] . However, they were not designed for Big Data systems. Espresso is a JVMbased runtime system that enables persistent heaps. Developers can allocate objects in a persistent heap using a new instruction pnew while the runtime system provides crash consistency for the heap. Applying Espresso requires rewriting the Big Data platforms (e.g., Spark) using pnew, which is not practical.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 23,
                        "text": "[36]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 121,
                        "end": 125,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 277,
                        "end": 281,
                        "text": "[80]",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 302,
                        "end": 306,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Write Rationing [11] is a GC technique that places highly mutated objects in DRAM and mostly read objects in NVM to increase NVM lifetime. Like Espresso, this GC focuses on individual objects and does not consider application semantics. Panthera's nursery space is also placed in DRAM, similar to the Kingsguard-Nursery in Write Rationing. However, instead of focusing on individual objects, Panthera utilizes Spark semantics to obtain access information at the array granularity, leading to effective pretenuring and efficient runtime object tracking.",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 20,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Memory Structure. There are two kinds of hybrid-memory structures: flat structure, where DRAM and NVM share a single memory space, and vertical structure, where DRAM is used as a buffer for NVM to store hot data. The vertical structure is normally managed by hardware and transparent to the OS and applications [44, 49, 54, 58, 69, 88, 90, 93] . Qureshi et al. [69] shows that a vertical structure with only 3% DRAM can reach similar performance to its DRAM-only version. However, the overhead of page monitoring and migration increases linearly with the working set [77] . The space overhead e.g., the tag store space of DRAM buffer, can also be high with a large volume of NVM [60] .",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 315,
                        "text": "[44,",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 316,
                        "end": 319,
                        "text": "49,",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 320,
                        "end": 323,
                        "text": "54,",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 324,
                        "end": 327,
                        "text": "58,",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 328,
                        "end": 331,
                        "text": "69,",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 332,
                        "end": 335,
                        "text": "88,",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 336,
                        "end": 339,
                        "text": "90,",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 340,
                        "end": 343,
                        "text": "93]",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 361,
                        "end": 365,
                        "text": "[69]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 567,
                        "end": 571,
                        "text": "[77]",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 679,
                        "end": 683,
                        "text": "[60]",
                        "ref_id": "BIBREF60"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Page-Based Migration. A great number of existing works use memory controllers to monitor page read/write frequency [20, 25, 30, 34, 53, 68, 70, 77, 88, 92] and migrate the top-ranked pages to DRAM. Another type of hybrid memory, composed of 3D-stacked DRAM and commodity DRAM, also adapts similar page monitoring policies [28, 38] . However, none of these techniques were designed for Big Data systems. Hassan et al. [34] show that, for some applications, migrating data at the object level can reduce power consumption.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 119,
                        "text": "[20,",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 120,
                        "end": 123,
                        "text": "25,",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 124,
                        "end": 127,
                        "text": "30,",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 128,
                        "end": 131,
                        "text": "34,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 132,
                        "end": 135,
                        "text": "53,",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 136,
                        "end": 139,
                        "text": "68,",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 140,
                        "end": 143,
                        "text": "70,",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 144,
                        "end": 147,
                        "text": "77,",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 148,
                        "end": 151,
                        "text": "88,",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 152,
                        "end": 155,
                        "text": "92]",
                        "ref_id": "BIBREF92"
                    },
                    {
                        "start": 322,
                        "end": 326,
                        "text": "[28,",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 327,
                        "end": 330,
                        "text": "38]",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 417,
                        "end": 421,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "For Big Data applications that have very large memory consumption, continuous monitoring at the page granularity can incur an unreasonable overhead. Page migration also incurs overhead in time and bandwidth. Bock et al. [18] report that page migration can increase execution time by 25% on average. Panthera uses static analysis to track memory usage at the RDD granularity, incorporating program semantics to reduce the dynamic monitoring overheads.",
                "cite_spans": [
                    {
                        "start": 220,
                        "end": 224,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Static Data Placement. There exists a body of work that attempts to place data directly in appropriate spaces based either on their access frequencies [20, 53, 68, 74, 88] or on the result of a program analysis [30, 34, 77] . Access frequency is normally calculated using a static data liveness analysis or offline profiling. Chatterjee et al. [20] place a single cache-line across multiple memory channels. Critical words (normally the first word) in a cache-line are placed in a low-latency channel. Wei et al. [77] show that the group of objects allocated by the same site in the source code exhibit similar lifetime behavior, which can be leveraged for static data placement. Dulloor et al. [30] classify memory accesses into three patterns and model the access time for a given mapping from the data structure with a specific access pattern to different memory types to get the optimal mapping configuration.",
                "cite_spans": [
                    {
                        "start": 151,
                        "end": 155,
                        "text": "[20,",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 156,
                        "end": 159,
                        "text": "53,",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 160,
                        "end": 163,
                        "text": "68,",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 164,
                        "end": 167,
                        "text": "74,",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 168,
                        "end": 171,
                        "text": "88]",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 211,
                        "end": 215,
                        "text": "[30,",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 216,
                        "end": 219,
                        "text": "34,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 220,
                        "end": 223,
                        "text": "77]",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 344,
                        "end": 348,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 513,
                        "end": 517,
                        "text": "[77]",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 695,
                        "end": 699,
                        "text": "[30]",
                        "ref_id": "BIBREF29"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "Li et al. [53] develop a binary instrumentation tool to statistically report memory access patterns in stack, heap, and global data. Phadke and Narayanasamy [68] profile an application's MLP and LLC misses to determine from which type of memory the application could benefit the most. Kim et al. [45] develop a key-value store for high-performance computers with large distributed NVM, which provides developers with a high-level interface to use the distributed NVM. However, none of these techniques were designed for managed Big Data systems.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 14,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 157,
                        "end": 161,
                        "text": "[68]",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 296,
                        "end": 300,
                        "text": "[45]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "section": "RELATED WORK",
                "sec_num": "8"
            },
            {
                "text": "We present Panthera, the first memory management technique for managed Big Data processing over hybrid memories. Panthera combines static analysis and GC techniques and profile-guided optimization to perform semantics-aware data placement in hybrid memory systems. Our evaluation shows that Panthera reduces energy significantly without incurring much extra time overhead.",
                "cite_spans": [],
                "section": "CONCLUSION",
                "sec_num": "9"
            },
            {
                "text": "ACM Transactions on Computer Systems, Vol. 39, No. 1-4, Article 2. Publication date: July 2022.",
                "cite_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "LIBSVM Data: Classification",
                "authors": [],
                "dblp_id": null,
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2012. LIBSVM Data: Classification. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Notre dame network dataset",
                "authors": [],
                "dblp_id": null,
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2017. Notre dame network dataset. http://konect.uni-koblenz.de/networks/web-NotreDame.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "QuickCached",
                "authors": [],
                "dblp_id": null,
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2017. QuickCached. https://github.com/QuickServerLab/QuickCached.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Wikipedia links, network dataset",
                "authors": [],
                "dblp_id": null,
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2017. Wikipedia links, network dataset. http://konect.uni-koblenz.de/networks.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A Breakthrough in Non-Volatile Memory Technology",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Xpoint",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "XPoint T M : A Breakthrough in Non-Volatile Memory Technology. https://www.intel.com/content/www/us/ en/architecture-and-technology/intel-micron-3d-xpoint-webcast.html.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Apache Spark T M",
                "authors": [],
                "dblp_id": null,
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2019. Apache Spark T M . https://spark.apache.org.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Intel VTune T M Amplifier",
                "authors": [],
                "dblp_id": null,
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2019. Intel VTune T M Amplifier. https://software.intel.com/en-us/vtune.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "OpenJDK",
                "authors": [],
                "dblp_id": null,
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2019. OpenJDK. https://openjdk.java.net.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Thermostat: Application-transparent page management for two-tiered main memory",
                "authors": [
                    {
                        "first": "Neha",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "F"
                        ],
                        "last": "Wenisch",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/AgarwalW17",
                "year": 2017,
                "venue": "Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems",
                "volume": "",
                "issue": "",
                "pages": "631--644",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Neha Agarwal and Thomas F. Wenisch. 2017. Thermostat: Application-transparent page management for two-tiered main memory. In Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems. 631-644.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Emulating hybrid memory on NUMA hardware",
                "authors": [
                    {
                        "first": "Shoaib",
                        "middle": [],
                        "last": "Akram",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [
                            "B"
                        ],
                        "last": "Sartor",
                        "suffix": ""
                    },
                    {
                        "first": "Kathryn",
                        "middle": [
                            "S"
                        ],
                        "last": "Mckinley",
                        "suffix": ""
                    },
                    {
                        "first": "Lieven",
                        "middle": [],
                        "last": "Eeckhout",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2018,
                "venue": "CoRR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shoaib Akram, Jennifer B. Sartor, Kathryn S. McKinley, and Lieven Eeckhout. 2018. Emulating hybrid memory on NUMA hardware. CoRR (2018).",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Write-rationing garbage collection for hybrid memories",
                "authors": [
                    {
                        "first": "Shoaib",
                        "middle": [],
                        "last": "Akram",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [
                            "B"
                        ],
                        "last": "Sartor",
                        "suffix": ""
                    },
                    {
                        "first": "Kathryn",
                        "middle": [
                            "S"
                        ],
                        "last": "Mckinley",
                        "suffix": ""
                    },
                    {
                        "first": "Lieven",
                        "middle": [],
                        "last": "Eeckhout",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/pldi/AkramSME18",
                "year": 2018,
                "venue": "Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'18)",
                "volume": "",
                "issue": "",
                "pages": "62--77",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shoaib Akram, Jennifer B. Sartor, Kathryn S. McKinley, and Lieven Eeckhout. 2018. Write-rationing garbage collection for hybrid memories. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'18). ACM, New York, 62-77.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Can far memory improve job throughput",
                "authors": [
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Amaro",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Branner-Augmon",
                        "suffix": ""
                    },
                    {
                        "first": "Zhihong",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Amy",
                        "middle": [],
                        "last": "Ousterhout",
                        "suffix": ""
                    },
                    {
                        "first": "Marcos",
                        "middle": [
                            "K"
                        ],
                        "last": "Aguilera",
                        "suffix": ""
                    },
                    {
                        "first": "Aurojit",
                        "middle": [],
                        "last": "Panda",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvia",
                        "middle": [],
                        "last": "Ratnasamy",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Shenker",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/eurosys/AmaroBLOAPRS20",
                "year": 2020,
                "venue": "Proceedings of the 15th European Conference on Computer Systems (EuroSys'20)",
                "volume": "14",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3342195.3387522"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Emmanuel Amaro, Christopher Branner-Augmon, Zhihong Luo, Amy Ousterhout, Marcos K. Aguilera, Aurojit Panda, Sylvia Ratnasamy, and Scott Shenker. 2020. Can far memory improve job throughput? In Proceedings of the 15th European Conference on Computer Systems (EuroSys'20). ACM, New York, Article 14, 16 pages. https://doi.org/10.1145/ 3342195.3387522",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Bztree: A high-performance latch-free range index for non-volatile memory",
                "authors": [
                    {
                        "first": "Joy",
                        "middle": [],
                        "last": "Arulraj",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Levandoski",
                        "suffix": ""
                    },
                    {
                        "first": "Umar",
                        "middle": [],
                        "last": "Farooq Minhas",
                        "suffix": ""
                    },
                    {
                        "first": "Per-Ake",
                        "middle": [],
                        "last": "Larson",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2018,
                "venue": "Proc. VLDB Endow",
                "volume": "11",
                "issue": "5",
                "pages": "553--565",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joy Arulraj, Justin Levandoski, Umar Farooq Minhas, and Per-Ake Larson. 2018. Bztree: A high-performance latch-free range index for non-volatile memory. Proc. VLDB Endow. 11, 5 (Jan. 2018), 553-565.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Let's talk about storage & recovery methods for nonvolatile memory database systems",
                "authors": [
                    {
                        "first": "Joy",
                        "middle": [],
                        "last": "Arulraj",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Pavlo",
                        "suffix": ""
                    },
                    {
                        "first": "Subramanya",
                        "middle": [
                            "R"
                        ],
                        "last": "Dulloor",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sigmod/ArulrajPD15",
                "year": 2015,
                "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data",
                "volume": "",
                "issue": "",
                "pages": "707--722",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joy Arulraj, Andrew Pavlo, and Subramanya R. Dulloor. 2015. Let's talk about storage & recovery methods for non- volatile memory database systems. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM, New York, 707-722.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Write-behind logging",
                "authors": [
                    {
                        "first": "Joy",
                        "middle": [],
                        "last": "Arulraj",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Perron",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Pavlo",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2016,
                "venue": "Proc. VLDB Endow",
                "volume": "10",
                "issue": "4",
                "pages": "337--348",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joy Arulraj, Matthew Perron, and Andrew Pavlo. 2016. Write-behind logging. Proc. VLDB Endow. 10, 4 (Nov. 2016), 337-348.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Workload analysis of a largescale key-value store",
                "authors": [
                    {
                        "first": "Berk",
                        "middle": [],
                        "last": "Atikoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuehai",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Eitan",
                        "middle": [],
                        "last": "Frachtenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Paleczny",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sigmetrics/AtikogluXFJP12",
                "year": 2012,
                "venue": "Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems",
                "volume": "",
                "issue": "",
                "pages": "53--64",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike Paleczny. 2012. Workload analysis of a large- scale key-value store. In Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems. ACM, New York, 53-64.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "An orthogonally persistent Java",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "P"
                        ],
                        "last": "Atkinson",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Dayn\u00e8s",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "J"
                        ],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Printezis",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Spence",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 1996,
                "venue": "SIGMOD Rec",
                "volume": "25",
                "issue": "4",
                "pages": "68--75",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. P. Atkinson, L. Dayn\u00e8s, M. J. Jordan, T. Printezis, and S. Spence. 1996. An orthogonally persistent Java. SIGMOD Rec. 25, 4 (Dec. 1996), 68-75.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Concurrent page migration for mobile systems with OS-managed hybrid memory",
                "authors": [
                    {
                        "first": "Santiago",
                        "middle": [],
                        "last": "Bock",
                        "suffix": ""
                    },
                    {
                        "first": "Bruce",
                        "middle": [
                            "R"
                        ],
                        "last": "Childers",
                        "suffix": ""
                    },
                    {
                        "first": "Rami",
                        "middle": [
                            "G"
                        ],
                        "last": "Melhem",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Moss\u00e9",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/cf/BockCMM14",
                "year": 2014,
                "venue": "Proceedings of the 11th ACM Conference on Computing Frontiers (CF'14)",
                "volume": "31",
                "issue": "",
                "pages": "1--31",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Santiago Bock, Bruce R. Childers, Rami G. Melhem, and Daniel Moss\u00e9. 2014. Concurrent page migration for mobile systems with OS-managed hybrid memory. In Proceedings of the 11th ACM Conference on Computing Frontiers (CF'14). ACM, New York, 31:1-31:10.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "The anatomy of a large-scale hypertextual web search engine",
                "authors": [
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Brin",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Page",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 1998,
                "venue": "Comput. Netw. ISDN Syst",
                "volume": "30",
                "issue": "1-7",
                "pages": "107--117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Comput. Netw. ISDN Syst. 30, 1-7 (April 1998), 107-117.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Leveraging heterogeneity in DRAM main memories to accelerate critical word access",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Shevgoor",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Balasubramonian",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Davis",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Illikkal",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/micro/ChatterjeeSBDFII12",
                "year": 2012,
                "venue": "Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-45)",
                "volume": "",
                "issue": "",
                "pages": "13--24",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Chatterjee, M. Shevgoor, R. Balasubramonian, A. Davis, Z. Fang, R. Illikkal, and R. Iyer. 2012. Leveraging hetero- geneity in DRAM main memories to accelerate critical word access. In Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-45). ACM, New York, 13-24.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "FlatStore: An efficient log-structured key-value storage engine for persistent memory",
                "authors": [
                    {
                        "first": "Youmin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Youyou",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwu",
                        "middle": [],
                        "last": "Shu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/ChenLYWWS20",
                "year": 2020,
                "venue": "Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems",
                "volume": "",
                "issue": "",
                "pages": "1077--1091",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Youmin Chen, Youyou Lu, Fan Yang, Qing Wang, Yang Wang, and Jiwu Shu. 2020. FlatStore: An efficient log-structured key-value storage engine for persistent memory. In Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems. 1077-1091.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Benchmarking cloud serving systems with YCSB",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [
                            "F"
                        ],
                        "last": "Cooper",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Silberstein",
                        "suffix": ""
                    },
                    {
                        "first": "Erwin",
                        "middle": [],
                        "last": "Tam",
                        "suffix": ""
                    },
                    {
                        "first": "Raghu",
                        "middle": [],
                        "last": "Ramakrishnan",
                        "suffix": ""
                    },
                    {
                        "first": "Russell",
                        "middle": [],
                        "last": "Sears",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/cloud/CooperSTRS10",
                "year": 2010,
                "venue": "Proceedings of the 1st ACM Symposium on Cloud Computing",
                "volume": "",
                "issue": "",
                "pages": "143--154",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of the 1st ACM Symposium on Cloud Computing. ACM, New York, 143-154.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "An introduction to pmemcheck",
                "authors": [],
                "dblp_id": null,
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Intel Corporation. 2015. An introduction to pmemcheck. https://pmem.io/2015/07/17/pmemcheck-basic.html.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Redis",
                "authors": [],
                "dblp_id": null,
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Intel Corporation. 2018. Redis. https://github.com/pmem/redis/tree/3.2-nvml.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "PDRAM: A hybrid PRAM and DRAM main memory system",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Dhiman",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Ayoub",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Rosing",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/dac/DhimanAR09",
                "year": 2009,
                "venue": "Proceedings of the 46th Annual Design Automation Conference (DAC'09)",
                "volume": "",
                "issue": "",
                "pages": "664--669",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Dhiman, R. Ayoub, and T. Rosing. 2009. PDRAM: A hybrid PRAM and DRAM main memory system. In Proceedings of the 46th Annual Design Automation Conference (DAC'09). ACM, New York, 664-669.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Fast, flexible, and comprehensive bug detection for persistent memory programs",
                "authors": [
                    {
                        "first": "Bang",
                        "middle": [],
                        "last": "Di",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/DiLCL21",
                "year": 2021,
                "venue": "Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
                "volume": "",
                "issue": "",
                "pages": "503--516",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bang Di, Jiawen Liu, Hao Chen, and Dong Li. 2021. Fast, flexible, and comprehensive bug detection for persistent memory programs. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. ACM, New York, 503-516.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Performance and protection in the ZoFS user-space NVM file system",
                "authors": [
                    {
                        "first": "Mingkai",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Bu",
                        "suffix": ""
                    },
                    {
                        "first": "Jifei",
                        "middle": [],
                        "last": "Yi",
                        "suffix": ""
                    },
                    {
                        "first": "Benchao",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Haibo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/DongBYDC19",
                "year": 2019,
                "venue": "Proceedings of the 27th ACM Symposium on Operating Systems Principles",
                "volume": "",
                "issue": "",
                "pages": "478--493",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mingkai Dong, Heng Bu, Jifei Yi, Benchao Dong, and Haibo Chen. 2019. Performance and protection in the ZoFS user-space NVM file system. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. ACM, New York, 478-493.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Simple but effective heterogeneous main memory with on-chip memory controller support",
                "authors": [
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Naveen",
                        "middle": [],
                        "last": "Muralimanohar",
                        "suffix": ""
                    },
                    {
                        "first": "Norman",
                        "middle": [
                            "P"
                        ],
                        "last": "Jouppi",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sc/DongXMJ10",
                "year": 2010,
                "venue": "Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC'10)",
                "volume": "",
                "issue": "",
                "pages": "1--11",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiangyu Dong, Yuan Xie, Naveen Muralimanohar, and Norman P. Jouppi. 2010. Simple but effective heterogeneous main memory with on-chip memory controller support. In Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC'10). ACM, New York, 1-11.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "System software for persistent memory",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Subramanya",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Dulloor",
                        "suffix": ""
                    },
                    {
                        "first": "Anil",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Keshavamurthy",
                        "suffix": ""
                    },
                    {
                        "first": "Dheeraj",
                        "middle": [],
                        "last": "Lantz",
                        "suffix": ""
                    },
                    {
                        "first": "Rajesh",
                        "middle": [],
                        "last": "Reddy",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Sankaran",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jackson",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/eurosys/RaoKKLRSJ14",
                "year": 2014,
                "venue": "Proceedings of the 9th European Conference on Computer Systems (EuroSys'14)",
                "volume": "15",
                "issue": "",
                "pages": "1--15",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Subramanya R. Dulloor, Sanjay Kumar, Anil Keshavamurthy, Philip Lantz, Dheeraj Reddy, Rajesh Sankaran, and Jeff Jackson. 2014. System software for persistent memory. In Proceedings of the 9th European Conference on Computer Systems (EuroSys'14). 15:1-15:15.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Data tiering in heterogeneous memory systems",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Subramanya",
                        "suffix": ""
                    },
                    {
                        "first": "Amitabha",
                        "middle": [],
                        "last": "Dulloor",
                        "suffix": ""
                    },
                    {
                        "first": "Zheguang",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Narayanan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Nadathur",
                        "middle": [],
                        "last": "Sundaram",
                        "suffix": ""
                    },
                    {
                        "first": "Rajesh",
                        "middle": [],
                        "last": "Satish",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Sankaran",
                        "suffix": ""
                    },
                    {
                        "first": "Karsten",
                        "middle": [],
                        "last": "Jackson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schwan",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/eurosys/DulloorRZSSSJS16",
                "year": 2016,
                "venue": "Proceedings of the 11th European Conference on Computer Systems (EuroSys'16)",
                "volume": "15",
                "issue": "",
                "pages": "1--15",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Subramanya R. Dulloor, Amitabha Roy, Zheguang Zhao, Narayanan Sundaram, Nadathur Satish, Rajesh Sankaran, Jeff Jackson, and Karsten Schwan. 2016. Data tiering in heterogeneous memory systems. In Proceedings of the 11th European Conference on Computer Systems (EuroSys'16). 15:1-15:16.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Using managed runtime systems to tolerate holes in wearable memories",
                "authors": [
                    {
                        "first": "Tiejun",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Karin",
                        "middle": [],
                        "last": "Strauss",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "M"
                        ],
                        "last": "Blackburn",
                        "suffix": ""
                    },
                    {
                        "first": "Kathryn",
                        "middle": [
                            "S"
                        ],
                        "last": "Mckinley",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Burger",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Larus",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/pldi/GaoSBMBL13",
                "year": 2013,
                "venue": "Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'13)",
                "volume": "",
                "issue": "",
                "pages": "297--308",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tiejun Gao, Karin Strauss, Stephen M. Blackburn, Kathryn S. McKinley, Doug Burger, and James R. Larus. 2013. Using managed runtime systems to tolerate holes in wearable memories. In Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'13). ACM, New York, 297-308.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "NumaGiC: A garbage collector for big data on big NUMA machines",
                "authors": [
                    {
                        "first": "Lokesh",
                        "middle": [],
                        "last": "Gidra",
                        "suffix": ""
                    },
                    {
                        "first": "Ga\u00ebl",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Sopena",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Shapiro",
                        "suffix": ""
                    },
                    {
                        "first": "Nhan",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/Gidra0SSN15",
                "year": 2015,
                "venue": "Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'15)",
                "volume": "",
                "issue": "",
                "pages": "661--673",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lokesh Gidra, Ga\u00ebl Thomas, Julien Sopena, Marc Shapiro, and Nhan Nguyen. 2015. NumaGiC: A garbage collector for big data on big NUMA machines. In Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'15). 661-673.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "GraphX: Graph processing in a distributed dataflow framework",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [
                            "E"
                        ],
                        "last": "Gonzalez",
                        "suffix": ""
                    },
                    {
                        "first": "Reynold",
                        "middle": [
                            "S"
                        ],
                        "last": "Xin",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Dave",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Crankshaw",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "J"
                        ],
                        "last": "Franklin",
                        "suffix": ""
                    },
                    {
                        "first": "Ion",
                        "middle": [],
                        "last": "Stoica",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/osdi/GonzalezXDCFS14",
                "year": 2014,
                "venue": "Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation (OSDI'14)",
                "volume": "",
                "issue": "",
                "pages": "599--613",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph E. Gonzalez, Reynold S. Xin, Ankur Dave, Daniel Crankshaw, Michael J. Franklin, and Ion Stoica. 2014. GraphX: Graph processing in a distributed dataflow framework. In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation (OSDI'14). 599-613.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Software-managed energy-efficient hybrid DRAM/NVM main memory",
                "authors": [
                    {
                        "first": "Ahmad",
                        "middle": [],
                        "last": "Hassan",
                        "suffix": ""
                    },
                    {
                        "first": "Hans",
                        "middle": [],
                        "last": "Vandierendonck",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitrios",
                        "middle": [
                            "S"
                        ],
                        "last": "Nikolopoulos",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/cf/HassanVN15",
                "year": 2015,
                "venue": "Proceedings of the 12th ACM International Conference on Computing Frontiers (CF'15)",
                "volume": "23",
                "issue": "",
                "pages": "1--23",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ahmad Hassan, Hans Vandierendonck, and Dimitrios S. Nikolopoulos. 2015. Software-managed energy-efficient hy- brid DRAM/NVM main memory. In Proceedings of the 12th ACM International Conference on Computing Frontiers (CF'15). ACM, New York, 23:1-23:8.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "AUTOTM: Automatic tensor movement in heterogeneous memory systems using integer linear programming",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Hildebrand",
                        "suffix": ""
                    },
                    {
                        "first": "Jawad",
                        "middle": [],
                        "last": "Khan",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Trika",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Lowe-Power",
                        "suffix": ""
                    },
                    {
                        "first": "Venkatesh",
                        "middle": [],
                        "last": "Akella",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/HildebrandKTLA20",
                "year": 2020,
                "venue": "Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems",
                "volume": "",
                "issue": "",
                "pages": "875--890",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Hildebrand, Jawad Khan, Sanjeev Trika, Jason Lowe-Power, and Venkatesh Akella. 2020. AUTOTM: Automatic tensor movement in heterogeneous memory systems using integer linear programming. In Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems. 875-890.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Identifying the sources of cache misses in Java programs without relying on hardware counters",
                "authors": [
                    {
                        "first": "Hiroshi",
                        "middle": [],
                        "last": "Inoue",
                        "suffix": ""
                    },
                    {
                        "first": "Toshio",
                        "middle": [],
                        "last": "Nakatani",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/iwmm/InoueN12",
                "year": 2012,
                "venue": "Proceedings of the 2012 International Symposium on Memory Management (ISMM'12)",
                "volume": "",
                "issue": "",
                "pages": "133--142",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hiroshi Inoue and Toshio Nakatani. 2012. Identifying the sources of cache misses in Java programs without relying on hardware counters. In Proceedings of the 2012 International Symposium on Memory Management (ISMM'12). 133-142.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Basic Performance Measurements of the Intel Optane DC Persistent Memory Module",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Izraelevitz",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Juno",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Amirsaman",
                        "middle": [],
                        "last": "Memaripour",
                        "suffix": ""
                    },
                    {
                        "first": "Yun",
                        "middle": [
                            "Joon"
                        ],
                        "last": "Soh",
                        "suffix": ""
                    },
                    {
                        "first": "Zixuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Subramanya",
                        "suffix": ""
                    },
                    {
                        "first": "Jishen",
                        "middle": [],
                        "last": "Dulloor",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Swanson",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen Zhao, and Steven Swanson. 2019. Basic Performance Measurements of the Intel Optane DC Persistent Memory Module. arXiv:cs.DC/1903.05714.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "CHOP: Adaptive filter-based DRAM caching for CMP server platforms",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Madan",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Upton",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Iyer",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Makineni",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Newell",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Solihin",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Balasubramonian",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/hpca/JiangMZUIMNSB10",
                "year": 2010,
                "venue": "Proceedings of the 16th International Symposium on High-Performance Computer Architecture (HPCA'10)",
                "volume": "",
                "issue": "",
                "pages": "1--12",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "X. Jiang, N. Madan, L. Zhao, M. Upton, R. Iyer, S. Makineni, D. Newell, Y. Solihin, and R. Balasubramonian. 2010. CHOP: Adaptive filter-based DRAM caching for CMP server platforms. In Proceedings of the 16th International Symposium on High-Performance Computer Architecture (HPCA'10). 1-12.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Early experiences with persistent Java",
                "authors": [
                    {
                        "first": "Mick",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 1996,
                "venue": "The First International Workshop on Persistence and Java",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mick Jordan. 1996. Early experiences with persistent Java. In The First International Workshop on Persistence and Java.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Orthogonal Persistence for the Java T M Platform: Specification and Rationale",
                "authors": [
                    {
                        "first": "Mick",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "Malcolm",
                        "middle": [],
                        "last": "Atkinson",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mick Jordan and Malcolm Atkinson. 2000. Orthogonal Persistence for the Java T M Platform: Specification and Rationale. Technical Report. Mountain View, CA.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "SplitFS: Reducing software overhead in file systems for persistent memory",
                "authors": [
                    {
                        "first": "Rohan",
                        "middle": [],
                        "last": "Kadekodi",
                        "suffix": ""
                    },
                    {
                        "first": "Kwon",
                        "middle": [],
                        "last": "Se",
                        "suffix": ""
                    },
                    {
                        "first": "Sanidhya",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Taesoo",
                        "middle": [],
                        "last": "Kashyap",
                        "suffix": ""
                    },
                    {
                        "first": "Aasheesh",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Vijay",
                        "middle": [],
                        "last": "Kolli",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chidambaram",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/KadekodiLKKKC19",
                "year": 2019,
                "venue": "Proceedings of the 27th ACM Symposium on Operating Systems Principles",
                "volume": "",
                "issue": "",
                "pages": "494--508",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim, Aasheesh Kolli, and Vijay Chidambaram. 2019. SplitFS: Reducing software overhead in file systems for persistent memory. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. ACM, New York, 494-508.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "SLM-DB: Single-level keyvalue store with persistent memory",
                "authors": [
                    {
                        "first": "Olzhas",
                        "middle": [],
                        "last": "Kaiyrakhmet",
                        "suffix": ""
                    },
                    {
                        "first": "Songyi",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Beomseok",
                        "middle": [],
                        "last": "Nam",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [
                            "H"
                        ],
                        "last": "Noh",
                        "suffix": ""
                    },
                    {
                        "first": "Young-Ri",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/fast/KaiyrakhmetLNNC19",
                "year": 2019,
                "venue": "Proceedings of the 17th {USENIX} Conference on File and Storage Technologies ({FAST} 19)",
                "volume": "",
                "issue": "",
                "pages": "191--205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Olzhas Kaiyrakhmet, Songyi Lee, Beomseok Nam, Sam H. Noh, and Young-ri Choi. 2019. SLM-DB: Single-level key- value store with persistent memory. In Proceedings of the 17th {USENIX} Conference on File and Storage Technologies ({FAST} 19). 191-205.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "HeteroOS: OS design for heterogeneous memory management in datacenter",
                "authors": [
                    {
                        "first": "Sudarsun",
                        "middle": [],
                        "last": "Kannan",
                        "suffix": ""
                    },
                    {
                        "first": "Ada",
                        "middle": [],
                        "last": "Gavrilovska",
                        "suffix": ""
                    },
                    {
                        "first": "Vishal",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Karsten",
                        "middle": [],
                        "last": "Schwan",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/KannanGGS17",
                "year": 2017,
                "venue": "Proceedings of the 44th Annual International Symposium on Computer Architecture",
                "volume": "",
                "issue": "",
                "pages": "521--534",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sudarsun Kannan, Ada Gavrilovska, Vishal Gupta, and Karsten Schwan. 2017. HeteroOS: OS design for heteroge- neous memory management in datacenter. In Proceedings of the 44th Annual International Symposium on Computer Architecture. 521-534.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Improving NAND flash based disk caches",
                "authors": [
                    {
                        "first": "Taeho",
                        "middle": [],
                        "last": "Kgil",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Mudge",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/KgilRM08",
                "year": 2008,
                "venue": "Proceedings of the 35th Annual International Symposium on Computer Architecture (ISCA'08)",
                "volume": "",
                "issue": "",
                "pages": "327--338",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Taeho Kgil, David Roberts, and Trevor Mudge. 2008. Improving NAND flash based disk caches. In Proceedings of the 35th Annual International Symposium on Computer Architecture (ISCA'08). 327-338.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "PapyrusKV: A high-performance parallel key-value store for distributed NVM architectures",
                "authors": [
                    {
                        "first": "Jungwon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Seyong",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "S"
                        ],
                        "last": "Vetter",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sc/KimLV17",
                "year": 2017,
                "venue": "Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC'17)",
                "volume": "57",
                "issue": "",
                "pages": "1--57",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jungwon Kim, Seyong Lee, and Jeffrey S. Vetter. 2017. PapyrusKV: A high-performance parallel key-value store for distributed NVM architectures. In Proceedings of the International Conference for High Performance Computing, Net- working, Storage and Analysis (SC'17). 57:1-57:14.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Evaluating STT-RAM as an energy-efficient main memory alternative",
                "authors": [
                    {
                        "first": "Emre",
                        "middle": [],
                        "last": "Kultursay",
                        "suffix": ""
                    },
                    {
                        "first": "Mahmut",
                        "middle": [],
                        "last": "Kandemir",
                        "suffix": ""
                    },
                    {
                        "first": "Anand",
                        "middle": [],
                        "last": "Sivasubramaniam",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/ispass/KultursayKSM13",
                "year": 2013,
                "venue": "Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS'13)",
                "volume": "",
                "issue": "",
                "pages": "256--267",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emre Kultursay, Mahmut Kandemir, Anand Sivasubramaniam, and Onur Mutlu. 2013. Evaluating STT-RAM as an energy-efficient main memory alternative. In Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS'13). 256-267.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "High performance metadata integrity protection in the {WAFL} copy-on-write file system",
                "authors": [
                    {
                        "first": "Harendra",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Yuvraj",
                        "middle": [],
                        "last": "Patel",
                        "suffix": ""
                    },
                    {
                        "first": "Ram",
                        "middle": [],
                        "last": "Kesavan",
                        "suffix": ""
                    },
                    {
                        "first": "Sumith",
                        "middle": [],
                        "last": "Makam",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/fast/KumarPKM17",
                "year": 2017,
                "venue": "Proceedings of the 15th {USENIX} Conference on File and Storage Technologies ({FAST} 17)",
                "volume": "",
                "issue": "",
                "pages": "197--212",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harendra Kumar, Yuvraj Patel, Ram Kesavan, and Sumith Makam. 2017. High performance metadata integrity pro- tection in the {WAFL} copy-on-write file system. In Proceedings of the 15th {USENIX} Conference on File and Storage Technologies ({FAST} 17). 197-212.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Strata: A cross media file system",
                "authors": [
                    {
                        "first": "Youngjin",
                        "middle": [],
                        "last": "Kwon",
                        "suffix": ""
                    },
                    {
                        "first": "Henrique",
                        "middle": [],
                        "last": "Fingler",
                        "suffix": ""
                    },
                    {
                        "first": "Tyler",
                        "middle": [],
                        "last": "Hunt",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Emmett",
                        "middle": [],
                        "last": "Witchel",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/KwonFHPWA17",
                "year": 2017,
                "venue": "Proceedings of the 26th Symposium on Operating Systems Principles",
                "volume": "",
                "issue": "",
                "pages": "460--477",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Youngjin Kwon, Henrique Fingler, Tyler Hunt, Simon Peter, Emmett Witchel, and Thomas Anderson. 2017. Strata: A cross media file system. In Proceedings of the 26th Symposium on Operating Systems Principles. 460-477.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Architecting phase change memory as a scalable DRAM alternative",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [
                            "C"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Engin",
                        "middle": [],
                        "last": "Ipek",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Burger",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/LeeIMB09",
                "year": 2009,
                "venue": "Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09)",
                "volume": "",
                "issue": "",
                "pages": "2--13",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin C. Lee, Engin Ipek, Onur Mutlu, and Doug Burger. 2009. Architecting phase change memory as a scalable DRAM alternative. In Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09). 2-13.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Phase-change technology and the future of main memory",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [
                            "C"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Youtao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Engin",
                        "middle": [],
                        "last": "Ipek",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Burger",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2010,
                "venue": "IEEE Micro",
                "volume": "30",
                "issue": "1",
                "pages": "143--143",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Benjamin C. Lee, Ping Zhou, Jun Yang, Youtao Zhang, Bo Zhao, Engin Ipek, Onur Mutlu, and Doug Burger. 2010. Phase-change technology and the future of main memory. IEEE Micro 30, 1 (Jan. 2010), 143-143.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Recipe: Converting concurrent DRAM indexes to persistent-memory indexes",
                "authors": [
                    {
                        "first": "Jayashree",
                        "middle": [],
                        "last": "Se Kwon Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sanidhya",
                        "middle": [],
                        "last": "Mohan",
                        "suffix": ""
                    },
                    {
                        "first": "Taesoo",
                        "middle": [],
                        "last": "Kashyap",
                        "suffix": ""
                    },
                    {
                        "first": "Vijay",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chidambaram",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/LeeMKKC19",
                "year": 2019,
                "venue": "Proceedings of the 27th ACM Symposium on Operating Systems Principles",
                "volume": "",
                "issue": "",
                "pages": "462--477",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and Vijay Chidambaram. 2019. Recipe: Converting concurrent DRAM indexes to persistent-memory indexes. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. 462-477.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Memcached-pmem",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lenovo",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lenovo. 2018. Memcached-pmem. https://github.com/lenovo/memcachedpmem.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Identifying opportunities for byte-addressable non-volatile memory in extreme-scale scientific applications",
                "authors": [
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "S"
                        ],
                        "last": "Vetter",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Marin",
                        "suffix": ""
                    },
                    {
                        "first": "Collin",
                        "middle": [],
                        "last": "Mccurdy",
                        "suffix": ""
                    },
                    {
                        "first": "Cristian",
                        "middle": [],
                        "last": "Cira",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuo",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Weikuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/ipps/LiVMMCLY12",
                "year": 2012,
                "venue": "Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium (IPDPS'12)",
                "volume": "",
                "issue": "",
                "pages": "945--956",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dong Li, Jeffrey S. Vetter, Gabriel Marin, Collin McCurdy, Cristian Cira, Zhuo Liu, and Weikuan Yu. 2012. Identifying opportunities for byte-addressable non-volatile memory in extreme-scale scientific applications. In Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium (IPDPS'12). 945-956.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Utility-based hybrid memory management",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Saugata",
                        "middle": [],
                        "last": "Ghose",
                        "suffix": ""
                    },
                    {
                        "first": "Jongmoo",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Hui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/cluster/LiGCSWM17",
                "year": 2017,
                "venue": "Proceedings of the 2017 IEEE International Conference on Cluster Computing (CLUSTER'17)",
                "volume": "",
                "issue": "",
                "pages": "152--165",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Li, Saugata Ghose, Jongmoo Choi, Jin Sun, Hui Wang, and Onur Mutlu. 2017. Utility-based hybrid memory management. In Proceedings of the 2017 IEEE International Conference on Cluster Computing (CLUSTER'17). 152-165.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Cross-failure bug detection in persistent memory programs",
                "authors": [
                    {
                        "first": "Sihang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Korakit",
                        "middle": [],
                        "last": "Seemakhupt",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhou",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Wenisch",
                        "suffix": ""
                    },
                    {
                        "first": "Aasheesh",
                        "middle": [],
                        "last": "Kolli",
                        "suffix": ""
                    },
                    {
                        "first": "Samira",
                        "middle": [],
                        "last": "Khan",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/0001SWWKK20",
                "year": 2020,
                "venue": "Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems",
                "volume": "",
                "issue": "",
                "pages": "1187--1202",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sihang Liu, Korakit Seemakhupt, Yizhou Wei, Thomas Wenisch, Aasheesh Kolli, and Samira Khan. 2020. Cross-failure bug detection in persistent memory programs. In Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems. 1187-1202.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Dash: Scalable hashing on persistent memory",
                "authors": [
                    {
                        "first": "Baotong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangpeng",
                        "middle": [],
                        "last": "Hao",
                        "suffix": ""
                    },
                    {
                        "first": "Tianzheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2003.07302"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Baotong Lu, Xiangpeng Hao, Tianzheng Wang, and Eric Lo. 2020. Dash: Scalable hashing on persistent memory. arXiv preprint arXiv:2003.07302 (2020).",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Taurus: A holistic language runtime system for coordinating distributed managed-language applications",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Maas",
                        "suffix": ""
                    },
                    {
                        "first": "Krste",
                        "middle": [],
                        "last": "Asanovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Harris",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Kubiatowicz",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/MaasA0K16",
                "year": 2016,
                "venue": "Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'16)",
                "volume": "",
                "issue": "",
                "pages": "457--471",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Maas, Krste Asanovi\u0107, Tim Harris, and John Kubiatowicz. 2016. Taurus: A holistic language runtime system for coordinating distributed managed-language applications. In Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'16). 457-471.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "A low-power phase change memory based hybrid cache architecture",
                "authors": [
                    {
                        "first": "Prasanth",
                        "middle": [],
                        "last": "Mangalagiri",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Sarpatwari",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Yanamandra",
                        "suffix": ""
                    },
                    {
                        "first": "Vijaykrishnan",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "Jane"
                        ],
                        "last": "Irwin",
                        "suffix": ""
                    },
                    {
                        "first": "Osama",
                        "middle": [],
                        "last": "Awadel",
                        "suffix": ""
                    },
                    {
                        "first": "Karim",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/glvlsi/MangalagiriSYNXIK08",
                "year": 2008,
                "venue": "Proceedings of the 18th ACM Great Lakes Symposium on VLSI (GLSVLSI'08)",
                "volume": "",
                "issue": "",
                "pages": "395--398",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Prasanth Mangalagiri, Karthik Sarpatwari, Aditya Yanamandra, VijayKrishnan Narayanan, Yuan Xie, Mary Jane Irwin, and Osama Awadel Karim. 2008. A low-power phase change memory based hybrid cache architecture. In Proceedings of the 18th ACM Great Lakes Symposium on VLSI (GLSVLSI'08). 395-398.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Unified Holistic Memory Management Supporting Multiple Big Data",
                "authors": [],
                "dblp_id": null,
                "year": null,
                "venue": "",
                "volume": "2",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Unified Holistic Memory Management Supporting Multiple Big Data 2:37",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Effectively prefetching remote memory with leap",
                "authors": [
                    {
                        "first": "Al",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Mosharaf",
                        "middle": [],
                        "last": "Maruf",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chowdhury",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/usenix/MarufC20",
                "year": 2020,
                "venue": "Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC 20). USENIX Association",
                "volume": "",
                "issue": "",
                "pages": "843--857",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hasan Al Maruf and Mosharaf Chowdhury. 2020. Effectively prefetching remote memory with leap. In Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC 20). USENIX Association, 843-857. https://www.usenix. org/conference/atc20/presentation/al-maruf.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Enabling efficient and scalable hybrid memories using fine-granularity DRAM cache management",
                "authors": [
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Meza",
                        "suffix": ""
                    },
                    {
                        "first": "Jichuan",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Hanbin",
                        "middle": [],
                        "last": "Yoon",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    },
                    {
                        "first": "Parthasarathy",
                        "middle": [],
                        "last": "Ranganathan",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2012,
                "venue": "IEEE Computer Architecture Letters",
                "volume": "11",
                "issue": "2",
                "pages": "61--64",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Justin Meza, Jichuan Chang, HanBin Yoon, Onur Mutlu, and Parthasarathy Ranganathan. 2012. Enabling efficient and scalable hybrid memories using fine-granularity DRAM cache management. IEEE Computer Architecture Letters 11, 2 (July 2012), 61-64.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "TN-40-07: Calculating Memory Power for DDR4 SDRAM Introduction",
                "authors": [],
                "dblp_id": null,
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Micron. 2017. TN-40-07: Calculating Memory Power for DDR4 SDRAM Introduction. https://www.micron.com/- /media/documents/products/technical-note/dram/tn4007_ddr4_power_calculation.pdf.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Operating system support for NVM+DRAM hybrid main memory",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "C"
                        ],
                        "last": "Mogul",
                        "suffix": ""
                    },
                    {
                        "first": "Eduardo",
                        "middle": [],
                        "last": "Argollo",
                        "suffix": ""
                    },
                    {
                        "first": "Mehul",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Faraboschi",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/hotos/MogulASF09",
                "year": 2009,
                "venue": "Proceedings of the 12th Conference on Hot Topics in Operating Systems (HotOS'09)",
                "volume": "",
                "issue": "",
                "pages": "14--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey C. Mogul, Eduardo Argollo, Mehul Shah, and Paolo Faraboschi. 2009. Operating system support for NVM+DRAM hybrid main memory. In Proceedings of the 12th Conference on Hot Topics in Operating Systems (HotOS'09). 14-14.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "NVM/DRAM hybrid memory management with language runtime support via MRW queue",
                "authors": [
                    {
                        "first": "Gaku",
                        "middle": [],
                        "last": "Nakagawa",
                        "suffix": ""
                    },
                    {
                        "first": "Shuichi",
                        "middle": [],
                        "last": "Oikawa",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/snpd/NakagawaO15",
                "year": 2015,
                "venue": "Proceedings of the 2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD'15",
                "volume": "",
                "issue": "",
                "pages": "357--362",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gaku Nakagawa and Shuichi Oikawa. 2015. NVM/DRAM hybrid memory management with language runtime sup- port via MRW queue. In Proceedings of the 2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD'15). 357-362.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Write-optimized dynamic hashing for persistent memory",
                "authors": [
                    {
                        "first": "Moohyeon",
                        "middle": [],
                        "last": "Nam",
                        "suffix": ""
                    },
                    {
                        "first": "Hokeun",
                        "middle": [],
                        "last": "Cha",
                        "suffix": ""
                    },
                    {
                        "first": "Young-Ri",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [
                            "H"
                        ],
                        "last": "Noh",
                        "suffix": ""
                    },
                    {
                        "first": "Beomseok",
                        "middle": [],
                        "last": "Nam",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/fast/NamCCNN19",
                "year": 2019,
                "venue": "Proceedings of the 17th {USENIX} Conference on File and Storage Technologies ({FAST} 19)",
                "volume": "",
                "issue": "",
                "pages": "31--44",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Moohyeon Nam, Hokeun Cha, Young-ri Choi, Sam H. Noh, and Beomseok Nam. 2019. Write-optimized dynamic hashing for persistent memory. In Proceedings of the 17th {USENIX} Conference on File and Storage Technologies ({FAST} 19). 31-44.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Yak: A high-performance big-data-friendly garbage collector",
                "authors": [
                    {
                        "first": "Khanh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Guoqing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Demsky",
                        "suffix": ""
                    },
                    {
                        "first": "Shan",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Sanazsadat",
                        "middle": [],
                        "last": "Alamian",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/osdi/NguyenFXDLAM16",
                "year": 2016,
                "venue": "Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI'16)",
                "volume": "",
                "issue": "",
                "pages": "349--365",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Khanh Nguyen, Lu Fang, Guoqing Xu, Brian Demsky, Shan Lu, Sanazsadat Alamian, and Onur Mutlu. 2016. Yak: A high-performance big-data-friendly garbage collector. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI'16). 349-365.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "FACADE: A compiler and runtime for (almost) object-bounded big data applications",
                "authors": [
                    {
                        "first": "Khanh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yingyi",
                        "middle": [],
                        "last": "Bu",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfei",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Guoqing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/NguyenWBFHX15",
                "year": 2015,
                "venue": "Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'15)",
                "volume": "",
                "issue": "",
                "pages": "675--690",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Khanh Nguyen, Kai Wang, Yingyi Bu, Lu Fang, Jianfei Hu, and Guoqing Xu. 2015. FACADE: A compiler and runtime for (almost) object-bounded big data applications. In Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'15). 675-690.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Concurrent compacting garbage collection of a persistent heap",
                "authors": [
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "James",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Toole",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Nettles",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gifford",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/OTooleNG93",
                "year": 1993,
                "venue": "Proceedings of the 14th ACM Symposium on Operating Systems Principles (SOSP'93)",
                "volume": "",
                "issue": "",
                "pages": "161--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James O'Toole, Scott Nettles, and David Gifford. 1993. Concurrent compacting garbage collection of a persistent heap. In Proceedings of the 14th ACM Symposium on Operating Systems Principles (SOSP'93). ACM, New York, 161-174.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "MLP aware heterogeneous memory system",
                "authors": [
                    {
                        "first": "Sujay",
                        "middle": [],
                        "last": "Phadke",
                        "suffix": ""
                    },
                    {
                        "first": "Satish",
                        "middle": [],
                        "last": "Narayanasamy",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/date/PhadkeN11",
                "year": 2011,
                "venue": "Proceedings of 2011 IEEE Design, Automation Test Conference in Europe (DATE'11)",
                "volume": "",
                "issue": "",
                "pages": "1--6",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sujay Phadke and Satish Narayanasamy. 2011. MLP aware heterogeneous memory system. In Proceedings of 2011 IEEE Design, Automation Test Conference in Europe (DATE'11). 1-6.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Scalable high performance main memory system using phase-change memory technology",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Moinuddin",
                        "suffix": ""
                    },
                    {
                        "first": "Vijayalakshmi",
                        "middle": [],
                        "last": "Qureshi",
                        "suffix": ""
                    },
                    {
                        "first": "Jude",
                        "middle": [
                            "A"
                        ],
                        "last": "Srinivasan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rivers",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/QureshiSR09",
                "year": 2009,
                "venue": "Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09)",
                "volume": "",
                "issue": "",
                "pages": "24--33",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Moinuddin K. Qureshi, Vijayalakshmi Srinivasan, and Jude A. Rivers. 2009. Scalable high performance main mem- ory system using phase-change memory technology. In Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09). 24-33.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Page placement in hybrid memory systems",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Luiz",
                        "suffix": ""
                    },
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Ramos",
                        "suffix": ""
                    },
                    {
                        "first": "Ricardo",
                        "middle": [],
                        "last": "Gorbatov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bianchini",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/ics/RamosGB11",
                "year": 2011,
                "venue": "Proceedings of the International Conference on Supercomputing (ICS'11)",
                "volume": "",
                "issue": "",
                "pages": "85--95",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luiz E. Ramos, Eugene Gorbatov, and Ricardo Bianchini. 2011. Page placement in hybrid memory systems. In Proceed- ings of the International Conference on Supercomputing (ICS'11). 85-95.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "ThyNVM: Enabling software-transparent crash consistency in persistent memory systems",
                "authors": [
                    {
                        "first": "Jishen",
                        "middle": [],
                        "last": "Jinglei Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Samira",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Jongmoo",
                        "middle": [],
                        "last": "Khan",
                        "suffix": ""
                    },
                    {
                        "first": "Yongwei",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mutiu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/micro/RenZKCWM15",
                "year": 2015,
                "venue": "Proceedings of the 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)",
                "volume": "",
                "issue": "",
                "pages": "672--685",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinglei Ren, Jishen Zhao, Samira Khan, Jongmoo Choi, Yongwei Wu, and Onur Mutiu. 2015. ThyNVM: Enabling software-transparent crash consistency in persistent memory systems. In Proceedings of the 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 672-685.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Lightweight recoverable virtual memory",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Satyanarayanan",
                        "suffix": ""
                    },
                    {
                        "first": "Henry",
                        "middle": [
                            "H"
                        ],
                        "last": "Mashburn",
                        "suffix": ""
                    },
                    {
                        "first": "Puneet",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "C"
                        ],
                        "last": "Steere",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "J"
                        ],
                        "last": "Kistler",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/SatyanarayananMKSK93",
                "year": 1994,
                "venue": "ACM Trans. Comput. Syst",
                "volume": "12",
                "issue": "1",
                "pages": "33--57",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Satyanarayanan, Henry H. Mashburn, Puneet Kumar, David C. Steere, and James J. Kistler. 1994. Lightweight recoverable virtual memory. ACM Trans. Comput. Syst. 12, 1 (Feb. 1994), 33-57.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Managing non-volatile memory in database systems",
                "authors": [
                    {
                        "first": "Viktor",
                        "middle": [],
                        "last": "Alexander Van Renen",
                        "suffix": ""
                    },
                    {
                        "first": "Alfons",
                        "middle": [],
                        "last": "Leis",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Kemper",
                        "suffix": ""
                    },
                    {
                        "first": "Takushi",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuichi",
                        "middle": [],
                        "last": "Hashida",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshiyasu",
                        "middle": [],
                        "last": "Oe",
                        "suffix": ""
                    },
                    {
                        "first": "Lilian",
                        "middle": [],
                        "last": "Doi",
                        "suffix": ""
                    },
                    {
                        "first": "Mitsuru",
                        "middle": [],
                        "last": "Harada",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sato",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sigmod/RenenLK0HODHS18",
                "year": 2018,
                "venue": "Proceedings of the 2018 International Conference on Management of Data",
                "volume": "",
                "issue": "",
                "pages": "1541--1555",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander van Renen, Viktor Leis, Alfons Kemper, Thomas Neumann, Takushi Hashida, Kazuichi Oe, Yoshiyasu Doi, Lilian Harada, and Mitsuru Sato. 2018. Managing non-volatile memory in database systems. In Proceedings of the 2018 International Conference on Management of Data. 1541-1555.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory",
                "authors": [
                    {
                        "first": "Nandita",
                        "middle": [],
                        "last": "Vijaykumar",
                        "suffix": ""
                    },
                    {
                        "first": "Abhilasha",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Diptesh",
                        "middle": [],
                        "last": "Majumdar",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Gennady",
                        "middle": [],
                        "last": "Pekhimenko",
                        "suffix": ""
                    },
                    {
                        "first": "Eiman",
                        "middle": [],
                        "last": "Ebrahimi",
                        "suffix": ""
                    },
                    {
                        "first": "Nastaran",
                        "middle": [],
                        "last": "Hajinazar",
                        "suffix": ""
                    },
                    {
                        "first": "Phillip",
                        "middle": [
                            "B"
                        ],
                        "last": "Gibbons",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/VijaykumarJMHPE18",
                "year": 2018,
                "venue": "Proceedings of the 45th Annual International Symposium on Computer Architecture (ISCA'18)",
                "volume": "",
                "issue": "",
                "pages": "207--220",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nandita Vijaykumar, Abhilasha Jain, Diptesh Majumdar, Kevin Hsieh, Gennady Pekhimenko, Eiman Ebrahimi, Nas- taran Hajinazar, Phillip B. Gibbons, and Onur Mutlu. 2018. A case for richer cross-layer abstractions: Bridging the semantic gap with expressive memory. In Proceedings of the 45th Annual International Symposium on Computer Archi- tecture (ISCA'18). 207-220.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Quartz: A lightweight performance emulator for persistent memory software",
                "authors": [
                    {
                        "first": "Haris",
                        "middle": [],
                        "last": "Volos",
                        "suffix": ""
                    },
                    {
                        "first": "Guilherme",
                        "middle": [],
                        "last": "Magalhaes",
                        "suffix": ""
                    },
                    {
                        "first": "Ludmila",
                        "middle": [],
                        "last": "Cherkasova",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/middleware/VolosMCL15",
                "year": 2015,
                "venue": "Proceedings of the 16th Annual Middleware Conference (Middleware'15)",
                "volume": "",
                "issue": "",
                "pages": "37--49",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Haris Volos, Guilherme Magalhaes, Ludmila Cherkasova, and Jun Li. 2015. Quartz: A lightweight performance emula- tor for persistent memory software. In Proceedings of the 16th Annual Middleware Conference (Middleware'15). 37-49.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Efficient management for hybrid memory in managed language runtime",
                "authors": [
                    {
                        "first": "Chenxi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Zigman",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Yunquan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaobing",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/npc/WangCZLZF16",
                "year": 2016,
                "venue": "Proceedings of the 16th IFIP International Conference on Network and Parallel Computing (NPC'16)",
                "volume": "",
                "issue": "",
                "pages": "29--42",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chenxi Wang, Ting Cao, John Zigman, Fang Lv, Yunquan Zhang, and Xiaobing Feng. 2016. Efficient management for hybrid memory in managed language runtime. In Proceedings of the 16th IFIP International Conference on Network and Parallel Computing (NPC'16). 29-42.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Exploiting program semantics to place data in hybrid memory",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Dejun",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Sally",
                        "middle": [
                            "A"
                        ],
                        "last": "Mckee",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Mingyu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/IEEEpact/WeiJMXC15",
                "year": 2015,
                "venue": "Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT'15)",
                "volume": "",
                "issue": "",
                "pages": "163--173",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei Wei, Dejun Jiang, Sally A. McKee, Jin Xiong, and Mingyu Chen. 2015. Exploiting program semantics to place data in hybrid memory. In Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT'15). 163-173.",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Metal-oxide RRAM",
                "authors": [
                    {
                        "first": "H",
                        "middle": [
                            "S P"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "T"
                        ],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Tsai",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2012,
                "venue": "Proc. IEEE",
                "volume": "100",
                "issue": "6",
                "pages": "1951--1970",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. S. P. Wong, H. Lee, S. Yu, Y. Chen, Y. Wu, P. Chen, B. Lee, F. T. Chen, and M. Tsai. 2012. Metal-oxide RRAM. Proc. IEEE 100, 6 (June 2012), 1951-1970.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "Phase change memory",
                "authors": [
                    {
                        "first": "H",
                        "middle": [
                            "S P"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Raoux",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "P"
                        ],
                        "last": "Reifenberg",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Rajendran",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Asheghi",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [
                            "E"
                        ],
                        "last": "Goodson",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2010,
                "venue": "Proc. IEEE",
                "volume": "98",
                "issue": "12",
                "pages": "2201--2227",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "H. S. P. Wong, S. Raoux, S. Kim, J. Liang, J. P. Reifenberg, B. Rajendran, M. Asheghi, and K. E. Goodson. 2010. Phase change memory. Proc. IEEE 98, 12 (Dec 2010), 2201-2227.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Espresso: Brewing Java for more non-volatility",
                "authors": [
                    {
                        "first": "Mingyu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Ziming",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Haoyu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Heting",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Haibo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Haibing",
                        "middle": [],
                        "last": "Zang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2018,
                "venue": "Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS'18)",
                "volume": "",
                "issue": "",
                "pages": "70--83",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mingyu Wu, Ziming Zhao, Haoyu Li, Heting Li, Haibo Chen, binyu Zang, and Haibing Guan. 2018. Espresso: Brewing Java for more non-volatility. In Proceedings of the 20th International Conference on Architectural Support for Program- ming Languages and Operating Systems (ASPLOS'18). 70-83.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "NVMcached: An NVM-based key-value cache",
                "authors": [
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Yandong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yufei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Zili",
                        "middle": [],
                        "last": "Hack",
                        "suffix": ""
                    },
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/apsys/WuNZWRHSJ16",
                "year": 2016,
                "venue": "Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xingbo Wu, Fan Ni, Li Zhang, Yandong Wang, Yufei Ren, Michel Hack, Zili Shao, and Song Jiang. 2016. NVMcached: An NVM-based key-value cache. In Proceedings of the 7th ACM SIGOPS Asia-Pacific Workshop on Systems. 1-7.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "HiKV: A hybrid index key-value store for DRAM-NVM memory systems",
                "authors": [
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Dejun",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Ninghui",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/usenix/XiaJXS17",
                "year": 2017,
                "venue": "Proceedings of the 2017 {USENIX} Annual Technical Conference ({USENIX}{ATC} 17)",
                "volume": "",
                "issue": "",
                "pages": "349--362",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Xia, Dejun Jiang, Jin Xiong, and Ninghui Sun. 2017. HiKV: A hybrid index key-value store for DRAM-NVM memory systems. In Proceedings of the 2017 {USENIX} Annual Technical Conference ({USENIX}{ATC} 17). 349-362.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "{NOVA}: A log-structured file system for hybrid volatile/non-volatile main memories",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Swanson",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/fast/XuS16",
                "year": 2016,
                "venue": "Proceedings of the 14th {USENIX} Conference on File and Storage Technologies ({FAST} 16)",
                "volume": "",
                "issue": "",
                "pages": "323--338",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian Xu and Steven Swanson. 2016. {NOVA}: A log-structured file system for hybrid volatile/non-volatile main mem- ories. In Proceedings of the 14th {USENIX} Conference on File and Storage Technologies ({FAST} 16). 323-338.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "Nova-fortis: A fault-tolerant non-volatile main memory file system",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Amirsaman",
                        "middle": [],
                        "last": "Memaripour",
                        "suffix": ""
                    },
                    {
                        "first": "Akshatha",
                        "middle": [],
                        "last": "Gangadharaiah",
                        "suffix": ""
                    },
                    {
                        "first": "Amit",
                        "middle": [],
                        "last": "Borase",
                        "suffix": ""
                    },
                    {
                        "first": "Tamires",
                        "middle": [],
                        "last": "Brito Da",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Swanson",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rudoff",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/sosp/XuZMGBSSR17",
                "year": 2017,
                "venue": "Proceedings of the 26th Symposium on Operating Systems Principles",
                "volume": "",
                "issue": "",
                "pages": "478--496",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jian Xu, Lu Zhang, Amirsaman Memaripour, Akshatha Gangadharaiah, Amit Borase, Tamires Brito Da Silva, Steven Swanson, and Andy Rudoff. 2017. Nova-fortis: A fault-tolerant non-volatile main memory file system. In Proceedings of the 26th Symposium on Operating Systems Principles. 478-496.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "Nimble page management for tiered memory systems",
                "authors": [
                    {
                        "first": "Zi",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Lustig",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Nellans",
                        "suffix": ""
                    },
                    {
                        "first": "Abhishek",
                        "middle": [],
                        "last": "Bhattacharjee",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/asplos/YanLNB19",
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3297858.3304024"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattacharjee. 2019. Nimble page management for tiered memory systems. ACM, New York. https://doi.org/10.1145/3297858.3304024",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Translation ranger: Operating system support for contiguity-aware TLBs",
                "authors": [
                    {
                        "first": "Zi",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Lustig",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Nellans",
                        "suffix": ""
                    },
                    {
                        "first": "Abhishek",
                        "middle": [],
                        "last": "Bhattacharjee",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/YanLNB19",
                "year": 2019,
                "venue": "Proceedings of the 46th International Symposium on Computer Architecture",
                "volume": "",
                "issue": "",
                "pages": "698--710",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zi Yan, Daniel Lustig, David Nellans, and Abhishek Bhattacharjee. 2019. Translation ranger: Operating system support for contiguity-aware TLBs. In Proceedings of the 46th International Symposium on Computer Architecture. 698-710.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Bridging the performance gap for copy-based garbage collectors atop non-volatile memory",
                "authors": [
                    {
                        "first": "Yanfei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Mingyu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Haibo",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Binyu",
                        "middle": [],
                        "last": "Zang",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/eurosys/YangW0Z21",
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3447786.3456246"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yanfei Yang, Mingyu Wu, Haibo Chen, and Binyu Zang. 2021. Bridging the performance gap for copy-based garbage collectors atop non-volatile memory. ACM, New York. https://doi.org/10.1145/3447786.3456246",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Row buffer locality aware caching policies for hybrid memories",
                "authors": [
                    {
                        "first": "Hanbin",
                        "middle": [],
                        "last": "Yoon",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Meza",
                        "suffix": ""
                    },
                    {
                        "first": "Rachata",
                        "middle": [],
                        "last": "Ausavarungnirun",
                        "suffix": ""
                    },
                    {
                        "first": "Rachael",
                        "middle": [],
                        "last": "Harding",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/iccd/YoonMAHM12",
                "year": 2012,
                "venue": "Proceedings of the 2012 IEEE 30th International Conference on Computer Design (ICCD'12)",
                "volume": "",
                "issue": "",
                "pages": "337--344",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "HanBin Yoon, Justin Meza, Rachata Ausavarungnirun, Rachael Harding, and Onur Mutlu. 2012. Row buffer locality aware caching policies for hybrid memories. In Proceedings of the 2012 IEEE 30th International Conference on Computer Design (ICCD'12). 337-344.",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Efficient data mapping and buffering techniques for multilevel cell phase-change memories",
                "authors": [
                    {
                        "first": "Hanbin",
                        "middle": [],
                        "last": "Yoon",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Meza",
                        "suffix": ""
                    },
                    {
                        "first": "Naveen",
                        "middle": [],
                        "last": "Muralimanohar",
                        "suffix": ""
                    },
                    {
                        "first": "Norman",
                        "middle": [
                            "P"
                        ],
                        "last": "Jouppi",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2014,
                "venue": "ACM Trans. Archit. Code Optim",
                "volume": "11",
                "issue": "4",
                "pages": "1--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hanbin Yoon, Justin Meza, Naveen Muralimanohar, Norman P. Jouppi, and Onur Mutlu. 2014. Efficient data mapping and buffering techniques for multilevel cell phase-change memories. ACM Trans. Archit. Code Optim. 11, 4 (Dec. 2014), 40:1-40:25.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Banshee: Bandwidthefficient DRAM caching via software/hardware cooperation",
                "authors": [
                    {
                        "first": "Xiangyao",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "J"
                        ],
                        "last": "Hughes",
                        "suffix": ""
                    },
                    {
                        "first": "Nadathur",
                        "middle": [],
                        "last": "Satish",
                        "suffix": ""
                    },
                    {
                        "first": "Onur",
                        "middle": [],
                        "last": "Mutlu",
                        "suffix": ""
                    },
                    {
                        "first": "Srinivas",
                        "middle": [],
                        "last": "Devadas",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/micro/YuHSMD17",
                "year": 2017,
                "venue": "Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-50)",
                "volume": "",
                "issue": "",
                "pages": "1--14",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiangyao Yu, Christopher J. Hughes, Nadathur Satish, Onur Mutlu, and Srinivas Devadas. 2017. Banshee: Bandwidth- efficient DRAM caching via software/hardware cooperation. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-50). ACM, New York, 1-14.",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing",
                "authors": [
                    {
                        "first": "Matei",
                        "middle": [],
                        "last": "Zaharia",
                        "suffix": ""
                    },
                    {
                        "first": "Mosharaf",
                        "middle": [],
                        "last": "Chowdhury",
                        "suffix": ""
                    },
                    {
                        "first": "Tathagata",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Dave",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Murphy",
                        "middle": [],
                        "last": "Mccauly",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "J"
                        ],
                        "last": "Franklin",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Shenker",
                        "suffix": ""
                    },
                    {
                        "first": "Ion",
                        "middle": [],
                        "last": "Stoica",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/nsdi/ZahariaCDDMMFSS12",
                "year": 2012,
                "venue": "Presented as part of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI'12)",
                "volume": "",
                "issue": "",
                "pages": "15--28",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Resilient distributed datasets: A fault-tolerant abstraction for in-memory clus- ter computing. In Presented as part of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI'12). 15-28.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "Exploring phase change memory and 3d die-stacking for power/thermal friendly, fast and durable memory architectures",
                "authors": [
                    {
                        "first": "Wangyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/IEEEpact/ZhangL09",
                "year": 2009,
                "venue": "Proceedings of the 2009 18th International Conference on Parallel Architectures and Compilation Techniques (PACT'09)",
                "volume": "",
                "issue": "",
                "pages": "101--112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wangyuan Zhang and Tao Li. 2009. Exploring phase change memory and 3d die-stacking for power/thermal friendly, fast and durable memory architectures. In Proceedings of the 2009 18th International Conference on Parallel Architectures and Compilation Techniques (PACT'09). 101-112.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "A durable and energy efficient main memory using phase change memory technology",
                "authors": [
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Youtao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/isca/ZhouZYZ09",
                "year": 2009,
                "venue": "Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09)",
                "volume": "",
                "issue": "",
                "pages": "14--23",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ping Zhou, Bo Zhao, Jun Yang, and Youtao Zhang. 2009. A durable and energy efficient main memory using phase change memory technology. In Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA'09). 14-23.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Phase-change memory: An architectural perspective",
                "authors": [
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Zilberberg",
                        "suffix": ""
                    },
                    {
                        "first": "Shlomo",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Sivan",
                        "middle": [],
                        "last": "Toledo",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2013,
                "venue": "ACM Comput. Surv",
                "volume": "45",
                "issue": "3",
                "pages": "1--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Omer Zilberberg, Shlomo Weiss, and Sivan Toledo. 2013. Phase-change memory: An architectural perspective. ACM Comput. Surv. 45, 3 (July 2013), 29:1-29:33.",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "Write-optimized and high-performance hashing index scheme for persistent memory",
                "authors": [
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Zuo",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Hua",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "dblp_id": "conf/osdi/Zuo0W18",
                "year": 2018,
                "venue": "Proceedings of the13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)",
                "volume": "",
                "issue": "",
                "pages": "461--476",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengfei Zuo, Yu Hua, and Jie Wu. 2018. Write-optimized and high-performance hashing index scheme for persistent memory. In Proceedings of the13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18). 461-476.",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "Efficient lock-free durable sets",
                "authors": [
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Zuriel",
                        "suffix": ""
                    },
                    {
                        "first": "Michal",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    },
                    {
                        "first": "Gali",
                        "middle": [],
                        "last": "Sheffi",
                        "suffix": ""
                    },
                    {
                        "first": "Nachshon",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Erez",
                        "middle": [],
                        "last": "Petrank",
                        "suffix": ""
                    }
                ],
                "dblp_id": null,
                "year": 2019,
                "venue": "Proceedings of the ACM on Programming Languages 3",
                "volume": "",
                "issue": "",
                "pages": "1--26",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoav Zuriel, Michal Friedman, Gali Sheffi, Nachshon Cohen, and Erez Petrank. 2019. Efficient lock-free durable sets. In Proceedings of the ACM on Programming Languages 3, (OOPSLA 2019). ACM, New York, 1-26. Received November 2020; revised October 2021; accepted January 2022",
                "links": null
            }
        }
    }
}